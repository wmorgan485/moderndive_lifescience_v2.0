[["index.html", "Modern Biological Data Analysis A ModernDive into R For Life Scientists, v. 2.1 ModernDive for Life Scientists", " Modern Biological Data Analysis A ModernDive into R For Life Scientists, v. 2.1 Chester Ismay and Albert Y. Kim Foreword by Kelly S. McConville Adapted by William R. Morgan November 06, 2022 ModernDive for Life Scientists Note: This book is an adaptation of ModernDive (version 2.0) for BIOL-20300 Research Skills for Life Scientists at The College of Wooster. Here is the original website for Statistical Inference via Data Science: A ModernDive into R and the Tidyverse! This work by Chester Ismay and Albert Y. Kim, and adapted by William R. Morgan, is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["foreword.html", "Foreword", " Foreword These are exciting times in statistics and data science education. (I am predicting this statement will continue to be true regardless of whether you are reading this foreword in 2020 or 2050.) But (isn’t there always a but?), as a statistics educator, it can also feel a bit overwhelming to stay on top of all the new statistical, technological, and pedagogical innovations. I find myself constantly asking, “Am I teaching my students the correct content, with the relevant software, and in the most effective way?”. Before I make all of us feel lost at sea, let me point out how great a life raft I have found in ModernDive. In a sea of intro stats textbooks, ModernDive floats to the top of my list, and let me tell you why. (Note my use of ModernDive here refers to the book in its shortened title version. This also matches up nicely with the neat hex sticker Drs. Ismay and Kim created for the cover of ModernDive, too.) My favorite aspect of ModernDive, if I must pick a favorite, is that students gain experience with the whole data analysis pipeline (see Figure 0.2). In particular, ModernDive is one of the few intro stats textbooks that teaches students how to wrangle data. And, while data cleaning may not be as groovy as model building, it’s often a prerequisite step! The world is full of messy data and ModernDive equips students to transform their data via the dplyr package. Speaking of dplyr, students of ModernDive are exposed to the tidyverse suite of R packages. Designed with a common structure, tidyverse functions are written to be easy to learn and use. And, since most intro stats students are programming newbies, ModernDive carefully walks the students through each new function it presents and provides frequent reinforcement through the many Learning checks dispersed throughout the chapters. Overall, ModernDive includes wise choices for the placement of topics. Starting with data visualization, ModernDive gets students building ggplot2 graphs early on and then continues to reinforce important concepts graphically throughout the book. After moving through data wrangling and data importing, modeling plays a prominent role, with two chapters devoted to building regression models and a later chapter on inference for regression. Lastly, statistical inference is presented through a computational lens with calculations done via the infer package. I first met Drs. Ismay and Kim while attending their workshop at the 2017 US Conference on Teaching Statistics. They pushed us as participants to put data first and to use computers, instead of math, as the engine for statistical inference. That experience helped me modernize my own intro stats course and introduced me to two really forward-thinking statistics and data science educators. It has been exciting to see ModernDive develop and grow into such a wonderful, timely textbook. I hope you have decided to dive on in! Kelly S. McConville, Reed College "],["preface.html", "Preface Introduction for students Introduction for instructors Connect and contribute Acknowledgements About this book", " Preface Help! I’m completely new to coding and I need to learn R and RStudio! What do I do? If you’re asking yourself this question, then you’ve come to the right place! Start with the “Introduction for students” section. Are you an instructor hoping to use this book in your courses? We recommend reading the “Introduction for students” section first. Then, read the “Introduction for instructors” section for more information on how to teach with this book. Are you looking to connect with and contribute to ModernDive? Then, read the “Connect and contribute” section for information on how. Are you curious about the publishing of this book? Then, read the “About this book” section for more information on the open-source technology, in particular R Markdown and the bookdown package. Introduction for students This book assumes no prerequisites: no algebra, no calculus, and no prior programming/coding experience. This is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians, data journalists, and other researchers would. We present a map of your upcoming journey in Figure 0.1. FIGURE 0.1: ModernDive for Life Scientists flowchart. You’ll first get started with data in Chapter 1 where you’ll learn about the difference between R and RStudio, start coding in R, install and load your first R packages, and explore and import datasets. Then you’ll cover the following three portions of this book: Data science with tidyverse. You’ll assemble your data science toolbox using tidyverse packages. In particular, you’ll Ch.2: Visualize data using the ggplot2 package. Ch.3: Wrangle data using the dplyr package. Ch.4: Learn about the concept of “tidy” data as a standardized data input and output format for all packages in the tidyverse. Statistical inference with infer. Using your newly acquired data science tools and helper functions from the moderndive package, you’ll unpack statistical inference using the infer package. In particular, you’ll: Ch.5: Learn about the role that sampling variability plays in statistical inference and the role that sample size plays in this sampling variability. Ch.6: Construct confidence intervals using bootstrapping. Ch.7: Conduct hypothesis tests using permutation. Data modeling with moderndive. Once again using these data science tools and helper functions from the infer and moderndive packages, you’ll fit your first data models. In particular, you’ll: Ch.8: Discover basic regression models with only one explanatory variable. Ch.9: Interpret confidence intervals and hypothesis tests in a regression setting. Ch.10: Examine multiple regression models with more than one explanatory variable. We’ll end with a discussion on what it means to “tell your story with data” in Chapter 11. 1 What we hope you will learn from this book We hope that by the end of this book, you’ll have learned how to: Use R and the tidyverse suite of R packages for data science. Perform statistical inference using sampling, confidence intervals. and hypothesis tests. Fit your first models to data, using a method known as linear regression. Tell your story with data using these tools. What do we mean by data stories? We mean any analysis involving data that engages the reader in answering questions with careful visuals and thoughtful discussion. Further discussions on data stories can be found in the blog post “Tell a Meaningful Story With Data.” Over the course of this book, you will develop your “data science toolbox,” equipping yourself with tools such as data visualization, data formatting, data wrangling, and data modeling using regression. In particular, this book will lean heavily on data visualization. In today’s world, we are bombarded with graphics that attempt to convey ideas. We will explore what makes a good graphic and what the standard ways are used to convey relationships within data. In general, we’ll use visualization as a way of building almost all of the ideas in this book. To impart the statistical lessons of this book, we have intentionally minimized the number of mathematical formulas used. Instead, you’ll develop a conceptual understanding of statistics using data visualization and computer simulations. We hope this is a more intuitive experience than the way statistics has traditionally been taught in the past and how it is commonly perceived. Finally, you’ll learn the importance of literate programming. By this we mean you’ll learn how to write code that is useful not just for a computer to execute, but also for readers to understand exactly what your analysis is doing and how you did it. This is part of a greater effort to encourage reproducible research (see the “Reproducible research” subsection in this Preface for more details). Hal Abelson coined the phrase that we will follow throughout this book: Programs must be written for people to read, and only incidentally for machines to execute. We understand that there may be challenging moments as you learn to program. Both of us continue to struggle and find ourselves often using web searches to find answers and reach out to colleagues for help. In the long run though, we all can solve problems faster and more elegantly via programming. We wrote this book as our way to help you get started and you should know that there is a huge community of R users that are happy to help everyone along as well. This community exists in particular on the internet on various forums and websites such as stackoverflow.com. Data/science pipeline You may think of statistics as just being a bunch of numbers. We commonly hear the phrase “statistician” when listening to broadcasts of sporting events. Statistics (in particular, data analysis), in addition to describing numbers like with baseball batting averages, plays a vital role in all of the sciences. You’ll commonly hear the phrase “statistically significant” thrown around in the media. You’ll see articles that say, “Science now shows that chocolate is good for you.” Underpinning these claims is data analysis. By the end of this book, you’ll be able to better understand whether these claims should be trusted or whether we should be wary. Inside data analysis are many sub-fields that we will discuss throughout this book (though not necessarily in this order): data collection data wrangling data visualization data modeling inference correlation and regression interpretation of results data communication/storytelling These sub-fields are summarized in what Grolemund and Wickham have previously termed the “data/science pipeline” in Figure 0.2. FIGURE 0.2: Data/science pipeline. We will begin by digging into the grey Understand portion of the cycle with data visualization, then with a discussion on what is meant by tidy data and data wrangling, and then conclude by talking about interpreting and discussing the results of our models via Communication. These steps are vital to any statistical analysis. But, why should you care about statistics? There’s a reason that many fields require a statistics course. Scientific knowledge grows through an understanding of statistical significance and data analysis. You needn’t be intimidated by statistics. It’s not the beast that it used to be and, paired with computation, you’ll see how reproducible research in the sciences particularly increases scientific knowledge. Reproducible research The most important tool is the mindset, when starting, that the end product will be reproducible. – Keith Baggerly Another goal of this book is to help readers understand the importance of reproducible analyses. The hope is to get readers into the habit of making their analyses reproducible from the very beginning. This means we’ll be trying to help you build new habits. This will take practice and be difficult at times. You’ll see just why it is so important for you to keep track of your code and document it well to help yourself later and any potential collaborators as well. Copying and pasting results from one program into a word processor is not an ideal way to conduct efficient and effective scientific research. It’s much more important for time to be spent on data collection and data analysis and not on copying and pasting plots back and forth across a variety of programs. In traditional analyses, if an error was made with the original data, we’d need to step through the entire process again: recreate the plots and copy-and-paste all of the new plots and our statistical analysis into our document. This is error prone and a frustrating use of time. We want to help you to get away from this tedious activity so that we can spend more time doing science. We are talking about computational reproducibility. - Yihui Xie Reproducibility means a lot of things in terms of different scientific fields. Are experiments conducted in a way that another researcher could follow the steps and get similar results? In this book, we will focus on what is known as computational reproducibility. This refers to being able to pass all of one’s data analysis, datasets, and conclusions to someone else and have them get exactly the same results on their machine. This allows for time to be spent interpreting results and considering assumptions instead of the more error prone way of starting from scratch or following a list of steps that may be different from machine to machine. Final note for students At this point, if you are interested in instructor perspectives on this book, ways to contribute and collaborate, or the technical details of this book’s construction and publishing, then continue with the rest of the chapter. Otherwise, let’s get started with R and RStudio in Chapter 1! Introduction for instructors Resources Here are some resources to help you use ModernDive: Dr. Jenny Smetzer and Albert Y. Kim have written a series of labs and problem sets. You can find them at https://moderndive.com/labs. You can see the webpages for two courses that use ModernDive: Smith College “SDS192 Introduction to Data Science”: https://rudeboybert.github.io/SDS192/. Smith College “SDS220 Introduction to Probability and Statistics”: https://rudeboybert.github.io/SDS220/. Why did we write this book? This book is inspired by Mathematical Statistics with Resampling and R (Chihara and Hesterberg 2011) OpenIntro: Intro Stat with Randomization and Simulation (Diez, Barr, and Çetinkaya-Rundel 2014) R for Data Science (Grolemund and Wickham 2017) The first book, designed for upper-level undergraduates and graduate students, provides an excellent resource on how to use resampling to impart statistical concepts like sampling distributions using computation instead of large-sample approximations and other mathematical formulas. The last two books are free options for learning about introductory statistics and data science, providing an alternative to the many traditionally expensive introductory statistics textbooks. When looking over the introductory statistics textbooks that currently exist, we found there wasn’t one that incorporated many newly developed R packages directly into the text, in particular the many packages included in the tidyverse set of packages, such as ggplot2, dplyr, tidyr, and readr that will be the focus of this book’s first part on “Data Science with tidyverse.” Additionally, there wasn’t an open-source and easily reproducible textbook available that exposed new learners to all four of the learning goals we listed in the “Introduction for students” subsection. We wanted to write a book that could develop theory via computational techniques and help novices master the R language in doing so. Who is this book for? This book is intended for instructors of traditional introductory statistics classes using RStudio, who would like to inject more data science topics into their syllabus. RStudio can be used in either the server version or the desktop version. (This is discussed further in Subsection 1.1.1.) We assume that students taking the class will have no prior algebra, no calculus, nor programming/coding experience. Here are some principles and beliefs we kept in mind while writing this text. If you agree with them, this is the book for you. Blur the lines between lecture and lab With increased availability and accessibility of laptops and open-source non-proprietary statistical software, the strict dichotomy between lab and lecture can be loosened. It’s much harder for students to understand the importance of using software if they only use it once a week or less. They forget the syntax in much the same way someone learning a foreign language forgets the grammar rules. Frequent reinforcement is key. Focus on the entire data/science research pipeline We believe that the entirety of Grolemund and Wickham’s data/science pipeline as seen in Figure 0.2 should be taught. We heed George Cobb’s call to “minimize prerequisites to research”: students should be answering questions with data as soon as possible. It’s all about the data We leverage R packages for rich, real, and realistic datasets that at the same time are easy-to-load into R, such as the nycflights13 and fivethirtyeight packages. We believe that data visualization is a “gateway drug” for statistics and that the grammar of graphics as implemented in the ggplot2 package is the best way to impart such lessons. However, we often hear: “You can’t teach ggplot2 for data visualization in intro stats!” We, like David Robinson, are much more optimistic and have found our students have been largely successful in learning it. dplyr has made data wrangling much more accessible to novices, and hence much more interesting datasets can be explored. Use simulation/resampling to introduce statistical inference, not probability/mathematical formulas Instead of using formulas, large-sample approximations, and probability tables, we teach statistical concepts using simulation-based inference. This allows for a de-emphasis of traditional probability topics, freeing up room in the syllabus for other topics. Bridges to these mathematical concepts are given as well to help with relation of these traditional topics with more modern approaches. Don’t fence off students from the computation pool, throw them in! Computing skills are essential to working with data in the 21st century. Given this fact, we feel that to shield students from computing is to ultimately do them a disservice. We are not teaching a course on coding/programming per se, but rather just enough of the computational and algorithmic thinking necessary for data analysis. Complete reproducibility and customizability We are frustrated when textbooks give examples, but not the source code and the data itself. We give you the source code for all examples as well as the whole book! While we have made choices to occasionally hide the code that produces more complicated figures, reviewing the book’s GitHub repository will provide you with all the code (see below). Ultimately the best textbook is one you’ve written yourself. You know best your audience, their background, and their priorities. You know best your own style and the types of examples and problems you like best. Customization is the ultimate end. We encourage you to take what we’ve provided and make it work for your own needs. For more about how to make this book your own, see “About this book” later in this Preface. Connect and contribute If you would like to connect with ModernDive, check out the following links: If you would like to receive periodic updates about ModernDive (roughly every 6 months), please sign up for our mailing list. Contact Albert at albert.ys.kim@gmail.com and Chester at chester.ismay@gmail.com. We’re on Twitter at https://twitter.com/ModernDive. If you would like to contribute to ModernDive, there are many ways! We would love your help and feedback to make this book as great as possible! For example, if you find any errors, typos, or areas for improvement, then please email us or post an issue on our GitHub issues page. If you are familiar with GitHub and would like to contribute, see the “About this book” section. Acknowledgements The [original] authors would like to thank Nina Sonneborn, Dr. Alison Hill, Kristin Bott, Dr. Jenny Smetzer, Prof. Katherine Kinnaird, and the participants of our 2017 and 2019 USCOTS workshops for their feedback and suggestions. We’d also like to thank Dr. Andrew Heiss for contributing nearly all of Subsection 1.2.3 on “Errors, warnings, and messages,” Evgeni Chasnovski for creating the new geom_parallel_slopes() extension to the ggplot2 package for plotting parallel slopes models, and Smith College Statistical &amp; Data Sciences students Starry Zhou and Marium Tapal for their many edits to the book. A special thanks goes to Dr. Yana Weinstein, cognitive psychological scientist and co-founder of The Learning Scientists, for her extensive feedback. We were both honored to have Dr. Kelly S. McConville write the Foreword of the book. Dr. McConville is a pioneer in statistics education and was a source of great inspiration to both of us as we continued to update the book to get it to its current form. Thanks additionally to the continued contributions by members of the community to the book on GitHub and to the many individuals that have recommended this book to others. We are so very appreciative of all of you! Lastly, a very special shout out to any student who has ever taken a class with us at either Pacific University, Reed College, Middlebury College, Amherst College, or Smith College. We couldn’t have made this book without you! The adapting author, Dr. Morgan, would like to thank Drs. Ismay and Kim for freely sharing their original textbook in a readily adaptable format, under a Creative Commons Attribution - NonCommercial - ShareAlike 4.0 license (see below). About this book This book was written using RStudio’s bookdown package by Yihui Xie (Xie 2022). This package simplifies the publishing of books by having all content written in R Markdown. The bookdown/R Markdown source code for all versions of ModernDive is available on GitHub: Latest online version The most up-to-date release: Version 2.1.0 released on August 24, 2022 (source code) Available at https://moderndive.com/ Print edition The CRC Press print edition of ModernDive corresponds to Version 1.1.0 (with some typos fixed). Future edition We are currently working on a future edition of ModernDive: A preview is available at https://moderndive.netlify.app/. Source code: Available on ModernDive’s GitHub repository page in the v2 branch https://github.com/moderndive/ModernDive_book/tree/v2. Previous online versions Older versions that may be out of date: Version 1.0.0 released on November 25, 2019 (source code) Version 0.6.1 released on August 28, 2019 (source code) Version 0.6.0 released on August 7, 2019 (source code) Version 0.5.0 released on February 24, 2019 (source code) Version 0.4.0 released on July 21, 2018 (source code) Version 0.3.0 released on February 3, 2018 (source code) Version 0.2.0 released on August 2, 2017 (source code) Version 0.1.3 released on February 9, 2017 (source code) Version 0.1.2 released on January 22, 2017 (source code) Could this be a new paradigm for textbooks? Instead of the traditional model of textbook companies publishing updated editions of the textbook every few years, we apply a software design influenced model of publishing more easily updated versions. We can then leverage open-source communities of instructors and developers for ideas, tools, resources, and feedback. As such, we welcome your GitHub pull requests. Finally, since this book is under a Creative Commons Attribution - NonCommercial - ShareAlike 4.0 license, feel free to modify the book as you wish for your own non-commercial needs, but please list the authors at the top of index.Rmd as: “Chester Ismay, Albert Y. Kim, and YOU!” References "],["about-the-authors.html", "About the authors", " About the authors Chester Ismay Albert Y. Kim Chester Ismay is a Data Science Evangelist at DataRobot in Portland, OR, USA. In this role, he leads data science, machine learning, and data engineering in-person workshops for DataRobot University. He completed his PhD in statistics from Arizona State University in 2013. He has previously worked in a variety of roles including as an actuary at Scottsdale Insurance Company (now Nationwide E&amp;S/Specialty), as a freelance data science consultant, and at Ripon College, Reed College, and Pacific University. In addition to his work for ModernDive, he also contributed as initial developer of the infer R package and is author and maintainer of the thesisdown R package. Email: chester.ismay@gmail.com Webpage: https://chester.rbind.io/ Twitter: old_man_chester GitHub: https://github.com/ismayc Albert Y. Kim is an Assistant Professor of Statistical &amp; Data Sciences at Smith College in Northampton, MA, USA. He completed his PhD in statistics at the University of Washington in 2011. Previously he worked in the Search Ads Metrics Team at Google Inc. as well as at Reed, Middlebury, and Amherst Colleges. In addition to his work for ModernDive, he is a co-author of the resampledata and SpatialEpi R packages. Email: albert.ys.kim@gmail.com Webpage: https://rudeboybert.rbind.io/ Twitter: rudeboybert GitHub: https://github.com/rudeboybert Both Drs. Ismay and Kim, along with Jennifer Chunn, are co-authors of the fivethirtyeight package of code and datasets published by the data journalism website FiveThirtyEight.com. "],["about-the-adapting-author.html", "About the adapting author", " About the adapting author William R. Morgan William R. Morgan is the Theron L. Peterson and Dorothy R. Peterson Professor of Biology at The College of Wooster in Wooster, OH, USA. He teaches courses in introductory biology, research skills for life scientists, genetics and genomics, and computational genomics. Using genomic and bioinformatics approaches, Dr. Morgan and his undergraduate research students investigate how plant pathogens infect host plants at the molecular level. As a member of the NIBLSE leadership team, Dr. Morgan has established a network of educators seeking to integrate bioinformatics into life science education. Email: wmorgan@wooster.edu Webpage: https://wooster.edu/bios/wmorgan/ GitHub: https://github.com/wmorgan485 "],["1-getting-started.html", "Chapter 1 Getting Started with Data in R 1.1 What are R and RStudio? 1.2 How do I code in R? 1.3 What are R packages? 1.4 Explore your first data sets 1.5 Importing data 1.6 Conclusion", " Chapter 1 Getting Started with Data in R Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? We’ll introduce these concepts in the upcoming Sections 1.1-1.3. If you are already somewhat familiar with these concepts, feel free to skip to Section 1.4 where we’ll introduce our first biological dataset with information on 39 species of mammals [see http://www.statsci.org/data/general/sleep.html]. Chapter Learning Objectives At the end of this chapter, you should be able to… • Use RStudio to enter and run code. • Use the following terms: data frame, vector, data frame, function/command, argument. • Distinguish between R messages, warnings, and errors – and respond accordingly. • Learn coding by copying, pasting and tweaking existing code. • Install and load R packages of functions and data. • Do computational tasks by using R functions. • View a data frame in R. • Import data from external sources, including.csv and .xlsx files, into R. • Recognize quantitative (numerical) and categorical (factor) variables. • Access help files in R. 1.1 What are R and RStudio? Throughout this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest, R is like a car’s engine while RStudio is like a car’s dashboard as illustrated in Figure 1.1. FIGURE 1.1: Analogy of difference between R and RStudio. More precisely, R is a programming language that runs computations, while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio’s interface makes using R much easier as well. 1.1.1 Installing R and RStudio Note about RStudio Server or RStudio Cloud: If your instructor has provided you with a link and access to RStudio Server or RStudio Cloud, then you can skip this section. We do recommend after a few months of working on RStudio Server/Cloud that you return to these instructions to install this software on your own computer though. You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio. You must do this first: Download and install R by going to https://cloud.r-project.org/. If you are a Windows user: Click on “Download R for Windows”, then click on “base”, then click on the Download link. If you are macOS user: Click on “Download R for (Mac) OS X”, then under “Latest release:” click on R-X.X.X.pkg, where R-X.X.X is the version number. For example, the latest version of R as of November 25, 2019 was R-3.6.1. If you are a Linux user: Click on “Download R for Linux” and choose your distribution for more information on installing R for your setup. You must do this second: Download and install RStudio at https://www.rstudio.com/products/rstudio/download/. Scroll down to “Installers for Supported Platforms” near the bottom of the page. Click on the download link corresponding to your computer’s operating system. 1.1.2 Using R via RStudio Recall our car analogy from earlier. Much as we don’t drive a car by interacting directly with the engine but rather by interacting with elements on the car’s dashboard, we won’t be using R directly but rather we will use RStudio’s interface. After you install R and RStudio on your computer, you’ll have two new programs (also called applications) you can open. We’ll always work in RStudio and not in the R application. Figure 1.2 shows what icon you should be clicking on your computer. FIGURE 1.2: Icons of R versus RStudio on your computer. After you open RStudio, you should see something similar to Figure 1.3. (Note that slight differences might exist if the RStudio interface is updated after 2019 to not be this by default.) FIGURE 1.3: RStudio interface to R. Note the three panes which are three panels dividing the screen: the console pane, the files pane, and the environment pane. Over the course of this chapter, you’ll come to learn what purpose each of these panes serves. 1.2 How do I code in R? Now that you’re set up with R and RStudio, you are probably asking yourself, “OK. Now how do I use R?”. The first thing to note is that unlike other statistical software programs like Excel, SPSS, or Minitab that provide point-and-click interfaces, R is an interpreted language. This means you have to type in commands written in R code. In other words, you have to code/program in R. Note that we’ll use the terms “coding” and “programming” interchangeably in this book. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that new R users need to understand. Consequently, while this book is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 1.2.1 Basic programming concepts and terminology We now introduce some basic programming concepts and terminology. Instead of asking you to memorize all these concepts and terminology right now, we’ll guide you so that you’ll “learn by doing.” To help you learn, we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, through deliberate practice with R and lots of repetition. Basics: Console pane: where you enter in commands. Running code: the act of telling R to perform an act by giving it commands in the console. Objects: where values are saved in R. We’ll show you how to assign values to objects and how to display the contents of objects. Data types: integers, doubles/numerics, logicals, and characters. Integers are values like -1, 0, 2, 4092. Doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8. Logicals are either TRUE or FALSE while characters are text such as “cabbage”, “Hamilton”, “The Wire is the greatest TV show ever”, and “This ramen is delicious.” Note that characters are often denoted with the quotation marks around them. Vectors: a series of values. These are created using the c() function, where c() stands for “combine” or “concatenate.” For example, c(6, 11, 13, 31, 90, 92) creates a six element series of positive integer values . Factors: categorical data are commonly represented in R as factors. Categorical data can also be represented as strings. We’ll study this difference as we progress through the book. Data frames: rectangular spreadsheets. They are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. We’ll cover data frames later in Section 1.4. Conditionals: Testing for equality in R using == (and not =, which is typically used for assignment). For example, 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). For example, 4 + 2 &gt;= 3 will return TRUE, but 3 + 5 &lt;= 1 will return FALSE. Logical operators: &amp; representing “and” as well as | representing “or.” For example, (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a function’s arguments or use the function’s default values. For example, the function seq() in R generates a sequence of numbers. If you just run seq() it will return the value 1. That doesn’t seem very useful! This is because the default arguments are set as seq(from = 1, to = 1). Thus, if you don’t pass in different values for from and to to change this behavior, R just assumes all you want is the number 1. You can change the argument values by updating the values after the = sign. If we try out seq(from = 2, to = 5) we get the result 2 3 4 5 that we might expect. We’ll work with functions a lot throughout this book and you’ll get lots of practice in understanding their behaviors. To further assist you in understanding when a function is mentioned in the book, we’ll also include the () after them as we did with seq() above. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn’t be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. We feel that you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more. 1.2.2 Errors, warnings, and messages One thing that intimidates new R and RStudio users is how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with “Error in…” and will try to explain what went wrong. Generally when there’s an error, the code will not run. For example, we’ll see in Subsection 1.3.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with “Warning:” and R will try to explain why there’s a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter 2 if you create a scatterplot based on a dataset where two of the rows of data have missing entries that would be needed to create points in the scatterplot, you will see this warning: Warning: Removed 2 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren’t there. Messages: When the red text doesn’t start with either “Error” or “Warning”, it’s just a friendly message. You’ll see these messages when you load R packages in the upcoming Subsection 1.3.2 or when you read data saved in spreadsheet files with the read_csv() function as you’ll see in Section 1.5. These are helpful diagnostic messages and they don’t stop your code from working. Additionally, you’ll see these messages when you install packages too using install.packages() as discussed in Subsection 1.3.1. Remember, when you see red text in the console, don’t panic. It doesn’t necessarily mean anything is wrong. Rather: If the text starts with “Error”, figure out what’s causing it. Think of errors as a red traffic light: something is wrong! If the text starts with “Warning”, figure out if it’s something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you’re fine. If that’s surprising, look at your data and see what’s missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise, the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine and keep on going! 1.2.3 Tips on learning to code Learning to code/program is quite similar to learning a foreign language. It can be daunting and frustrating at first. Such frustrations are common and it is normal to feel discouraged as you learn. However, just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn and improve. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone is “smart,” but really people spent a lot of time and energy designing them to appear “smart.” In reality, you have to tell a computer everything it needs to do. Furthermore, the instructions you give your computer can’t have any mistakes in them, nor can they be ambiguous in any way. Take the “copy, paste, and tweak” approach: Especially when you learn your first programming language or you need to understand particularly complicated code, it is often much easier to take existing code that you know works and modify it to suit your ends. This is as opposed to trying to type out the code from scratch. We call this the “copy, paste, and tweak” approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. After you start feeling more confident, you can slowly move away from this approach and write code from scratch. Think of the “copy, paste, and tweak” approach as training wheels for a child learning to ride a bike. After getting comfortable, they won’t need them anymore. The best way to learn to code is by doing: Rather than learning to code for its own sake, we find that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in and that is important to you. Practice is key: Just as the only method to improve your foreign language skills is through lots of practice and speaking, the only method to improving your coding skills is through lots of practice. Don’t worry, however, we’ll give you plenty of opportunities to do so! 1.3 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package (Wickham, Chang, et al. 2022) for data visualization in Chapter 2, the dplyr package (Wickham, François, et al. 2022) for data wrangling in Chapter 3, the moderndive package (Kim and Ismay 2022) that accompanies this book, and the infer package (Bray et al. 2021) for “tidy” and transparent statistical inference in Chapters 6, 7, and 9. A good analogy for R packages is they are like apps you can download onto a mobile phone: FIGURE 1.4: Analogy of R versus R packages. So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn’t have everything. R packages are like the apps you can download onto your phone from Apple’s App Store or Android’s Google Play. Let’s continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you’re set for the time being. You might need to do this again in the future when there is an update to the app. Open the app: After you’ve installed Instagram, you need to open it. Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you’ve installed a package, you likely won’t install it again unless you want to update it to a newer version. “Load” the package: “Loading” a package is like opening an app on your phone. Packages are not “loaded” by default when you start RStudio on your computer; you need to “load” each package you want to use every time you start RStudio. Let’s perform these two steps for the ggplot2 package for data visualization. 1.3.1 Package installation Note about RStudio Server or RStudio Cloud: If your instructor has provided you with a link and access to RStudio Server or RStudio Cloud, you might not need to install packages, as they might be preinstalled for you by your instructor. That being said, it is still a good idea to know this process for later on when you are not using RStudio Server or Cloud, but rather RStudio Desktop on your own computer. There are two ways to install an R package: an easy way and a more advanced way. Let’s install the ggplot2 package the easy way first as shown in Figure 1.5. In the Files pane of RStudio: Click on the “Packages” tab. Click on “Install” next to Update. Type the name of the package under “Packages (separate multiple with space or comma):” In this case, type ggplot2. Click “Install.” FIGURE 1.5: Installing packages in R the easy way. An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package. Much like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps. Learning check (LC1.1) Repeat the earlier installation steps, but for the following R packages: dplyr, knitr, openintro, airports, cherryblossom, and usdata packages. This will install the earlier mentioned dplyr package for data wrangling, the knitr package for generating easy-to-read tables in R, and the openintro package with the mammals data set that we’ll examine next, as well as a few packages required by openintro. We’ll use these packages in the next section. 1.3.2 Package loading Recall that after you’ve installed a package, you need to “load it.” In other words, you need to “open it.” We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the console pane. What do we mean by “run the following code”? Either type or copy-and-paste the following code into the console pane and then hit the Enter key. library(ggplot2) If after running the earlier code, a blinking cursor returns next to the &gt; “prompt” sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If, however, you get a red “error message” that reads ... Error in library(ggplot2) : there is no package called ‘ggplot2’ ... it means that you didn’t successfully install it. This is an example of an “error message” we discussed in Subsection 1.2.2. If you get this error message, go back to Subsection 1.3.1 on R package installation and make sure to install the ggplot2 package before proceeding. Learning check (LC1.2) “Load” the dplyr, knitr, openintro, airports, cherryblossom, and usdata packages as well by repeating the earlier steps. 1.3.3 Package use One very common mistake new R users make when wanting to use particular packages is they forget to “load” them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you don’t first “load” a package, but attempt to use one of its features, you’ll see an error message similar to: Error: could not find function This is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been “loaded.” R doesn’t know where to find the function you are using. Almost all new users forget to do this when starting out, and it is a little annoying to get used to doing it. However, you’ll remember with practice and after some time it will become second nature for you. 1.4 Explore your first data sets Let’s put everything we’ve learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we’ll focus on datasets that are saved in “spreadsheet”-type format. This is probably the most common way data are collected and saved in many fields. Remember from Subsection 1.2.1 that these “spreadsheet”-type datasets are called data frames in R. We’ll focus on working with data saved as data frames throughout this book. Let’s first load all the packages needed for this chapter. Before you load the following packages, check that they are installed by looking in RStudio’s Packages tab. The openintro package also requires the airports, cherryblossom, and usdata packages, so look for these too. If any of the needed packages are missing, read Section 1.3 for information on how to install R packages. library(openintro) library(dplyr) library(knitr) library(readr) At the beginning of all subsequent chapters in this book, we’ll always have a list of packages that you should have installed and loaded in order to work with that chapter’s R code. 1.4.1 openintro package The openintro package is developed by the OpenIntro project, which provides free educational resources. You can find a list of all included data sets on the package reference page as well as by running data(package = \"openintro\") in the R console after loading the package. 1.4.2 mammals data frame As you will see, there are many ways to access data sets in R. We’ll start with mammals, a dataset built into the openintro R package that contains sleep data and other information about 39 mammalian species. We’ll begin by previewing the mammals data frame. In the Console panel of RStudio, run the following code, either by typing or cutting-and-pasting it. It displays the contents of the mammals data frame in your console. Note that depending on the size of your monitor, the number of displayed columns may vary. mammals # A tibble: 62 × 11 species body_wt brain_wt non_dreaming dreaming total_sleep life_span &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africanelephant 6.65e+3 5712 NA NA 3.3 38.6 2 Africangiantpou… 1 e+0 6.6 6.3 2 8.3 4.5 3 ArcticFox 3.38e+0 44.5 NA NA 12.5 14 4 Arcticgroundsqu… 9.2 e-1 5.7 NA NA 16.5 NA 5 Asianelephant 2.55e+3 4603 2.1 1.8 3.9 69 6 Baboon 1.06e+1 180. 9.1 0.7 9.8 27 7 Bigbrownbat 2.3 e-2 0.3 15.8 3.9 19.7 19 8 Braziliantapir 1.6 e+2 169 5.2 1 6.2 30.4 9 Cat 3.3 e+0 25.6 10.9 3.6 14.5 28 10 Chimpanzee 5.22e+1 440 8.3 1.4 9.7 50 # … with 52 more rows, and 4 more variables: gestation &lt;dbl&gt;, predation &lt;int&gt;, # exposure &lt;int&gt;, danger &lt;int&gt; Let’s unpack this output: A tibble: 62 x 11: A tibble is a specific kind of data frame in R. This particular data frame has 62 rows corresponding to different observations. Here, each observation is a mammalian species. 11 columns corresponding to 11 variables describing each observation. body_wt, brain_wt, non_dreaming, dreaming, etc. are the different columns, in other words, the different variables of this dataset. We then have a preview of the first 10 rows of observations corresponding to the first 10 species. R is only showing the first 10 rows, because if it showed all 62 rows, it would overwhelm your screen. ... with 52 more rows, and 0 more variables: indicating to us that 52 more rows of data and 0 more variables could not fit in this screen. (If you’re screen is wide enough all 11 variables may fit and there will be no more variables to show.) Unfortunately, this output does not allow us to explore the data very well, but it does give a nice preview. Let’s look at some different ways to explore data frames more closely. 1.4.3 Exploring data frames There are many ways to get a feel for the data contained in a complex data frame such as mammals. We present three ways here: The View() function brings up RStudio’s built-in data viewer. The glimpse() function is included in the dplyr package. The $ “extraction operator” is used to view a single variable/column in a data frame. 1. View(): Run View(mammals) in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. (Note the uppercase V in View(). R is case-sensitive, so you’ll get an error message if you run view(mammals) instead.) Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note that there are missing values , which are encoded as NA for “not available” or “not applicable.” If a value for a particular row and a particular column does not exist, NA is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it. Perhaps the data was not collected at all because it was too difficult to do so. Perhaps there was an erroneous value that someone entered that has been corrected to read as missing. You’ll often encounter issues with missing values when working with real data; we’ll describe some ways to deal with the missing values in Chapter 3. Learning check (LC1.3) What does any ONE row in this mammals dataset refer to? A. Data on a species B. Data on a family C. Data on a genus D. Data on mammalian weight Note that if you look in the leftmost column of the View(mammals) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row is representing. This will allow you to identify what object is being described in a given row by taking note of the values of the columns in that specific row. This is often called the observational unit. The observational unit in this example is an individual mammalian species. You can identify the observational unit by determining what “thing” is being measured or described by each of the variables. 2. glimpse(): The second way we’ll cover to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after you’ve loaded the dplyr package by running library(dplyr). This function provides us with an alternative perspective for exploring a data frame that highlights the variables: glimpse(mammals) Rows: 62 Columns: 11 $ species &lt;fct&gt; Africanelephant, Africangiantpouchedrat, ArcticFox, Arcti… $ body_wt &lt;dbl&gt; 6654.000, 1.000, 3.385, 0.920, 2547.000, 10.550, 0.023, 1… $ brain_wt &lt;dbl&gt; 5712.0, 6.6, 44.5, 5.7, 4603.0, 179.5, 0.3, 169.0, 25.6, … $ non_dreaming &lt;dbl&gt; NA, 6.3, NA, NA, 2.1, 9.1, 15.8, 5.2, 10.9, 8.3, 11.0, 3.… $ dreaming &lt;dbl&gt; NA, 2.0, NA, NA, 1.8, 0.7, 3.9, 1.0, 3.6, 1.4, 1.5, 0.7, … $ total_sleep &lt;dbl&gt; 3.3, 8.3, 12.5, 16.5, 3.9, 9.8, 19.7, 6.2, 14.5, 9.7, 12.… $ life_span &lt;dbl&gt; 38.6, 4.5, 14.0, NA, 69.0, 27.0, 19.0, 30.4, 28.0, 50.0, … $ gestation &lt;dbl&gt; 645, 42, 60, 25, 624, 180, 35, 392, 63, 230, 112, 281, NA… $ predation &lt;int&gt; 3, 3, 1, 5, 3, 4, 1, 4, 1, 1, 5, 5, 2, 5, 1, 2, 2, 2, 1, … $ exposure &lt;int&gt; 5, 1, 1, 2, 5, 4, 1, 5, 2, 1, 4, 5, 1, 5, 1, 2, 2, 2, 2, … $ danger &lt;int&gt; 3, 3, 1, 3, 4, 4, 1, 4, 1, 1, 4, 5, 2, 5, 1, 2, 2, 2, 1, … Observe that glimpse() will give you the first few entries of each variable in a row after the variable name. In addition, the data type (see Subsection 1.2.1) of the variable is given immediately after each variable’s name inside &lt; &gt;. Here, int and dbl refer to “integer” and “double”, which are computer coding terms for quantitative/numerical variables. “Doubles” take up twice the size to store on a computer compared to integers. In contrast, fct refers to “factor”, which is computer terminology for categorical data. Categorical data and other text can also be stored as a chr or “character” data type. There are other types of variables in R, some of which we will encounter later. Learning check (LC1.4) Examine the births14 data set in the openintro package. What are examples in this dataset of categorical variables? What makes them different than quantitative variables? 3. $ operator Lastly, the $ operator allows us to extract and then explore a single variable within a data frame. For example, run the following in your console mammals$species We used the $ operator to extract only the species variable and return it as a vector of length 62. We’ll only be occasionally exploring data frames using the $ operator, instead favoring the View() and glimpse() functions. 1.4.4 Identification and measurement variables By running View(mammals), we can explore the different variables listed in the columns. Notice that there are many different types of variables. Some of the variables like body_wt and brain_wt are quantitative variables. These variables are numerical in nature and usually stored as int or dbl data types. As noted above, other variables, such as species, are categorical and stored as a fct data type. The danger variable is also categorical, but is stored as an int. More commonly, categorical variables are stored as a chr data type as seen in the births14 data set that you examined in the previous Learning Check. We can also distinguish between identification variables and measurement variables. For example, in the output of glimpse(mammals) the variable species is what we will call an identification variable, one that uniquely identifies each observational unit. In this case, the identification variable uniquely identifies each mammalian species. Such variables are mainly used in practice to uniquely identify each row in a data frame. The species variable gives the valid scientific name of the mammalian species. The remaining variables (from body_wt to danger) are often called measurement or characteristic variables: variables that describe properties of each observational unit. For example, exposure characterizes how much exposed each mammalian species is during sleep (1, least exposed; 5, most exposed). Furthermore, sometimes a single variable might not be enough to uniquely identify each observational unit: combinations of variables might be needed (e.g., your first and last name on a class roster). While it is not an absolute rule, for organizational purposes it is considered good practice to have your identification variables in the leftmost columns of your data frame. Learning check (LC1.5) Can you guess what properties of each mammalian species are described in the variables body_wt, non_dreaming, and danger of the mammals data frame? 1.4.5 Help files Another nice feature of R are help files, which provide documentation for various functions and datasets. You can bring up help files by adding a ? before the name of a function or data frame and then run this in the console. You will then be presented with a page showing the corresponding documentation if it exists. For example, let’s look at the help file for the mammals data set ?mammals Each help file should pop up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away. However, while these help files are a good starting point, you may often find it necessary to look online for additional assistance. Learning check (LC1.6) Look at the help file for the mammals data frame. Revise your earlier guesses about what the describe. –&gt; 1.5 Importing data Instead of using a data frame built into an R package as we did above, more often you will use your own data saved on your computer or you will access data from an online source. Next we’ll show you how to import spreadsheet data from an external source into R. Spreadsheet data is often saved in one of the three common formats: comma separated values .csv , Excel .xlsx spreadsheet, or Google Sheets. A comma separated values .csv file is a bare-bones spreadsheet where: Each line in the file corresponds to one row of data/one observation. Values for each line are separated with commas. In other words, the values of different variables are separated by commas in each row. The first line is often, but not always, a header row indicating the names of the columns/variables. An Excel .xlsx spreadsheet file is based on Microsoft’s proprietary Excel software. As opposed to bare-bones .csv files, .xlsx Excel files contain a lot of meta-data (data about data). Recall we saw a previous example of meta-data in Section 3.6 when adding “group structure” meta-data to a data frame by using the group_by() verb. Some examples of Excel spreadsheet meta-data include the use of bold and italic fonts, colored cells, different column widths, and formula macros. A Google Sheets file is a “cloud” or online-based way to work with a spreadsheet. Google Sheets allows you to download your data in both comma separated values .csv and Excel .xlsx formats. One way to import Google Sheets data in R is to go to the Google Sheets menu bar -&gt; File -&gt; Download as -&gt; Select “Microsoft Excel” or “Comma-separated values” and then load that data into R. A more advanced way to import Google Sheets data in R is by using the googlesheets package, a method we leave to a more advanced data science book. We’ll cover two methods for importing .csv and .xlsx spreadsheet data in R: one using the console and the other using RStudio’s graphical user interface, abbreviated as “GUI.” 1.5.1 Using the console First, let’s import a Comma Separated Values .csv file that exists on the internet. The .csv file chap12e2BlackbirdTestosterone.csv compares the humoral immunocompetence, measured as secondary antibody production to a protein antigen, of red-winged blackbirds before and after testosterone implants, as described in Hasselquist, D., J. A. Marsh, P. W. Sherman, and J. C. Wingfield. 1999. Behavioral Ecology and Sociobiology 45: 167–175. Let’s use the read_csv() function from the readr (Wickham, Hester, and Bryan 2022) package to read the dataset from the Analysis of Biological Data textbook website, import it into R, and save it in a data frame called blackbird. blackbird &lt;- read_csv(&quot;http://www.zoology.ubc.ca/~schluter/WhitlockSchluter/wp-content/data/chapter12/chap12e2BlackbirdTestosterone.csv&quot;) # A tibble: 13 × 5 blackbird beforeImplant afterImplant logBeforeImplant logAfterImplant &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 105 85 4.65 4.44 2 2 50 74 3.91 4.3 3 3 136 145 4.91 4.98 4 4 90 86 4.5 4.45 5 5 122 148 4.8 5 6 6 132 148 4.88 5 7 7 131 150 4.88 5.01 8 8 119 142 4.78 4.96 9 9 145 151 4.98 5.02 10 10 130 113 4.87 4.73 11 11 116 118 4.75 4.77 12 12 110 99 4.7 4.6 13 13 138 150 4.93 5.01 (If you saw a message about column specifications, you can ignore it.) Note that the read_csv() function included in the readr package is different than the read.csv() function that comes installed with R. While the difference in the names might seem trivial (an _ instead of a .), the read_csv() function is, in our opinion, easier to use since it can more easily read data off the web and generally imports data at a much faster speed. Furthermore, the read_csv() function included in the readr saves data frames as tibbles by default. 1.5.2 Using RStudio’s interface Let’s read in the exact same data, but this time from a file saved on your computer. Furthermore, we’ll do this using RStudio’s graphical interface instead of running read_csv() in the console. First, download the file chap12e2BlackbirdTestosterone.csv by going to &lt;a href=“https://whitlockschluter.zoology.ubc.ca/wp-content/data/chapter12/chap12e2BlackbirdTestosterone.csv, then Go to the Files pane of RStudio. Navigate to the directory (i.e., folder on your computer) where the downloaded chap12e2BlackbirdTestosterone.csv file is saved. For example, this might be in your Downloads folder. Click on chap12e2BlackbirdTestosterone.csv. Click “Import Dataset…” At this point, you should see a screen pop-up like in Figure 1.6. After clicking on the “Import” button on the bottom right of Figure 1.6, RStudio will save this spreadsheet’s data in a data frame called chap12e2BlackbirdTestosterone and display its contents in the spreadsheet viewer. FIGURE 1.6: Importing an Excel file to R. Furthermore, note the “Code Preview” block in the bottom right of Figure 1.6. You can copy and paste this code to reload your data again later programmatically, instead of repeating this manual point-and-click process. Let’s see how. Copy the text in the “Code Preview” block and then create an “R Script” file by selecting RStudio’s “File” menu, “New File” and then “R Script”. Paste your copied text into this new file and then save it as “Chapter1_script”. Later, we’ll show you how to use R scripts to save and run a sequence of commands. 1.6 Conclusion We’ve given you what we feel is a minimally viable set of tools to explore data in R. Does this chapter contain everything you need to know? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn’t be useful! As we said earlier, the best way to add to your toolbox is to get into RStudio and run and write code as much as possible. 1.6.1 Additional resources If you are new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out the short book, Getting Used to R, RStudio, and R Markdown (Ismay and Kennedy 2016). It includes screencast recordings that you can follow along and pause as you learn. This book also contains an introduction to R Markdown, a tool used for reproducible research in R. FIGURE 1.7: Preview of Getting Used to R, RStudio, and R Markdown. 1.6.2 What’s to come? We’re now going to start the “Data Science with tidyverse” portion of this book in Chapter 2 as shown in Figure 1.8 with what we feel is the most important tool in a data scientist’s toolbox: data visualization. We’ll continue to explore the data included in the mammals dataset using the ggplot2 package for data visualization. You’ll see that data visualization is a powerful tool to add to your toolbox for data exploration that provides additional insight to what the View() and glimpse() functions can provide. FIGURE 1.8: ModernDive for Life Scientists flowchart - on to Part I! References "],["2-viz.html", "Chapter 2 Data Visualization 2.1 The grammar of graphics 2.2 Five named graphs - the 5NG 2.3 5NG#1: Scatterplots 2.4 5NG#2: Linegraphs 2.5 Facets 2.6 5NG#3: Histograms 2.7 5NG#4: Boxplots 2.8 5NG#5: Barplots 2.9 Plots to avoid! 2.10 Conclusion", " Chapter 2 Data Visualization We begin the development of your data science toolbox with data visualization. By visualizing data, we gain valuable insights we couldn’t initially obtain from just looking at the raw data values. We’ll use the ggplot2 package, as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as the grammar of graphics (Wilkinson 2005), developed by Leland Wilkinson. At their most basic, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way to explore the patterns in data, such as the presence of outliers, distributions of individual variables, and relationships between groups of variables. Graphics are designed to emphasize the findings and insights you want your audience to understand. This does, however, require a balancing act. On the one hand, you want to highlight as many interesting findings as possible. On the other hand, you don’t want to include so much information that it overwhelms your audience. As we will see, plots also help us to identify patterns and outliers in our data. We’ll see that a common extension of these ideas is to compare the distribution of one numerical variable, such as what are the center and spread of the values, as we go across the levels of a different categorical variable. Chapter Learning Objectives At the end of this chapter, you should be able to… • Explore and present data using plots (a.k.a., charts and graphics) in R. • Use the grammar of graphics to create basic plots by specifying the data, its mapping and the geometric object(s). • Produce more complex plots by adding layers to modify labels, add facets, etc. • Adjust plot features such as colors and bin number by modifying function arguments (inputs). • Select and create the most appropriate graph to visualize your data. • Recognize and correct difficulties in visualizing data due to overplotting. • Interpret data presented in histograms and boxplots. • Explain the problems with using bar charts to present quantitative data and pie charts to present categorical data. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Read Section 1.3 for information on how to install and load R packages. library(openintro) library(ggplot2) library(dplyr) 2.1 The grammar of graphics We start with a discussion of a theoretical framework for data visualization known as “the grammar of graphics.” This framework serves as the foundation for the ggplot2 package which we’ll use extensively in this chapter. Think of how we construct and form sentences in English by combining different elements, like nouns, verbs, articles, subjects, objects, etc. We can’t just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, “the grammar of graphics” defines a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (Wilkinson 2005) and has been implemented in a variety of data visualization software platforms like R, but also Plotly and Tableau. 2.1.1 Components of the grammar In short, the grammar tells us that: A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. Specifically, we can break a graphic into the following three essential components: data: the dataset containing the variables of interest. geom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars. aes: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are mapped to variables in the dataset. You might be wondering why we wrote the terms data, geom, and aes in a computer code type font. We’ll see very shortly that we’ll specify the elements of the grammar in R using these terms. However, let’s first break down the grammar with an example. 2.1.2 Gapminder data In February 2006, a Swedish physician and data advocate named Hans Rosling gave a TED talk titled “The best stats you’ve ever seen” where he presented global economic, health, and development data from the website gapminder.org. For example, for data on 142 countries in 2007, let’s consider only a few countries in Table 2.1 as a peek into the data. TABLE 2.1: Gapminder 2007 Data: First 3 of 142 countries Country Continent Life Expectancy Population GDP per Capita Afghanistan Asia 43.8 31889923 975 Albania Europe 76.4 3600523 5937 Algeria Africa 72.3 33333216 6223 Each row in this table corresponds to a country in 2007. For each row, we have 5 columns: Country: Name of country. Continent: Which of the five continents the country is part of. Note that “Americas” includes countries in both North and South America and that Antarctica is excluded. Life Expectancy: Life expectancy in years. Population: Number of people living in the country. GDP per Capita: Gross domestic product (in US dollars). Now consider Figure 2.1, which plots this for all 142 of the data’s countries. FIGURE 2.1: Life expectancy over GDP per capita in 2007. Let’s view this plot through the grammar of graphics: The data variable GDP per Capita gets mapped to the x-position aesthetic of the points. The data variable Life Expectancy gets mapped to the y-position aesthetic of the points. The data variable Population gets mapped to the size aesthetic of the points. The data variable Continent gets mapped to the color aesthetic of the points. We’ll see shortly that data corresponds to the particular data frame where our data is saved and that “data variables” correspond to particular columns in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. We can also use lines, bars, and other geometric objects. Let’s summarize the three essential components of the grammar in Table 2.2. TABLE 2.2: Summary of the grammar of graphics for this plot data variable aes geom GDP per Capita x point Life Expectancy y point Population size point Continent color point 2.1.3 Other components There are other components of the grammar of graphics we can control as well. As you start to delve deeper into the grammar of graphics, you’ll start to encounter these topics more frequently. In this book, we’ll keep things simple and only work with these two additional components: faceting breaks up a plot into several plots split by the values of another variable (Section 2.5) position adjustments for barplots (Section 2.8) Other more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (Grolemund and Wickham 2017). Generally speaking, the grammar of graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them. 2.1.4 ggplot2 package In this book, we will use the ggplot2 package for data visualization, which is an implementation of the grammar of graphics for R (Wickham, Chang, et al. 2022). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the grammar of graphics are specified in the ggplot() function included in the ggplot2 package. For the purposes of this book, we’ll always provide the ggplot() function with the following arguments (i.e., inputs) at a minimum: The data frame where the variables exist: the data argument. The mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved. After we’ve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets (which we’ll see in Section 2.5). Let’s now put the theory of the grammar of graphics into practice. 2.2 Five named graphs - the 5NG In order to keep things simple in this book, we will only focus on five different types of graphics, each with a commonly given name. We term these “five named graphs” or in abbreviated form, the 5NG: scatterplots linegraphs histograms boxplots barplots We’ll also present some variations of these plots, but with this basic repertoire of five graphics in your toolbox, you can visualize a wide array of different variable types. As we’ll see, certain plots are only appropriate for categorical variables, while others are only appropriate for numerical variables. 2.3 5NG#1: Scatterplots The simplest of the 5NG are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, let’s view them through the lens of the grammar of graphics we presented in Section 2.1. Specifically, we will visualize the relationship between the following two numerical variables in the mammals data frame we created in Chapter 1.4: brain_wt: brain weight (in kg) on the horizontal “x” axis and life_span: total number of hours of sleep on the vertical “y” axis for each mammalian species. 2.3.1 Scatterplots via geom_point Let’s now go over the code that will create the desired scatterplot, while keeping in mind the grammar of graphics framework we introduced in Section 2.1. Let’s take a look at the code and break it down piece-by-piece. ggplot(data = mammals, mapping = aes(x = brain_wt, y = life_span)) + geom_point() Within the ggplot() function, we specify two of the components of the grammar of graphics as arguments (i.e., inputs): The data as the mammals data frame via data = mammals. The aesthetic mapping by setting mapping = aes(x = brain_wt, y = life_span). Specifically, the variable brain_wt maps to the x position aesthetic, while the variable life_span maps to the y position. We then add a layer to the ggplot() function call using the + sign. The added layer in question specifies the third component of the grammar: the geometric object. In this case, the geometric object is set to be points by specifying geom_point(). After running these two lines of code in your console, you’ll notice two outputs: a warning message and the graphic shown in Figure 2.2. Warning: Removed 4 rows containing missing values (geom_point). FIGURE 2.2: Brain weight vs. life span in mammals data frame. Let’s first unpack the graphic in Figure 2.2. Observe that a positive relationship exists overall between brain_wt and life_span: as brain_wt increases, the life_span tends to also increase. Observe also the large number of points clustered near (0, 0), which we’ll discuss further in Subsection 2.3.2. Now, let’s turn our attention to the warning message. R is alerting us to the fact that some rows were ignored due to missing information. For these rows, either the value for brain_wt or life_span or both were missing, and thus these rows were ignored in our plot. Before we continue, let’s make a few more observations about this code that created the scatterplot. Note that the + sign comes at the end of lines, and not at the beginning. You’ll get an error in R if you put it at the beginning of a line. When adding layers to a plot, you are encouraged to start a new line after the + (by pressing the Return/Enter button on your keyboard) so that the code for each layer is on a new line. As we add more and more layers to plots, you’ll see this will greatly improve the legibility of your code. To stress the importance of adding the layer specifying the geometric object, consider Figure 2.3 where no layers are added. Because the geometric object was not specified, we have a blank plot which is not very useful! ggplot(data = mammals, mapping = aes(x = brain_wt, y = life_span)) FIGURE 2.3: A plot with no layers. Learning check (LC2.1) What are some practical reasons why brain_wt and life_span have a positive relationship? (LC2.2) For which of these two variables, brain_wt and life_span, is the data more frequently missing? How did you determine this? (LC2.3) What are some other features of the plot that stand out to you? (LC2.4) Create a new scatterplot using different quantitative variables in the mammals data frame by modifying the example given. 2.3.2 Overplotting The large mass of points near (0, 0) in Figure 2.2 can cause some confusion since it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to points being plotted on top of each other over and over again. When overplotting occurs, it is difficult to know the number of points being plotted. There are two methods to address the issue of overplotting. Either by Adjusting the transparency of the points or Adding a little random “jitter”, or random “nudges”, to each of the points. Method 1: Changing the transparency The first way of addressing overplotting is to change the transparency/opacity of the points by setting the alpha argument in geom_point(). We can change the alpha argument to be any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1. In other words, if we don’t explicitly set an alpha value, R will use alpha = 1. Note how the following code is identical to the code in Section 2.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point() function: ggplot(data = mammals, mapping = aes(x = brain_wt, y = life_span)) + geom_point(alpha = 0.2) FIGURE 2.4: Brain weight vs. life span scatterplot with alpha = 0.2. The key feature to note in Figure 2.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, you’ll receive an error if you try to change the second line to read geom_point(aes(alpha = 0.2)). Method 2: Jittering the points The second way of addressing overplotting is by jittering all the points. This means giving each point a small “nudge” in a random direction. You can think of “jittering” as shaking the points around a bit on the plot. Let’s illustrate using a simple example first. Say we have a data frame with 4 identical rows of x and y values: (0,0), (0,0), (0,0), and (0,0). In Figure 2.5, we present both the regular scatterplot of these 4 points (on the left) and its jittered counterpart (on the right). FIGURE 2.5: Regular and jittered scatterplot. In the left-hand regular scatterplot, observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. In the right-hand jittered scatterplot, it is now plainly evident that this plot involves four points since each point is given a random “nudge.” Keep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged. To create a jittered scatterplot, instead of using geom_point(), we use geom_jitter(). Observe how the following code is very similar to the code that created the scatterplot with overplotting in Subsection 2.3.1, but with geom_point() replaced with geom_jitter(). ggplot(data = mammals, mapping = aes(x = brain_wt, y = life_span)) + geom_jitter(width = 30, height = 1) FIGURE 2.6: Brain weight vs. life span jittered scatterplot. In order to specify how much jitter to add, we adjusted the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. How much jitter should we add using the width and height arguments? On the one hand, it is important to add just enough jitter to break any overlap in points, but on the other hand, not so much that we completely alter the original pattern in points. As can be seen in the resulting Figure 2.6, in this case jittering doesn’t really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting alpha proved more effective. When would it be better to use a jittered scatterplot? When would it be better to alter the points’ transparency? There is no single right answer that applies to all situations. You need to make a subjective choice and own that choice. At the very least when confronted with overplotting, however, we suggest you make both types of plots and see which one better emphasizes the point you are trying to make. Learning check (LC2.5) Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? (LC2.6) After viewing Figure 2.4, give an approximate range of brain weights and life spans that occur most frequently in mammalian species. How has that region changed compared to when you observed the same plot without alpha = 0.2 set in Figure 2.2? 2.3.3 Summary Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one numerical variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful! With medium to large datasets, you may need to play around with the different modifications to scatterplots, such as changing the transparency/opacity of the points or by jittering the points. This tweaking is often a fun part of data visualization, since you’ll have the chance to see different relationships emerge as you tinker with your plots. 2.4 5NG#2: Linegraphs The next of the five named graphs are linegraphs. Linegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory variable, is of a sequential nature. In other words, there is an inherent ordering to the variable. The most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots. Let’s illustrate linegraphs using a dataset in the datasets package included in base R: the ChickWeight data frame. Let’s explore the ChickWeight data frame by running View(ChickWeight) and glimpse(ChickWeight). Furthermore let’s read the associated help file by running ?ChickWeight to bring up the help file. Observe that there is a variable called weight which records the weight (in grams) for the first 21 days of 50 new born chicks fed one of four different diets (1-4). However, for simplicity let’s focus on weights of Chick 1. To focus on Chick 1, we will use dplyr's filter() function to choose the subset of rows in ChickWeight corresponding to Chick 1. Recall from Section 1.2 that testing for equality is specified with == and not =. If you don’t fully understand the code, don’t worry for now; we’ll cover the filter function in more depth in Chapter 3 on data wrangling. chick1_weight &lt;- filter(ChickWeight, Chick == 1) Learning check (LC2.7) Take a look at both the ChickWeight and chick1_weight data frames by running View(ChickWeight) and View(chick1_weight). In what respect do these data frames differ? (LC2.8) Glimpse() the ChickWeight data frame again. Notice that two of the variables use a data type that you have seen previously. What are the data types? (LC2.9) One of the ChickWeight variables uses a data type that you have not seen previously. What abbreviation is used for the new data type? These ord and fct data types are used in R for categorical variables, when only a subset of options or “levels” are available. The &lt;ord&gt; or “ordered factor” data type is used when the different levels of the categorical variable are in a specific order, such as Small, Medium, and Large, etc.. The &lt;fct&gt; or (unordered) “factor” is used when the levels are not in a specific order, such as Red, Blue, Green, etc. 2.4.1 Linegraphs via geom_line Let’s create a time series plot of the body weights saved in the chick1_weight data frame by using geom_line() to create a linegraph, instead of using geom_point() like we used previously to create scatterplots: ggplot(data = chick1_weight, mapping = aes(x = Time, y = weight)) + geom_line() FIGURE 2.7: body weights of Chick 1 on Diet 1 Much as with the ggplot() code that created the scatterplot of brain weight and life span for mammals in Figure 2.2, let’s break down this code piece-by-piece in terms of the grammar of graphics: Within the ggplot() function call, we specify two of the components of the grammar of graphics as arguments: The data to be the chick1_weight data frame by setting data = chick1_weight. The aesthetic mapping by setting mapping = aes(x = Time, y = weight). Specifically, the variable Time maps to the x position aesthetic, while the variable weight maps to the y position aesthetic. We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object. In this case, the geometric object is a line set by specifying geom_line(). Learning check (LC2.10) Why should linegraphs be avoided when there is not a clear ordering of the x-axis? (LC2.11) Why are linegraphs frequently used when time is the explanatory variable on the x-axis? 2.4.2 Summary Linegraphs, just like scatterplots, display the relationship between two numerical variables. However, it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e., the explanatory variable) has an inherent ordering, such as time or altitude. 2.5 Facets Before continuing with the next of the 5NG, let’s briefly introduce a new concept called faceting. Faceting is used when we’d like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ. For example, suppose we were interested in comparing the timeline of weight gain among the 20 chicks fed Diet 1. We could “duplicate” the linegraph above for Chick 1 for the other 19 chicks on Diet 1. In other words, we would plot linegraphs of weight for each Chick on Diet 1 separately. First we create a new data frame by filtering ChickWeight for rows with Chicks on Diet 1: Diet1_ChickWeight &lt;- filter(ChickWeight, Diet == 1) Now we make a linegraph for each chick on Diet 1 by adding facet_wrap(~ Chick) layer. Note the ~ is a “tilde” and can generally be found on the key next to the “1” key on US keyboards. The tilde is required and you’ll receive an error message if you don’t include it here. ggplot(data = Diet1_ChickWeight, mapping = aes(x = Time, y = weight)) + geom_line() + facet_wrap(~ Chick) FIGURE 2.8: Faceted histogram of body weights by chicks on Diet 1 We can also specify the number of rows and columns in the grid by using the nrow and ncol arguments inside of facet_wrap(). For example, say we would like our faceted histogram to have 5 rows instead of 4. We simply add an nrow = 5 argument to facet_wrap(~ Chick). ggplot(data = Diet1_ChickWeight, mapping = aes(x = Time, y = weight)) + geom_line() + facet_wrap(~ Chick, nrow = 5) FIGURE 2.9: Faceted histogram with 5 instead of 4 rows. Observe in both Figures 2.8 and 2.9 that as we would expect chick weights increased with time. Learning check (LC2.12) What do the numbers above each plot correspond to? What about the numbers below the bottom plots? (LC2.13) What other things do you notice about this faceted plot? (LC2.14) Does the rate of weight gain over time in the chick_weight_d21 dataset have a lot of variability? Why do you say that? (LC2.15) How does a faceted plot help us see relationships between two variables? (LC2.16) For which types of datasets would faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics. 2.6 5NG#3: Histograms Let’s consider the weight variable in the ChickWeight data frame once again, but unlike with the linegraphs in Section 2.4, let’s say we don’t care about its relationship with time, but rather we only care about how the values of weight distribute in 3-week old chicks. In other words: What are the smallest and largest values? What is the “center” or “most typical” value? How do the values spread out? What are frequent and infrequent values? First, we filter the ChickWeight data frame for the desired rows, each chick’s weight at Day 21: chick_weight_d21 &lt;- filter(ChickWeight, Time == 21) One way to visualize the distribution of this single variable weight is to plot the weights on a horizontal line as we do in Figure 2.10: FIGURE 2.10: Plot of body weights for Chick 1 This gives us a general idea of how the values of weight distribute: observe that weights vary from around 70 grams up to 370 grams. Furthermore, there appear to be more recorded weights between 150 g and 250 g than outside this range. However, because of the high degree of overplotting in the points, it’s hard to get a sense of exactly how many values are between say 150 g and 200 g. What is commonly produced instead of Figure 2.10 is known as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows: We first cut up the x-axis into a series of bins, where each bin represents a range of values. For each bin, we count the number of observations that fall in the range corresponding to that bin. Then for each bin, we draw a bar whose height marks the corresponding count. Let’s drill-down on an example of a histogram, shown in Figure 2.11. FIGURE 2.11: Example histogram. Let’s focus only on weights between 100 g and 200 g for now. Observe that there are five bins of equal width between 100 g and 200 g. Thus we have five bins of width 20 g each: one bin for the 100-120 g range, another bin for the 120-140 g range, etc. Since: The bins for the 100-120 g and 120-140 g ranges each have a height of 1. In other words, one of the Day 21 weight recordings is between 100-120 g and another one is between 120-140 g. The bin for the 140-160 g range has a height of 6. In other words, six of the Day 21 weight recordings are between 140-160 g. All 16 bins spanning 60 g to 380 g on the x-axis have this interpretation. 2.6.1 Histograms via geom_histogram Let’s now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable weight. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram(). After running the following code, you’ll see the histogram in Figure 2.12 as well as warning messages. We’ll discuss the warning message first. ggplot(data = chick_weight_d21, mapping = aes(x = weight)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. FIGURE 2.12: Histogram of chick weights at day 21. The warning message is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We’ll see in the next section how to change the number of bins to another value than the default. Now let’s unpack the resulting histogram in Figure 2.12. Observe that values less than 150 g as well as values above 325 g are rather rare. However, because of the large number of bins, it’s hard to get a sense for which range of weights is spanned by each bin; everything is one giant amorphous blob. So let’s add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram() and ignore the warning about setting the number of bins to a better value: ggplot(data = chick_weight_d21, mapping = aes(x = weight)) + geom_histogram(color = &quot;white&quot;) FIGURE 2.13: Histogram of chick weights at day 21. We now have an easier time associating ranges of weights to each of the bins in Figure 2.13. We can also vary the color of the bars by setting the fill argument. For example, you can set the bin colors to be “blue steel” by setting fill = \"steelblue\": ggplot(data = chick_weight_d21, mapping = aes(x = weight)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;) If you’re curious, run colors() to see all 657 possible choice of colors in R! 2.6.2 Adjusting the bins Observe in Figure 2.13 that in the 100-200 g range there appear to be about 10 bins. Thus each bin has width 100 divided by about 10, or about 10 g. Let’s improve this histogram by adjusting the number of bins in our histogram in one of two ways: By adjusting the number of bins via the bins argument to geom_histogram(). By adjusting the width of the bins via the binwidth argument to geom_histogram(). Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 20 bins, as follows: ggplot(data = chick_weight_d21, mapping = aes(x = weight)) + geom_histogram(bins = 20, color = &quot;white&quot;) Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, let’s set the width of each bin to be 20 g. ggplot(data = chick_weight_d21, mapping = aes(x = weight)) + geom_histogram(binwidth = 20, color = &quot;white&quot;) We compare both resulting histograms side-by-side in Figure 2.14. FIGURE 2.14: Setting histogram bins in two ways. Learning check (LC2.17) What does changing the number of bins from 30 to 20 tell us about the distribution of weights? (LC2.18) Would you classify the distribution of weights as symmetric or skewed in one direction or another? (LC2.19) What would you guess is the “center” value in this distribution? Why did you make that choice? (LC2.20) Is this data spread out greatly from the center or is it close? Explain your answer. 2.6.3 Summary Histograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question. 2.7 5NG#4: Boxplots While faceted histograms are one type of visualization used to compare the distribution of a numerical variable split by the values of another variable, another type of visualization that achieves this same goal is a side-by-side boxplot. A boxplot is constructed from the information provided in the five-number summary of a numerical variable: minimum, first quartile, median, third quartile, and maximum values (see Appendix A.1). To keep things simple for now, let’s only consider the Day 21 weight recordings for the 16 chicks on Diet 1, each represented as a jittered point in Figure 2.15. FIGURE 2.15: Day 21 weights of Diet 1 chicks represented as jittered points. These 16 observations have the following five-number summary: Minimum: 96 g First quartile (25th percentile): 138 g Median (second quartile, 50th percentile): 166 g Third quartile (75th percentile): 208 g Maximum: 305 g In the leftmost plot of Figure 2.16, let’s mark these 5 values with dashed horizontal lines on top of the 16 points. In the middle plot of Figure 2.16 let’s add the boxplot. In the rightmost plot of Figure 2.16, let’s remove the points and the dashed horizontal lines for clarity’s sake. FIGURE 2.16: Building up a boxplot of Diet 1 chick weights. What the boxplot does is visually summarize the 16 points by cutting the 16 weight recordings into quartiles at the dashed lines, where each quartile contains roughly 16 \\(\\div\\) 4 \\(\\approx\\) 4 observations. Thus 25% of points fall below the bottom edge of the box, which is the first quartile of 138 g. In other words, 25% of observations were below 138 g. 25% of points fall between the bottom edge of the box and the solid middle line, which is the median of 166 g. Thus, 25% of observations were between 138 g and 166 g and 50% of observations were below 166 g. 25% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 208 g. It follows that 25% of observations were between 166 g and 208 g and 75% of observations were below 208 g. 25% of points fall above the top edge of the box. In other words, 25% of observations were above 208 g. The middle 50% of points lie within the interquartile range (IQR) between the first and third quartile. Thus, the IQR for this example is 208 - 138 = 70 g. The interquartile range is a measure of a numerical variable’s spread. Furthermore, in the rightmost plot of Figure 2.16, we see the whiskers of the boxplot. The whiskers stick out from either end of the box all the way to the minimum and maximum observed weights of 96 g and 305 g, respectively. However, the whiskers don’t always extend to the smallest and largest observed values as they do here. They in fact extend no more than 1.5 \\(\\times\\) the interquartile range from either end of the box. In this case of the Diet 1 weights, no more than 1.5 \\(\\times\\) 70 g = 105 g from either end of the box. Any observed values outside this range get marked with points called outliers, which we’ll see in the next section. 2.7.1 Boxplots via geom_boxplot Let’s now create a side-by-side boxplot of Day 21 weights split by the four diets. We do this by first filtering for the rows where Time equals 21 days. We then plot these values, mapping the Diet variable to the x-position aesthetic, the weight variable to the y-position aesthetic, and adding a geom_boxplot() layer: Time21_ChickWeight &lt;- ChickWeight %&gt;% filter(Time == 21) ggplot(data = Time21_ChickWeight, mapping = aes(x = Diet, y = weight)) + geom_boxplot() FIGURE 2.17: Side-by-side boxplot of weight split by Diet. Note that boxplots require a categorical – not a numerical – variable to be mapped to the x-position aesthetic. Although the Diets are numbered, recall that this variable is stored as a factor (&lt;fct&gt;) data type, which you can confirm using glimpse(ChickWeight). So R knows that this is a categorical variable on the x-position aesthetic. If the variable Diet had been stored as a &lt;num&gt; data type instead, we would have needed to convert it into a categorical variable by using the factor() function. The resulting Figure 2.17 shows four separate “box and whiskers” plots similar to the rightmost plot of Figure 2.16 of only Diet 1 weights. Thus the different boxplots are shown “side-by-side.” The “box” portions of the visualization represent the 1st quartile, the median (the 2nd quartile), and the 3rd quartile. The height of each box (the value of the 3rd quartile minus the value of the 1st quartile) is the interquartile range (IQR). It is a measure of the spread of the middle 50% of values, with longer boxes indicating more variability. The “whisker” portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles, respectively. They’re set to extend out no more than \\(1.5 \\times IQR\\) units away from either end of the boxes. We say “no more than” because the ends of the whiskers have to correspond to observed weights. The length of these whiskers show how the data outside the middle 50% of values vary, with longer whiskers indicating more variability. If any values had been more than \\(1.5 \\times IQR\\) away from either box end and therefore beyond the whiskers, then they would have been indicated by dots. These so-called outliers can be thought of as anomalous (“out-of-the-ordinary”) values. It is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In these boxplots, they are defined by the length of the whiskers, which are no more than \\(1.5 \\times IQR\\) units long for each boxplot. Looking at this side-by-side plot we can see that Diet 3 produced chicks with the highest median weight at Day 21 as evidenced by the higher solid line in the middle of its box. We can easily compare weights across diets by drawing imaginary horizontal lines across the plot. Furthermore, the heights of the four boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of weights recorded for a given diet. Learning check (LC2.21) Which diet produced the greatest variability in weights? Explain your reasoning. (LC2.22) Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram? 2.7.2 Summary Side-by-side boxplots provide us with a way to compare the distribution of a numerical variable across multiple values of another variable. One can see where the median falls across the different groups by comparing the solid lines in the center of the boxes. To study the spread of a numerical variable within one of the boxes, look at both the length of the box and also how far the whiskers extend from either end of the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with distinct points. 2.8 5NG#5: Barplots Both histograms and boxplots are tools to visualize the distribution of numerical variables. Another commonly desired task is to visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with barplots (also called barcharts). One complication, however, is how your data is represented. Is the categorical variable of interest “pre-counted” or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges. fruits &lt;- tibble( fruit = c(&quot;apple&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;apple&quot;, &quot;orange&quot;) ) fruits_counted &lt;- tibble( fruit = c(&quot;apple&quot;, &quot;orange&quot;), number = c(3, 2) ) We see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually… # A tibble: 5 × 1 fruit &lt;chr&gt; 1 apple 2 apple 3 orange 4 apple 5 orange … fruits_counted has a variable count which represent the “pre-counted” values of each fruit. # A tibble: 2 × 2 fruit number &lt;chr&gt; &lt;dbl&gt; 1 apple 3 2 orange 2 Depending on how your categorical data is represented, you’ll need to add a different geometric layer type to your ggplot() to create a barplot, as we now explore. 2.8.1 Barplots via geom_bar or geom_col Let’s generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer: ggplot(data = fruits, mapping = aes(x = fruit)) + geom_bar() FIGURE 2.18: Barplot when counts are not pre-counted. However, using the fruits_counted data frame where the fruits have been “pre-counted”, we once again map the fruit variable to the x-position aesthetic, but here we also map the count variable to the y-position aesthetic, and add a geom_col() layer instead. ggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) + geom_col() FIGURE 2.19: Barplot when counts are pre-counted. Compare the barplots in Figures 2.18 and 2.19. They are identical because they reflect counts of the same five fruits. However, depending on how our categorical data is represented, either “pre-counted” or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize Is not pre-counted in your data frame, we use geom_bar(). Is pre-counted in your data frame, we use geom_col() with the y-position aesthetic mapped to the variable that has the counts. Let’s now go back to the mammals data frame from earlier (Subsection 1.4) and visualize the distribution of the categorical variable predation, which indicates the likelihood of the mammalian species to be preyed upon (1, lowest; 5, highest). In other words, let’s visualize the number of mammalian species in each of the 5 predation categories. Recall from Subsection 1.4.3 when you first explored the mammals data frame, you saw that each row corresponds to a species. In other words, the mammals data frame is more like the fruits data frame than the fruits_counted data frame because the species have not been pre-counted by predation. Thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable predation gets mapped to the x-position. ggplot(data = mammals, mapping = aes(x = predation)) + geom_bar() FIGURE 2.20: Number of species in each predation category using geom_bar(). Observe in Figure 2.20 that predation category 4 has the fewest species. Also notice that bar graphs have white space between the bars whereas histograms have bars that touch. Alternatively, you may have a data frame where the number of mammalian species in each predation category was pre-counted as in Table 2.3. TABLE 2.3: Number of mammalian species pre-counted by predation category. predation number 1 14 2 15 3 12 4 7 5 14 In order to create a barplot visualizing the distribution of the categorical variable predation in this case, we would now use geom_col() instead of geom_bar(), with an additional y = number in the aesthetic mapping on top of the x = predation. The resulting barplot would be identical to Figure 2.20. Learning check (LC2.23) Why are histograms inappropriate for categorical variables? (LC2.24) What is the difference between histograms and barplots? (LC2.25) What was the 2nd most common predation category for the mammalian species? How could we rearrange the table to get this answer more quickly? 2.8.2 Two categorical variables Barplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the joint distribution of two categorical variables at the same time. Let’s examine the joint distribution of mammalian species by predation as well as exposure, (an index of how exposed the mammal is during sleep). In other words, the number of species for each combination of predation and exposure. For example, the number of species with predation category 2 and exposure category. Recall the ggplot() code that created the barplot of predation frequency in Figure 2.20: ggplot(data = mammals, mapping = aes(x = predation)) + geom_bar() We can now map the additional variable exposure by adding fill = factor(exposure) inside the aes() aesthetic mapping. Note that we use the factor() function here because the fill argument is expecting a factor, so we coerced exposure from int to the fct data type. ggplot(data = mammals, mapping = aes(x = predation, fill = factor(exposure))) + geom_bar() FIGURE 2.21: Stacked barplot of mammalian species by predation category and exposure index. Figure 2.21 is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of species with each exposure type between the predation categories. Before we continue, let’s address some common points of confusion among new R users. First, the fill aesthetic corresponds to the color used to fill the bars, while the color aesthetic corresponds to the color of the outline of the bars. This is identical to how we added color to our histogram in Subsection 2.6.1: we set the outline of the bars to white by setting color = \"white\" and the colors of the bars to blue steel by setting fill = \"steelblue\". Observe in Figure 2.22 that mapping exposure to color and not fill yields grey bars with different colored outlines. ggplot(data = mammals, mapping = aes(x = predation, color = factor(exposure))) + geom_bar() FIGURE 2.22: Stacked barplot with color aesthetic used instead of fill. Second, note that fill is another aesthetic mapping much like x-position; thus we were careful to include it within the parentheses of the aes() mapping. The following code, where the fill aesthetic is specified outside the aes() mapping will not produce the desired plot. This is a fairly common error that new ggplot users make: ggplot(data = mammals, mapping = aes(x = predation), fill = factor(exposure)) + geom_bar() An alternative to stacked barplots are side-by-side barplots, also known as dodged barplots, as seen in Figure 2.23. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a position = \"dodge\" argument added to geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead. ggplot(data = mammals, mapping = aes(x = predation, fill = factor(exposure))) + geom_bar(position = &quot;dodge&quot;) FIGURE 2.23: Side-by-side barplot comparing number of species by predation and exposure variables. Another type of graph to visualize two categorical variables at the same time is a faceted barplot. Recall in Section 2.5 we visualized the distribution of weight gain for the chicks on Diet 1 split by the Chick identifier (1-50) using facets. We apply the same principle to our barplot visualizing the frequency of predation split by exposure: instead of mapping exposure to fill, we include it as the variable to create small multiples of the plot across the levels of exposure. ggplot(data = mammals, mapping = aes(x = predation)) + geom_bar() + facet_wrap(~ exposure, ncol = 1) FIGURE 2.24: Faceted barplot comparing the number of species by the predation and exposure variables. Finally, in Section 7.1, we will visualize the joint frequency of two categorical variables at the same time using a mosaic plot. Learning check (LC2.26) What kinds of questions are not easily answered by looking at Figure 2.21? (LC2.27) What can you say, if anything, about the relationship between predation category and exposure index in regards to the number of mammalian species? (LC2.28) Why might the side-by-side barplot be preferable to a stacked barplot in this case? (LC2.29) What are the disadvantages of using a dodged barplot, in general? (LC2.30) Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case? (LC2.31) What information about the different exposure indexes of different predation categories is more easily seen in the faceted barplot? 2.8.3 Summary Barplots are a common way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories (also called levels) occur. They are easy to understand and make it easy to make comparisons across levels. Furthermore, when trying to visualize the relationship of two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the relationship you are trying to emphasize, you will need to make a choice between these three types of barplots and own that choice. 2.9 Plots to avoid! When we visualize data, the goal is to display the key features as clearly and succinctly as possible. These key features may include the central location of our data (e.g., median), its spread (e.g., IQR), and the sample size of each group. Depending on the data to display (numerical/categorical, one/multiple variables), some plots are more effective at presenting the key features of the data. In this section, we describe two plots that are frequently seen in the scientific literature but are less effective than common alternatives and should therefore be avoided. 2.9.1 Barplots with a continuous variable The first plot to avoid is a bar plot to display the distribution of a continuous (numerical) variable. But wait, then why was Section 2.8 all about how to create bar plots? Recall from the opening paragraph of Section 2.8 that barplots should be used to visualize the distribution of categorical variables, while histograms and boxplots should be used to visualize the distribution of numerical variables. Despite this, a recent study (Riedel et al. 2022) found that bar plots are more often used to inappropriately display continuous data than they are used appropriately to display count data. So, what is the problem with using barplots to display continuous data? As shown in Figure 2.25, barplots hide important information that can be easily indicated using a scatterplot (dotplot) or boxplot. While the bar graph displays the mean of the data and some indication of its spread, it hides much of the data (zone of invisibility). In contrast, the dotplot succinctly presents the complete distribution of the numerical variable and indicates the sample size of each group. Furthermore, the bar graph includes a lot of “dead space” (zone of irrelevance) distracting our attention from the difference between means compared to the spread. For these reasons, more and more scientific journals are banning the use of barplots to display continuous data – and you should too. FIGURE 2.25: Bar graphs hide important information about continuous data. Source: (Weissgerber et al. 2017), CC BY 4.0. Learning check (LC2.32) Why should barplots be avoided when displaying continuous data? (LC2.33) Why do you think people continue to use barplots when displaying continuous data 2.9.2 Pie charts One of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book, Creating More Effective Graphs (Robbins 2013), we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another. Let’s examine the same data used in our previous barplot of the number of mammalian species preferring each predation environment in Figure 2.20, but this time we will use a pie chart in Figure 2.26. Try to answer the following questions: How much larger is the portion of the pie for predation category 2 compared to category 4? What is the second most common predation category for mammalian species? FIGURE 2.26: The dreaded pie chart. While it is quite difficult to answer these questions when looking at the pie chart in Figure 2.26, we can much more easily answer these questions using the barchart in Figure 2.20. This is true since barplots present the information in a way such that comparisons between categories can be made with single horizontal lines, whereas pie charts present the information in a way such that comparisons must be made by comparing angles. Learning check (LC2.34) Why should pie charts be avoided and replaced by barplots? (LC2.35) Why do you think people continue to use pie charts? 2.10 Conclusion 2.10.1 Summary table Let’s recap all five of the five named graphs (5NG) in Table 2.4 summarizing their differences. Using these 5NG, you’ll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each geometric object’s aesthetic attribute options, further unlocking the awesome power of the ggplot2 package. TABLE 2.4: Summary of Five Named Graphs Named graph Shows Geometric object Notes 1 Scatterplot Relationship between 2 numerical variables geom_point() 2 Linegraph Relationship between 2 numerical variables geom_line() Used when there is a sequential order to x-variable, e.g., time 3 Histogram Distribution of 1 numerical variable geom_histogram() Like boxplots, facetted histograms show distribution of 1 numerical variable split by the values of a categorical variable 4 Boxplot Distribution of 1 numerical variable split by the values of a categorical variable geom_boxplot() 5 Barplot Distribution of 1 categorical variable geom_bar() when counts are not pre-counted, geom_col() when counts are pre-counted Stacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables 2.10.2 Function argument specification Let’s go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code: # Segment 1: ggplot(data = mammals, mapping = aes(x = predation)) + geom_bar() # Segment 2: ggplot(mammals, aes(x = predation)) + geom_bar() You’ll notice that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() function by default assumes that the data argument comes first and the mapping argument comes second. As long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. Going forward for the rest of this book, all ggplot() code will be like the second segment: with the data = and mapping = explicit naming of the argument omitted with the default ordering of arguments respected. We’ll do this for brevity’s sake; it’s common to see this style when reviewing other R users’ code. 2.10.3 Additional resources If you want to further unlock the power of the ggplot2 package for data visualization, we suggest that you check out RStudio’s “Data Visualization with ggplot2” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter. (For example in Figure 2.20, you saw that we could add code to rotate labels.) In particular, the ggplot2 cheatsheet presents many more than the 5 geometric objects we covered in this chapter while providing quick and easy to read visual descriptions. For all the geometric objects, it also lists all the possible aesthetic attributes one can tweak. In the current version of RStudio, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheat Sheets -&gt; “Data Visualization with ggplot2.” You can see a preview in the figure below. FIGURE 2.27: Data Visualization with ggplot2 cheatsheet. 2.10.4 What’s to come Recall in Figure 2.7 in Section 2.4 we visualized weight gain over time for Chick 1. This necessitated paring down the ChickWeight data frame to a new data frame chick1_weight consisting of body weight recordings only for Chick == 1: chick1_weight &lt;- filter(ChickWeight, Chick == 1) This code segment is a preview of Chapter 3 on data wrangling using the dplyr package. Data wrangling is the process of transforming and modifying existing data with the intent of making it more appropriate for analysis purposes. For example, this code segment used the filter() function to create a new data frame (chick1_weight) by choosing only a subset of rows of an existing data frames (ChickWeight). In the next chapter, we’ll formally introduce the filter() and other data wrangling functions as well as the pipe operator %&gt;% which allows you to combine multiple data wrangling actions into a single sequential chain of actions. On to Chapter 3 on data wrangling! References "],["3-wrangling.html", "Chapter 3 Data Wrangling 3.1 The pipe operator: %&gt;% 3.2 filter rows 3.3 slice rows 3.4 select variables 3.5 summarize variables 3.6 group_by rows 3.7 mutate existing variables 3.8 arrange and sort rows 3.9 Conclusion", " Chapter 3 Data Wrangling So far in our journey, we’ve seen how to look at data saved in data frames (Chapter 1) and how to create data visualizations using the grammar of graphics, which maps variables in a data frame to the aesthetic attributes of geometric objects. Recall that for some of our visualizations, we first needed to transform/modify existing data frames a little. For example, to create the linegraph in Figure 2.7 with measurements only for Chick 1, we first needed to pare down the ChickWeight data frame to a chick1_weight data frame consisting of only Chick == 1 rows. Thus, chick1_weight will have fewer rows than ChickWeight. We did this using the filter() function: chick1_weight &lt;- filter(ChickWeight, Chick == 1) In this chapter, we’ll extend this example and we’ll introduce a series of functions from the dplyr package for data wrangling that will allow you to take a data frame and “wrangle” it (transform it) to suit your needs. Such functions include: filter() and slice a data frame’s existing rows to only pick out a subset of them. For example, the mammals data frame. select() a data frame’s existing columns to only pick out a subset of them or rename existing columns. summarize() one or more of its columns/variables with a summary statistic. Examples of summary statistics include the median and interquartile range of chick weights as we saw in Section 2.7 on boxplots. group_by() its rows. In other words, assign different rows to be part of the same group. We can then combine group_by() with summarize() to report summary statistics for each group separately. For example, say you don’t want a single overall average weight on Day 21 for the chick_weight_d21 dataset, but rather four separate averages, one computed for each of the four Diet groups. mutate() its existing columns/variables to create new ones. For example, convert weight recordings from grams to ounces. arrange() its rows. For example, sort the rows of mammals in ascending or descending order of life_span. Notice how we used computer_code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling has intuitively verb-named functions that are easy to remember. There is a further benefit to learning to use the dplyr package for data wrangling: its similarity to the database querying language SQL (pronounced “sequel” or spelled out as “S”, “Q”, “L”). SQL (which stands for “Structured Query Language”) is used to manage large databases quickly and efficiently and is widely used by many institutions with a lot of data. While SQL is a topic left for a book or a course on database management, keep in mind that once you learn dplyr, you can learn SQL easily. Chapter Learning Objectives At the end of this chapter, you should be able to… • Use pipes to make it easier to create and read complex code. • Use data wrangling functions to rearrange a data frame, retrieve a subset of rows or columns, and create new variables. • Calculate summary statistics, such as mean and median, for separate groups. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 1.3 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(openintro) 3.1 The pipe operator: %&gt;% Before we start data wrangling, let’s first introduce a nifty tool that gets loaded with the dplyr package: the pipe operator %&gt;%. The pipe operator allows us to combine multiple operations in R into a single sequential chain of actions. Let’s start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) This code isn’t so hard to read since we are applying only three functions: f(), then g(), then h() and each of the functions is short in its name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as “then” or “and then.” For example, you can obtain the same output as the hypothetical sequence of functions as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example, the mammals data frame we explored in Section 1.4.2. The sequence of functions, here f(), g(), and h(), will mostly be a sequence of any number of the data wrangling verb-named functions we listed in the introduction to this chapter. For example, the filter(Chick == 1) function and argument we previewed earlier. The result will be the transformed/modified data frame that you want. In our example, we’ll save the result in a new data frame by using the &lt;- assignment operator with the name chick1_weight via chick1_weight &lt;-. chick1_weight &lt;- ChickWeight %&gt;% filter(Chick == 1) Much like when adding layers to a ggplot() using the + sign, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence using the pipe operator %&gt;%. Furthermore, much like how the + sign has to come at the end of lines when constructing plots, the pipe operator %&gt;% has to come at the end of lines as well. Keep in mind, there are many more advanced data wrangling functions than just those listed in the introduction to this chapter. However, just with these verb-named functions you’ll be able to perform a broad array of data wrangling tasks for the rest of this book. 3.2 filter rows FIGURE 3.1: Diagram of filter() rows operation. The filter() function here works much like the “Filter” option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria. We begin by focusing only on mammalian species in predationcategory 1. Run the following and look at the results in RStudio’s spreadsheet viewer to ensure that only species in predationcategory 1 are chosen predation1 &lt;- mammals %&gt;% filter(predation == 1) View(predation1) Note the order of the code. First, take the mammals data frame; then filter() the data frame so that only those where predation equals 1 are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(predation = 1) will yield an error. This is a convention across many programming languages. If you are new to coding, you’ll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other operators beyond just the == operator that tests for equality: &gt; corresponds to “greater than” &lt; corresponds to “less than” &gt;= corresponds to “greater than or equal to” &lt;= corresponds to “less than or equal to” != corresponds to “not equal to.” The ! is used in many programming languages to indicate “not.” Furthermore, you can combine multiple criteria using operators that make comparisons: | corresponds to “or” &amp; corresponds to “and” To see many of these in action, let’s filter mammals for mammalian species that are in predation category 1 or 2 and not in exposure category 1. Note that this example uses the ! “not” operator to pick rows that don’t match a criteria. As mentioned earlier, the ! can be read as “not.” Run the following: predation_notexp1 &lt;- mammals %&gt;% filter((predation == 1 | predation == 2) &amp; exposure != 1) predation_notexp1 # A tibble: 12 × 11 species body_wt brain_wt non_dreaming dreaming total_sleep life_span &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Cat 3.3 25.6 10.9 3.6 14.5 28 2 Echidna 3 25 8.6 0 8.6 50 3 Europeanhedgehog 0.785 3.5 6.6 4.1 10.7 6 4 Galago 0.2 5 9.5 1.2 10.7 10.4 5 Genet 1.41 17.5 4.8 1.3 6.1 34 6 Gorilla 207 406 NA NA 12 39.3 7 Grayseal 85 325 4.7 1.5 6.2 41 8 Owlmonkey 0.48 15.5 15.2 1.8 17 12 9 Raccoon 4.29 39.2 NA NA 12.5 13.7 10 Rhesusmonkey 6.8 179 8.4 1.2 9.6 29 11 Rockhyrax(Heter… 0.75 12.3 5.7 0.9 6.6 7 12 Slowloris 1.4 12.5 NA NA 11 12.7 # … with 4 more variables: gestation &lt;dbl&gt;, predation &lt;int&gt;, exposure &lt;int&gt;, # danger &lt;int&gt; As expected, the predation_notexp1 data frame contains a subset, 12 rows, of the 62 rows in the original mammals data frame. This alternative code where we do not select species that are exposure category 1 achieves the same aim: predation_notexp1 &lt;- mammals %&gt;% filter((predation == 1 | predation == 2) &amp; !exposure == 1) predation_notexp1 Note that even though colloquially speaking one might say “all mammalian species that are predation category 1 and 2,” in terms of computer operations, we really mean “all mammalian species that are predation category 1 or 2.” For a given row in the data, predation can be 1, or 2, or something else, but not both 1 and 2 at the same time. We can often skip the use of &amp; and just separate our conditions with a comma. The following code will return the identical output predation_notexp1 as the previous code: predation_notexp1 &lt;- mammals %&gt;% filter((predation == 1 | predation == 2), !exposure == 1) predation_notexp1 Now say we have a larger number of categories we want to filter for, say a subset of species such as \"Africanelephant\", \"Asianelephant\", \"Deserthedgehog\", and \"Europeanhedgehog\". We could continue to use the | (or) operator: many_species &lt;- mammals %&gt;% filter(species == &quot;Africanelephant&quot; | species == &quot;Asianelephant&quot; | species == &quot;Deserthedgehog&quot; | species == &quot;Europeanhedgehog&quot;) but as we progressively include more categories, this will get unwieldy to write. A slightly shorter approach uses the %in% operator along with the c() function. Recall from Subsection 1.2.1 that the c() function “combines” or “concatenates” values into a single vector of values. # A tibble: 4 × 11 species body_wt brain_wt non_dreaming dreaming total_sleep life_span gestation &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africa… 6.65e+3 5712 NA NA 3.3 38.6 645 2 Asiane… 2.55e+3 4603 2.1 1.8 3.9 69 624 3 Desert… 5.5 e-1 2.4 7.6 2.7 10.3 NA NA 4 Europe… 7.85e-1 3.5 6.6 4.1 10.7 6 42 # … with 3 more variables: predation &lt;int&gt;, exposure &lt;int&gt;, danger &lt;int&gt; This code filters mammals for all mammalian species where species is in the vector of species c(\"Africanelephant\", \"Asianelephant\", \"Deserthedgehog\", \"Europeanhedgehog\"). Both outputs of many_species are the same, but as you can see the latter takes much less energy to code. The %in% operator is useful for looking for matches commonly in one vector/variable compared to another. As a final note, we recommend that filter() should be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations you care about. Learning check (LC3.1) Adapt the previous code using the “not” operator ! to filter only the species that are not danger category 4 or 5 in the mammals data frame. 3.3 slice rows Similar to filter, the slice function function returns a subset of row from a data frame. While filter returns the rows that match a specified criteria about the values of a variable (e.g., species == \"Cat\"), the slice function returns rows based on their positions. For example, let’s slice the first 5 rows of the mammals data frame: mammals %&gt;% slice(1:5) However, even more useful is a derivative of slice called slice_max that allow us to retrieve rows with the top values of a specified variable. For example, we can return a data frame of the 5 mammalian species with the longest life spans. Observe that we set the number of values to return to n = 5 and order_by = life_span to indicate that we want the rows corresponding to the top 5 values of life_span. mammals %&gt;% slice_max(n = 5, order_by = life_span) See the help file for slice() by running ?slice for more information about its related functions. Learning check (LC3.2) Repeat the previous command substituting the function slice_head for slice_max. How does the output differ? (LC3.3) Create a new data frame notLongLife that shows the rows of the mammals data frame with the 5 smallest values of the life_span variable. (Check the slice() help file for hints.) 3.4 select variables We recommended that you consider applying the filter() function to your data to narrow down the scope of your data frame to just the observations you care about. It may also be the case that you are only interested in a subset of the variables in your dataset. For example, the mammals data frame has 11 variables, but typically only a few variables will be of interest for a particular analysis. You can identify the names of these 11 variables by running the glimpse() function from the dplyr package: glimpse(mammals) In the same way that filter and slice return a subset of rows, the select function and its selection helpers allow us to return a subset of columns from a data frame. FIGURE 3.2: Diagram of select() columns. Returning to many_species, our data frame with four mammalian species, we might only really be interested in the variables species, body_wt and danger. However, with the current data frame, it’s a bit difficult to compare these columns among all the others: many_species # A tibble: 4 × 11 species body_wt brain_wt non_dreaming dreaming total_sleep life_span gestation &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africa… 6.65e+3 5712 NA NA 3.3 38.6 645 2 Asiane… 2.55e+3 4603 2.1 1.8 3.9 69 624 3 Desert… 5.5 e-1 2.4 7.6 2.7 10.3 NA NA 4 Europe… 7.85e-1 3.5 6.6 4.1 10.7 6 42 # … with 3 more variables: predation &lt;int&gt;, exposure &lt;int&gt;, danger &lt;int&gt; Examining these columns is much easier if we work with a smaller data frame by select()ing the desired variables: slim_many_species &lt;- many_species %&gt;% select(species, body_wt, danger) This slimmer data frame makes it easy to verify that we correctly filtered for species and also helps us to see the various ways that body weight relates to the danger faced by each species from other animals: slim_many_species # A tibble: 4 × 3 species body_wt danger &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; 1 Africanelephant 6654 3 2 Asianelephant 2547 4 3 Deserthedgehog 0.55 2 4 Europeanhedgehog 0.785 2 Let’s say instead you want to drop, or de-select, certain variables. For example, perhaps you want to drop the gestation variable. We can deselect gestation by using the - sign: mammals_10cols &lt;- mammals %&gt;% select(-gestation) Another way of selecting columns/variables is by specifying a range of columns. For example, we might want to only focus on the sleep-related variables for the mammalian species: mammals_sleep &lt;- mammals %&gt;% select(species, non_dreaming:total_sleep) mammals_sleep # A tibble: 62 × 4 species non_dreaming dreaming total_sleep &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africanelephant NA NA 3.3 2 Africangiantpouchedrat 6.3 2 8.3 3 ArcticFox NA NA 12.5 4 Arcticgroundsquirrel NA NA 16.5 5 Asianelephant 2.1 1.8 3.9 6 Baboon 9.1 0.7 9.8 7 Bigbrownbat 15.8 3.9 19.7 8 Braziliantapir 5.2 1 6.2 9 Cat 10.9 3.6 14.5 10 Chimpanzee 8.3 1.4 9.7 # … with 52 more rows The select() function can also be used to reorder columns when combined with the everything() helper function. For example, suppose we want the species and sleep-related variables to appear first, while not discarding the rest of the variables. In the following code, everything() will pick up all remaining variables: mammals_reorder &lt;- mammals %&gt;% select(species, non_dreaming:total_sleep, everything()) glimpse(mammals_reorder) See the help file for select() by running ?select for more information about other selection helpers. Learning check (LC3.4) Run the code mammals %&gt;% select(ends_with(\"wt\")) to select columns with names that end with “wt”. How many columns are returned? (LC3.5) Use the contains() helper function to select columns from the mammals data frame that contain “dream”. How many columns are returned? (LC3.6) What if you forgot to include the double-quotes for the first command above? Run the code mammals %&gt;% select(ends_with(\"wt\")) to see what happens. The last command shows an example of what happens when you forget to include double-quotes. If you see an Error message about an object ... not found, try adding double-quotes to see if that fixes the problem. 3.4.1 rename variables Another useful function similar to select() is rename(), which as you may have guessed changes the name of specified variables. Suppose we want to rename the danger variable to something more understandable, such as danger_faced: mammals %&gt;% rename(danger_faced = danger) Only the name of the specified variable has changed, and all of the other variables remain intact and unchanged. Note that here we used a single = sign within the rename(). For example, danger_faced = danger renames the danger variable to have the new name danger_faced. This is because we are not testing for equality like we would using ==. Instead we want to assign a new variable danger_faced to have the same values as danger and then delete the variable danger. Tip: New dplyr users often forget that the new variable name comes before the equal sign, followed by the old variable. You can remember this as “New Before Old”. Tip 2: Avoid spaces and special symbols in your variable names, which in our experience can cause problems in R. Pro-tip: We can also rename variables as we select() them from a data frame: mammals %&gt;% select(species, danger_faced = danger) # A tibble: 62 × 2 species danger_faced &lt;fct&gt; &lt;int&gt; 1 Africanelephant 3 2 Africangiantpouchedrat 3 3 ArcticFox 1 4 Arcticgroundsquirrel 3 5 Asianelephant 4 6 Baboon 4 7 Bigbrownbat 1 8 Braziliantapir 4 9 Cat 1 10 Chimpanzee 1 # … with 52 more rows Here we selected two columns from mammals, changing the name of the second one in the process. 3.5 summarize variables Another common task when working with data frames is to compute summary statistics. Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (also called the average) and the median (the middle value). Other examples of summary statistics that might not immediately come to mind include the sum, the smallest value also called the minimum, the largest value also called the maximum, and the standard deviation, a measure of the variability or “spread” in the values. See Appendix A.1 for a glossary of such summary statistics. Let’s calculate two summary statistics of the life_span variable in the mammals data frame: the mean and standard deviation. To compute these summary statistics, we need the mean() and sd() summary functions in R. Summary functions in R take in many values and return a single value, as illustrated in Figure 3.3. FIGURE 3.3: Diagram illustrating a summary function in R. More precisely, we’ll use the mean() and sd() summary functions within the summarize() function from the dplyr package. Note you can also use the British English spelling of summarise(). As shown in Figure 3.4, the summarize() function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics. FIGURE 3.4: Diagram of summarize() rows. We’ll save the results in a new data frame called summary_mammals that will have two columns/variables: the mean and the std_dev: summary_mammals &lt;- mammals %&gt;% summarize(mean = mean(life_span), std_dev = sd(life_span)) summary_mammals # A tibble: 1 × 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 NA NA Why are the values returned NA? As we saw in Subsection 2.3.1, NA is how R encodes missing values . By default any time you try to calculate a summary statistic of a variable that has one or more NA missing values in R, NA is returned. To work around this fact, you can set the na.rm argument to TRUE, where rm is short for “remove”; this will ignore any NA missing values and only return the summary value for all non-missing values. The code that follows computes the mean and standard deviation of all non-missing values of life_span: summary_mammals &lt;- mammals %&gt;% summarize(mean = mean(life_span, na.rm = TRUE), std_dev = sd(life_span, na.rm = TRUE)) summary_mammals # A tibble: 1 × 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 19.9 18.2 Notice how the na.rm = TRUE are used as arguments to the mean() and sd() summary functions individually, and not to the summarize() function. However, one needs to be cautious whenever ignoring missing values. In the upcoming Learning checks questions, we’ll consider the possible ramifications of blindly sweeping rows with missing values “under the rug.” This is in fact why the na.rm argument to any summary statistic function in R is set to FALSE by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this absence and any potential causes of this absence throughout your analysis. What are other summary functions we can use inside the summarize() verb to compute summary statistics? As seen in the diagram in Figure 3.3, you can use any summary function in R that takes many values and returns just one. Here are just a few: mean(): the average sd(): the standard deviation, which is a measure of spread min() and max(): the minimum and maximum values, respectively IQR(): interquartile range sum(): the total amount when adding multiple numbers n(): a count of the number of rows in each group. This particular summary function will make more sense when group_by() is covered in Section 3.6. In Section 3.7, we will introduce the skim() function, which can save you a lot of time writing code to calculate the most common summary statistics. Learning check (LC3.7) A doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five-year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor’s approach? (LC3.8) Modify the earlier summarize() function code that creates the summary_mammals data frame to also use the n() summary function: summarize(... , count = n()). What does the returned value correspond to? (LC3.9) Why doesn’t the following code work? Run the code line-by-line instead of all at once, and then look at the data. In other words, run summary2_mammals &lt;- mammals %&gt;% summarize(mean = mean(life_span, na.rm = TRUE)) first. summary2_mammals &lt;- mammals %&gt;% summarize(mean = mean(life_span, na.rm = TRUE)) %&gt;% summarize(std_dev = sd(life_span, na.rm = TRUE)) 3.6 group_by rows FIGURE 3.5: Diagram of group_by() and summarize(). Above we calculated the mean life_span of mammals. Say instead of a single mean life_span for a dataset, we would like to compute the mean life_spans of species in different predation categories separately, that is, the mean life_span split by predation. We can do this by “grouping” the life_span observations by the values of another variable, in this case by the values of the variable predation. Run the following code: summary_pred_life &lt;- mammals %&gt;% group_by(predation) %&gt;% summarize(mean = mean(life_span, na.rm = TRUE), std_dev = sd(life_span, na.rm = TRUE)) summary_pred_life # A tibble: 5 × 3 predation mean std_dev &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 29.2 24.5 2 2 13.3 12.9 3 3 15.4 20.6 4 4 19.1 11.7 5 5 20.7 12.9 This code is similar to the code that created summary_mammals, but with an extra group_by() command added before the summarize(). Grouping the mammals dataset by predation and then applying the summarize() functions yields a data frame that displays the mean and standard deviation length split by the different predation categories. It is important to note that the group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the data frame changes. For example, let’s consider the mammals data frame again. Run this code: mammals # A tibble: 62 × 11 species body_wt brain_wt non_dreaming dreaming total_sleep life_span &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africanelephant 6.65e+3 5712 NA NA 3.3 38.6 2 Africangiantpou… 1 e+0 6.6 6.3 2 8.3 4.5 3 ArcticFox 3.38e+0 44.5 NA NA 12.5 14 4 Arcticgroundsqu… 9.2 e-1 5.7 NA NA 16.5 NA 5 Asianelephant 2.55e+3 4603 2.1 1.8 3.9 69 6 Baboon 1.06e+1 180. 9.1 0.7 9.8 27 7 Bigbrownbat 2.3 e-2 0.3 15.8 3.9 19.7 19 8 Braziliantapir 1.6 e+2 169 5.2 1 6.2 30.4 9 Cat 3.3 e+0 25.6 10.9 3.6 14.5 28 10 Chimpanzee 5.22e+1 440 8.3 1.4 9.7 50 # … with 52 more rows, and 4 more variables: gestation &lt;dbl&gt;, predation &lt;int&gt;, # exposure &lt;int&gt;, danger &lt;int&gt; Observe that the first line of the output reads # A tibble: 62 x 11. This is an example of meta-data, in this case the number of observations/rows and variables/columns in mammals. The actual data itself are the subsequent table of values. Now let’s pipe the mammals data frame into group_by(predation): mammals %&gt;% group_by(predation) # A tibble: 62 × 11 # Groups: predation [5] species body_wt brain_wt non_dreaming dreaming total_sleep life_span &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africanelephant 6.65e+3 5712 NA NA 3.3 38.6 2 Africangiantpou… 1 e+0 6.6 6.3 2 8.3 4.5 3 ArcticFox 3.38e+0 44.5 NA NA 12.5 14 4 Arcticgroundsqu… 9.2 e-1 5.7 NA NA 16.5 NA 5 Asianelephant 2.55e+3 4603 2.1 1.8 3.9 69 6 Baboon 1.06e+1 180. 9.1 0.7 9.8 27 7 Bigbrownbat 2.3 e-2 0.3 15.8 3.9 19.7 19 8 Braziliantapir 1.6 e+2 169 5.2 1 6.2 30.4 9 Cat 3.3 e+0 25.6 10.9 3.6 14.5 28 10 Chimpanzee 5.22e+1 440 8.3 1.4 9.7 50 # … with 52 more rows, and 4 more variables: gestation &lt;dbl&gt;, predation &lt;int&gt;, # exposure &lt;int&gt;, danger &lt;int&gt; Observe that now there is additional meta-data: # Groups: predation [5] indicating that the grouping structure meta-data has been set based on the 5 possible levels of the categorical variable predation. On the other hand, observe that the data has not changed: it is still a table of 62 \\(\\times\\) 11 values. Only by combining a group_by() with another data wrangling operation, in this case summarize(), will the data actually be transformed. mammals %&gt;% group_by(predation) %&gt;% summarize(avg_life = mean(life_span, na.rm = TRUE)) # A tibble: 5 × 2 predation avg_life &lt;int&gt; &lt;dbl&gt; 1 1 29.2 2 2 13.3 3 3 15.4 4 4 19.1 5 5 20.7 If you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the ungroup() function: mammals %&gt;% group_by(predation) %&gt;% ungroup() # A tibble: 62 × 11 species body_wt brain_wt non_dreaming dreaming total_sleep life_span &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africanelephant 6.65e+3 5712 NA NA 3.3 38.6 2 Africangiantpou… 1 e+0 6.6 6.3 2 8.3 4.5 3 ArcticFox 3.38e+0 44.5 NA NA 12.5 14 4 Arcticgroundsqu… 9.2 e-1 5.7 NA NA 16.5 NA 5 Asianelephant 2.55e+3 4603 2.1 1.8 3.9 69 6 Baboon 1.06e+1 180. 9.1 0.7 9.8 27 7 Bigbrownbat 2.3 e-2 0.3 15.8 3.9 19.7 19 8 Braziliantapir 1.6 e+2 169 5.2 1 6.2 30.4 9 Cat 3.3 e+0 25.6 10.9 3.6 14.5 28 10 Chimpanzee 5.22e+1 440 8.3 1.4 9.7 50 # … with 52 more rows, and 4 more variables: gestation &lt;dbl&gt;, predation &lt;int&gt;, # exposure &lt;int&gt;, danger &lt;int&gt; Observe how the # Groups: predation [5] meta-data is no longer present. Let’s now revisit the n() counting summary function we briefly introduced previously. Recall that the n() function counts rows. This is opposed to the sum() summary function that returns the sum of a numerical variable. For example, suppose we’d like to count how many mammalian species were in each predation group: by_pred &lt;- mammals %&gt;% group_by(predation) %&gt;% summarize(count = n()) by_pred # A tibble: 5 × 2 predation count &lt;int&gt; &lt;int&gt; 1 1 14 2 2 15 3 3 12 4 4 7 5 5 14 We see that the greatest number of species are in predation category 2 and the fewest in category 4. 3.6.1 Grouping by more than one variable You are not limited to grouping by one variable. Say you want to know the number of species of each predation category for each exposure level. We can also group by a second variable exposure using group_by(predation, exposure): by_pred_exp &lt;- mammals %&gt;% group_by(predation, exposure) %&gt;% summarize(count = n()) by_pred_exp # A tibble: 19 × 3 # Groups: predation [5] predation exposure count &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 1 10 2 1 2 2 3 1 3 1 4 1 4 1 5 2 1 7 6 2 2 7 7 2 3 1 8 3 1 7 9 3 2 2 10 3 5 3 11 4 1 2 12 4 3 1 13 4 4 3 14 4 5 1 15 5 1 1 16 5 2 2 17 5 3 1 18 5 4 1 19 5 5 9 Why do we group_by(predation, exposure) and not group_by(predation) and then group_by(exposure)? Let’s investigate: by_pred_exp_incorrect &lt;- mammals %&gt;% group_by(predation) %&gt;% group_by(exposure) %&gt;% summarize(count = n()) by_pred_exp_incorrect # A tibble: 5 × 2 exposure count &lt;int&gt; &lt;int&gt; 1 1 27 2 2 13 3 3 4 4 4 5 5 5 13 What happened here is that the second group_by(exposure) overwrote the grouping structure meta-data of the earlier group_by(predation), so that in the end we are only grouping by exposure. The lesson here is if you want to group_by() two or more variables, you should include all the variables at the same time in the same group_by() adding a comma between the variable names. Learning check (LC3.10) Examining the summary_pred_life data frame, in which predation category do mammalian species have the longest average life span? Which predation category shows the greatest variation in life spans? (LC3.11) What code would be required to get the mean and standard deviation of life_span for each danger level of mammals? (LC3.12) Recreate by_pred_exp, but instead of grouping via group_by(predation, exposure), group variables in the reverse order group_by(exposure, predation). What differs in the resulting dataset? (LC3.13) How could we identify how many mammalian species are in each danger level? (LC3.14) How does the filter() operation differ from a group_by() followed by a summarize()? 3.7 mutate existing variables FIGURE 3.6: Diagram of mutate() columns. Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of weight in pounds (lb) instead of kilograms (hg). The formula to convert weights from kg to lb is \\[ \\text{weight in lb} = {\\text{weight in kg}} \\cdot {2.205} \\] We can apply this formula to the body_wt variable using the mutate() function from the dplyr package, which takes existing variables and mutates them to create new ones. mammals &lt;- mammals %&gt;% mutate(wt_in_lb = body_wt * 2.205) mammals # A tibble: 62 × 12 species body_wt brain_wt non_dreaming dreaming total_sleep life_span &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africanelephant 6.65e+3 5712 NA NA 3.3 38.6 2 Africangiantpou… 1 e+0 6.6 6.3 2 8.3 4.5 3 ArcticFox 3.38e+0 44.5 NA NA 12.5 14 4 Arcticgroundsqu… 9.2 e-1 5.7 NA NA 16.5 NA 5 Asianelephant 2.55e+3 4603 2.1 1.8 3.9 69 6 Baboon 1.06e+1 180. 9.1 0.7 9.8 27 7 Bigbrownbat 2.3 e-2 0.3 15.8 3.9 19.7 19 8 Braziliantapir 1.6 e+2 169 5.2 1 6.2 30.4 9 Cat 3.3 e+0 25.6 10.9 3.6 14.5 28 10 Chimpanzee 5.22e+1 440 8.3 1.4 9.7 50 # … with 52 more rows, and 5 more variables: gestation &lt;dbl&gt;, predation &lt;int&gt;, # exposure &lt;int&gt;, danger &lt;int&gt;, wt_in_lb &lt;dbl&gt; In this code, we mutate() the mammals data frame by creating a new variable wt_in_lb = body_wt * 2.205 and then overwrite the original mammals data frame. Why did we overwrite the data frame mammals, instead of assigning the result to a new data frame like mammals_new? As a rough rule of thumb, as long as you are not losing original information that you might need later, it’s acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable body_wt, but instead created a new variable called wt_in_lb? Because if we did this, we would have erased the original information contained in body_wt of weights in grams that may still be valuable to us. Let’s now compute average body weights of mammalian species in different predation categories in both kg and lb using the group_by() and summarize() code we saw in Section 3.6: summary_pred_wt &lt;- mammals %&gt;% group_by(predation) %&gt;% summarize(mean_wt_in_kg = mean(body_wt, na.rm = TRUE), mean_wt_in_lb = mean(wt_in_lb, na.rm = TRUE)) summary_pred_wt # A tibble: 5 × 3 predation mean_wt_in_kg mean_wt_in_lb &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 43.9 96.9 2 2 1.97 4.35 3 3 771. 1699. 4 4 53.8 119. 5 5 147. 324. Let’s look at some summary statistics of the wt_in_lb variable by considering multiple summary functions at once in the same summarize() code: lb_summary &lt;- mammals %&gt;% summarize( min = min(wt_in_lb, na.rm = TRUE), q1 = quantile(wt_in_lb, 0.25, na.rm = TRUE), median = quantile(wt_in_lb, 0.5, na.rm = TRUE), q3 = quantile(wt_in_lb, 0.75, na.rm = TRUE), max = max(wt_in_lb, na.rm = TRUE), mean = mean(wt_in_lb, na.rm = TRUE), sd = sd(wt_in_lb, na.rm = TRUE), missing = sum(is.na(wt_in_lb)) ) lb_summary # A tibble: 1 × 8 min q1 median q3 max mean sd missing &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.0110 1.32 7.37 106. 14672. 438. 1983. 0 We see for example that the mean weight for all mammalian species is about 438 pounds, while the heaviest species is 14,672 pounds! However, typing out all these summary statistic functions in summarize() is long and tedious. Fortunately, there is a much more succinct way to compute a variety of common summary statistics using the skim() function from the skimr package. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s select the wt_in_lb column from mammals and pipe it into the skim() function: mammals %&gt;% select(wt_in_lb) %&gt;% skim() For the numerical variable wt_in_lb, skim() returns: n_missing: the number of missing values complete_rate: the proportion of complete values mean: the average sd: the standard deviation p0: the 0th percentile: the value at which 0% of observations are smaller than it (the minimum value) p25: the 25th percentile: the value at which 25% of observations are smaller than it (the 1st quartile) p50: the 50th percentile: the value at which 50% of observations are smaller than it (the 2nd quartile and more commonly called the median) p75: the 75th percentile: the value at which 75% of observations are smaller than it (the 3rd quartile) p100: the 100th percentile: the value at which 100% of observations are smaller than it (the maximum value) Recall from Section 2.6 that since wt_in_lb is a numerical variable, we can visualize its distribution using a histogram. ggplot(data = mammals, mapping = aes(x = wt_in_lb)) + geom_histogram(color = &quot;white&quot;, boundary = 0, binwidth = 200) FIGURE 3.7: Histogram of wt_in_lb variable. The resulting histogram in Figure 3.7 provides a different perspective on the wt_in_lb variable than the summary statistics we computed earlier. For example, note that most values of wt_in_lb are right around 0. To close out our discussion on the mutate() function to create new variables, note that we can create multiple new variables at once in the same mutate() code. Furthermore, within the same mutate() code we can refer to new variables we just created, as shown in this example: mammals &lt;- mammals %&gt;% mutate( wt_in_oz = wt_in_lb * 16, oz_per_year = wt_in_oz / life_span ) Learning check (LC3.15) What can we say about the distribution of wt_in_lb? Describe it in a few sentences using the plot and the lb_summary data frame values. 3.8 arrange and sort rows One of the most commonly performed data wrangling tasks is to sort a data frame’s rows in the alphanumeric order of one of the variables. The dplyr package’s arrange() function allows us to sort/reorder a data frame’s rows according to the values of the specified variable. Suppose we are interested in determining the predation level most common among mammalian species: freq_pred &lt;- mammals %&gt;% group_by(predation) %&gt;% summarize(num_species = n()) freq_pred # A tibble: 5 × 2 predation num_species &lt;int&gt; &lt;int&gt; 1 1 14 2 2 15 3 3 12 4 4 7 5 5 14 Observe that by default the rows of the resulting freq_pred data frame are sorted in numerical order of predation category. Say instead we would like to see the same data, but sorted from the most to the least number of species (num_species) instead: freq_pred %&gt;% arrange(num_species) # A tibble: 5 × 2 predation num_species &lt;int&gt; &lt;int&gt; 1 4 7 2 3 12 3 1 14 4 5 14 5 2 15 This is, however, the opposite of what we want. The rows are sorted with the least frequently preferred habitats displayed first. This is because arrange() always returns rows sorted in ascending order by default. To switch the ordering to be in “descending” order instead, we use the desc() function as so: freq_pred %&gt;% arrange(desc(num_species)) # A tibble: 5 × 2 predation num_species &lt;int&gt; &lt;int&gt; 1 2 15 2 1 14 3 5 14 4 3 12 5 4 7 3.9 Conclusion 3.9.1 Summary table Let’s recap our data wrangling verbs in Table 3.1. Using these verbs and the pipe %&gt;% operator from Section 3.1, you’ll be able to write easily legible code to perform almost all the data wrangling and data transformation necessary for the rest of this book. TABLE 3.1: Summary of data wrangling verbs Verb Data wrangling operation filter() Pick out a subset of rows based on values of a variable slice() Pick out a subset of rows based on row number select() Pick out a subset of columns based on name or other criteria summarize() Summarize many values to one using a summary statistic function like mean(), median(), etc. group_by() Add grouping structure to rows in data frame. Note this does not change values in data frame, rather only the meta-data mutate() Create new variables by mutating existing ones arrange() Arrange rows of a data variable in ascending (default) or descending order 3.9.2 Additional resources If you want to further unlock the power of the dplyr package for data wrangling, we suggest that you check out RStudio’s “Data Transformation with dplyr” cheatsheet. This cheatsheet summarizes much more than what we’ve discussed in this chapter, including more advanced data wrangling functions, while providing quick and easy-to-read visual descriptions. In fact, many of the diagrams illustrating data wrangling operations in this chapter, such as Figure 3.1 on filter(), originate from this cheatsheet. You can access this cheatsheet by going to the RStudio Menu Bar and selecting “Help -&gt; Cheatsheets -&gt; Data Transformation with dplyr.” You can see a preview in the figure below. FIGURE 3.8: Data Transformation with dplyr cheatsheet. On top of the data wrangling verbs and examples we presented in this section, if you’d like to see more examples of using the dplyr package for data wrangling, check out Chapter 5 of R for Data Science (Grolemund and Wickham 2017). 3.9.3 What’s to come? So far in this book, we’ve explored, visualized, and wrangled data saved in data frames. These data frames were saved in a spreadsheet-like format: in a rectangular shape with a certain number of rows corresponding to observations and a certain number of columns corresponding to variables describing these observations. We’ll see in the upcoming Chapter 4 that there are actually two ways to represent data in spreadsheet-type rectangular format: (1) “wide” format and (2) “tall/narrow” format. The tall/narrow format is also known as “tidy” format in R user circles. While the distinction between “tidy” and non-“tidy” formatted data is subtle, it has immense implications for our data science work. This is because almost all the packages used in this book, including the ggplot2 package for data visualization and the dplyr package for data wrangling, assume that all data frames are in “tidy” format. References "],["4-tidy.html", "Chapter 4 “Tidy” Data 4.1 “Tidy” data 4.2 Case study: Weight loss data 4.3 tidyverse package 4.4 Conclusion", " Chapter 4 “Tidy” Data In Subsection 1.2.1, we introduced the concept of a data frame in R: a rectangular spreadsheet-like representation of data where the rows correspond to observations and the columns correspond to variables describing each observation. In Chapter 2, we created visualizations using data stored in a data frame, and in Chapter 3, we learned how to take existing data frames and transform/modify them to suit our ends. In this final chapter of the “Data Science with tidyverse” portion of the book, we extend some of these ideas by discussing a type of data formatting called “tidy” data. You will see that having data stored in “tidy” format is about more than just what the everyday definition of the term “tidy” might suggest: having your data “neatly organized.” Instead, we define the term “tidy” as it’s used by data scientists who use R, outlining a set of rules by which data is saved. Knowledge of this type of data formatting was not necessary for our treatment of data visualization in Chapter 2 and data wrangling in Chapter 3. This is because all the data used were already in “tidy” format. In this chapter, we’ll now see that this format is essential to using the tools we covered up until now. Furthermore, it will also be useful for all subsequent chapters in this book when we cover statistical inference and regression. Chapter Learning Objectives At the end of this chapter, you should be able to… • Determine if a dataset is in the “tidy” format necessary for using tidyverse functions. • Convert a data frame from a “wide” format to a “tidy” format. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). If needed, read Section 1.3 for information on how to install and load R packages. library(dplyr) library(ggplot2) library(readr) library(tidyr) library(carData) library(fivethirtyeight) 4.1 “Tidy” data We will learn about the concept of “tidy” data with a motivating example from the fivethirtyeight package. The fivethirtyeight package (Kim, Ismay, and Chunn 2021) provides access to the datasets used in many articles published by the data journalism website, FiveThirtyEight.com. For a complete list of all 129 datasets included in the fivethirtyeight package, check out the package webpage by going to: https://fivethirtyeight-r.netlify.app/articles/fivethirtyeight.html. Let’s focus our attention on the drinks data frame and look at its first 5 rows: # A tibble: 5 × 5 country beer_servings spirit_servings wine_servings total_litres_of_pure_… &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan 0 0 0 0 2 Albania 89 132 54 4.9 3 Algeria 25 0 14 0.7 4 Andorra 245 138 312 12.4 5 Angola 217 57 45 5.9 After reading the help file by running ?drinks, you’ll see that drinks is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed in 193 countries. This data was originally reported on FiveThirtyEight.com in Mona Chalabi’s article: “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”. Let’s apply some of the data wrangling verbs we learned in Chapter 3 on the drinks data frame: filter() the drinks data frame to only consider 4 countries: the United States, China, Italy, and Saudi Arabia, then select() all columns except total_litres_of_pure_alcohol by using the - sign, then rename() the variables beer_servings, spirit_servings, and wine_servings to beer, spirit, and wine, respectively. and save the resulting data frame in drinks_smaller: drinks_smaller &lt;- drinks %&gt;% filter(country %in% c(&quot;USA&quot;, &quot;China&quot;, &quot;Italy&quot;, &quot;Saudi Arabia&quot;)) %&gt;% select(-total_litres_of_pure_alcohol) %&gt;% rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings) drinks_smaller # A tibble: 4 × 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 Let’s now ask ourselves a question: “Using the drinks_smaller data frame, how would we create the side-by-side barplot in Figure 4.1?”. Recall we saw barplots displaying two categorical variables in Subsection 2.8.2. FIGURE 4.1: Comparing alcohol consumption in 4 countries. Let’s break down the grammar of graphics we introduced in Section 2.1: The categorical variable country with four levels (China, Italy, Saudi Arabia, USA) would have to be mapped to the x-position of the bars. The numerical variable servings would have to be mapped to the y-position of the bars (the height of the bars). The categorical variable type with three levels (beer, spirit, wine) would have to be mapped to the fill color of the bars. Observe that drinks_smaller has three separate variables beer, spirit, and wine. In order to use the ggplot() function to recreate the barplot in Figure 4.1 however, we need a single variable type with three possible values: beer, spirit, and wine. We could then map this type variable to the fill aesthetic of our plot. In other words, to recreate the barplot in Figure 4.1, our data frame would have to look like this: drinks_smaller_tidy # A tibble: 12 × 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 Italy beer 85 3 Saudi Arabia beer 0 4 USA beer 249 5 China spirit 192 6 Italy spirit 42 7 Saudi Arabia spirit 5 8 USA spirit 158 9 China wine 8 10 Italy wine 237 11 Saudi Arabia wine 0 12 USA wine 84 Observe that while drinks_smaller and drinks_smaller_tidy are both rectangular in shape and contain the same 12 numerical values (3 alcohol types by 4 countries), they are formatted differently. drinks_smaller is formatted in what’s known as “wide” format, whereas drinks_smaller_tidy is formatted in what’s known as “long/narrow” format. In the context of doing data science in R, long/narrow format is also known as “tidy” format. In order to use the ggplot2 and dplyr packages for data visualization and data wrangling, your input data frames must be in “tidy” format. Thus, all non-“tidy” data must be converted to “tidy” format first. Before we convert non-“tidy” data frames like drinks_smaller to “tidy” data frames like drinks_smaller_tidy, let’s define “tidy” data. 4.1.1 Definition of “tidy” data You have surely heard the word “tidy” in your life: “Tidy up your room!” “Write your homework in a tidy way so it is easier to provide feedback.” Marie Kondo’s best-selling book, The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing, and Netflix TV series Tidying Up with Marie Kondo. “I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - ‘Read me, please!’” - Linda Grant What does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized,” the word “tidy” in data science using R means that your data follows a standardized format. We will follow Hadley Wickham’s definition of “tidy” data (Wickham 2014) shown also in Figure 4.2: A dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes. “Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. FIGURE 4.2: Tidy data graphic from R for Data Science. For example, say you have the following table of chick weights (in g) in Table 4.1: TABLE 4.1: Chick weights (non-tidy format) Date Chick 1 weight Chick 2 weight Chick 3 weight 2009-01-01 173.55 g 174.90 g 174.34 g 2009-01-02 172.61 g 171.42 g 170.04 g Although the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in “tidy” format. While there are three variables corresponding to three unique pieces of information (date, chick ID, and weight), there are not three columns. In “tidy” data format, each variable should be its own column, as shown in Table 4.2. Notice that both tables present the same information, but in different formats. TABLE 4.2: Chick weights (tidy format) Date Chick ID Weight 2009-01-01 1 173.55 g 2009-01-01 2 174.90 g 2009-01-01 3 174.34 g 2009-01-02 1 172.61 g 2009-01-02 2 171.42 g 2009-01-02 3 170.04 g Now we have the requisite three columns Date, Chick ID, and Weight. On the other hand, consider the data in Table 4.3. TABLE 4.3: Example of tidy data Date Chick 1 weight Weather 2009-01-01 173.55 g Sunny 2009-01-02 172.61 g Overcast In this case, even though the variable “Chick 1 weight” occurs just like in our non-“tidy” data in Table 4.1, the data is “tidy” since there are three variables corresponding to three unique pieces of information: Date, Chick 1 weight, and the Weather that particular day. Learning check (LC4.1) What are common characteristics of “tidy” data frames? (LC4.2) What makes “tidy” data frames useful for organizing data? 4.1.2 Converting to “tidy” data In this book so far, you’ve only seen data frames that were already in “tidy” format. Furthermore, for the rest of this book, you’ll mostly only see data frames that are already in “tidy” format as well. This is not always the case however with all datasets in the world. If your original data frame is in wide (non-“tidy”) format and you would like to use the ggplot2 or dplyr packages, you will first have to convert it to “tidy” format. To do so, we recommend using the pivot_longer() function in the tidyr package (Wickham and Girlich 2022). Going back to our drinks_smaller data frame from earlier: drinks_smaller # A tibble: 4 × 4 country beer spirit wine &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 China 79 192 8 2 Italy 85 42 237 3 Saudi Arabia 0 5 0 4 USA 249 158 84 We convert it to “tidy” format by using the pivot_longer() function from the tidyr package as follows: drinks_smaller_tidy &lt;- drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = -country) drinks_smaller_tidy # A tibble: 12 × 3 country type servings &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 China beer 79 2 China spirit 192 3 China wine 8 4 Italy beer 85 5 Italy spirit 42 6 Italy wine 237 7 Saudi Arabia beer 0 8 Saudi Arabia spirit 5 9 Saudi Arabia wine 0 10 USA beer 249 11 USA spirit 158 12 USA wine 84 We set the arguments to pivot_longer() as follows: names_to here corresponds to the name of the variable in the new “tidy”/long data frame that will contain the column names of the original data. Observe how we set names_to = \"type\". In the resulting drinks_smaller_tidy, the column type contains the three types of alcohol beer, spirit, and wine. Since type is a variable name that doesn’t appear in drinks_smaller, we use quotation marks around it. You’ll receive an error if you just use names_to = type here. values_to here is the name of the variable in the new “tidy” data frame that will contain the values of the original data. Observe how we set values_to = \"servings\" since each of the numeric values in each of the beer, wine, and spirit columns of the drinks_smaller data corresponds to a value of servings. In the resulting drinks_smaller_tidy, the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values. Note again that servings doesn’t appear as a variable in drinks_smaller so it again needs quotation marks around it for the values_to argument. The third argument cols is the columns in the drinks_smaller data frame you either want to or don’t want to “tidy.” Observe how we set this to -country indicating that we don’t want to “tidy” the country variable in drinks_smaller and rather only beer, spirit, and wine. Since country is a column that appears in drinks_smaller we don’t put quotation marks around it. The third argument here of cols is a little nuanced, so let’s consider code that’s written slightly differently but that produces the same output: drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = c(beer, spirit, wine)) Note that the third argument now specifies which columns we want to “tidy” with c(beer, spirit, wine), instead of the columns we don’t want to “tidy” using -country. We use the c() function to create a vector of the columns in drinks_smaller that we’d like to “tidy.” Note that since these three columns appear one after another in the drinks_smaller data frame, we could also do the following for the cols argument: drinks_smaller %&gt;% pivot_longer(names_to = &quot;type&quot;, values_to = &quot;servings&quot;, cols = beer:wine) With our drinks_smaller_tidy “tidy” formatted data frame, we can now produce the barplot you saw in Figure 4.1 using geom_col(). This is done in Figure 4.3. Recall from Section 2.8 on barplots that we use geom_col() and not geom_bar(), since we would like to map the “pre-counted” servings variable to the y-aesthetic of the bars. ggplot(drinks_smaller_tidy, aes(x = country, y = servings, fill = type)) + geom_col(position = &quot;dodge&quot;) FIGURE 4.3: Comparing alcohol consumption in 4 countries using geom_col(). Converting “wide” format data to “tidy” format often confuses new R users. The only way to learn to get comfortable with the pivot_longer() function is with practice, practice, and more practice using different datasets. For example, run ?pivot_longer and look at the examples in the bottom of the help file. We’ll show another example of using pivot_longer() to convert a “wide” formatted data frame to “tidy” format in Section 4.2. If however you want to convert a “tidy” data frame to “wide” format, you will need to use the pivot_wider() function instead. Run ?pivot_wider and look at the examples in the bottom of the help file for examples. You can also view examples of both pivot_longer() and pivot_wider() on the tidyverse.org webpage. There’s a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a dataset in the weekly #TidyTuesday event that might serve as a nice place for you to find other data to explore and transform. Learning check (LC4.3) Take a look again at the blackbird data frame that you imported earlier. Run the following: blackbird As mentioned earlier, blackbird is a data frame containing information that compares the immunocompetence of red-winged blackbirds before and after testosterone implants. For simplicity, let’s ignore the last two columns, which are the log-transformed values of the data in the previous two columns: blackbird_smaller &lt;- blackbird %&gt;% select(!starts_with(&quot;log&quot;)) blackbird_smaller # A tibble: 13 × 3 blackbird beforeImplant afterImplant &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 105 85 2 2 50 74 3 3 136 145 4 4 90 86 5 5 122 148 6 6 132 148 7 7 131 150 8 8 119 142 9 9 145 151 10 10 130 113 11 11 116 118 12 12 110 99 13 13 138 150 This data frame is not in “tidy” format. How would you convert this data frame to be in “tidy” format, so that it has a variable Time_Period indicating the time period (before or after implant) and a variable Measurement of the immunocompetence level? 4.2 Case study: Weight loss data In this section, we’ll show you another example of how to convert a data frame that isn’t in “tidy” format (“wide” format) to a data frame that is in “tidy” format (“long/narrow” format). We’ll do this using the pivot_longer() function from the tidyr package again. Then, we’ll make use of functions from the ggplot2 and dplyr packages to produce a time-series plot showing weight loss data for three groups of individuals: control, diet, and diet plus exercise. Recall that we saw time-series plots in Section 2.4 on creating linegraphs using geom_line(). We’ll summarize the WeightLoss data frame available in the carData package and focus on average weight loss data for the Diet + Exercise group. WeeklyAverage &lt;- WeightLoss %&gt;% group_by(group) %&gt;% summarise(mean_wl1 = mean(wl1), mean_wl2 = mean(wl2), mean_wl3 = mean(wl3)) WeeklyAverage # A tibble: 3 × 4 group mean_wl1 mean_wl2 mean_wl3 &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Control 4.5 3.33 2.08 2 Diet 5.33 3.92 2.25 3 DietEx 6.2 6.1 2.2 Now, let’s lay out the grammar of graphics we saw in Section 2.1. First we know we need to set data = WeeklyAverage and use a geom_line() layer, but what is the aesthetic mapping of variables? We’d like to see how the weight loss has changed over the months for each group, so we need to map: month to the x-position aesthetic, weightloss to the y-position aesthetic, and group to the color aesthetic. Now we are stuck in a predicament, much like with our drinks_smaller example in Section 4.1. We have a variable/column named group, but the other variables/columns have combined information about both the month and the average weight loss. Because the WeeklyAverage data frame is not “tidy”, we cannot use the ggplot2 package until the data is in the appropriate format to apply the grammar of graphics. So how do we convert this data frame into a tidy format. First, we need to take the numeric values from the mean_wl column names in WeeklyAverage and place them into a new “names” variable called month. Then, we need to take the average weight loss values inside the data frame and turn them into a new “values” variable called weightloss. Our resulting data frame will have three columns: group, month, and weightloss. Recall that the pivot_longer() function in the tidyr package does this for us: WeeklyAverage_tidy &lt;- WeeklyAverage %&gt;% pivot_longer(cols = -group, names_to = &quot;month&quot;, values_to = &quot;weightloss&quot;) WeeklyAverage_tidy # A tibble: 9 × 3 group month weightloss &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; 1 Control mean_wl1 4.5 2 Control mean_wl2 3.33 3 Control mean_wl3 2.08 4 Diet mean_wl1 5.33 5 Diet mean_wl2 3.92 6 Diet mean_wl3 2.25 7 DietEx mean_wl1 6.2 8 DietEx mean_wl2 6.1 9 DietEx mean_wl3 2.2 The first argument specifies the columns you either want to or don’t want to “tidy.” We set this to cols = -group indicating that we don’t want to “tidy” the group variable but do want to “tidy” the other variables in WeeklyAverage. names_to is the name of the variable in the new “tidy” data frame that will contain the column names of the original data. Observe how we set names_to = \"month\". In the resulting WeeklyAverage_tidy, the column month indicates the month that the weight loss was measured. values_to is the name of the variable in the new “tidy” data frame that will contain the values of the original data. Observe how we set values_to = \"weightloss\". In the resulting WeeklyAverage_tidy the column weightloss contains the 3 \\(\\times\\) 3 = 3 weight loss scores as numeric values. The last argument of names_transform tells R what type of variable year should be set to. Without specifying that it is an integer as we’ve done here, pivot_longer() will set it to be a character value by default. This is a good start, but to create the time-series plot, the month variable needs to be numeric, not a character string. We can fix this by converting the month variable to an integer using two additional arguments in the pivot_longer command. We’ll use names_prefix to strip off the mean_wl prefix from each column name, and names_transform to convert month into an integer variable. WeeklyAverage_tidy &lt;- WeeklyAverage %&gt;% pivot_longer(cols = -group, names_to = &quot;month&quot;, names_prefix = &quot;mean_wl&quot;, names_transform = list(month = as.integer), values_to = &quot;weightloss&quot;) WeeklyAverage_tidy # A tibble: 9 × 3 group month weightloss &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; 1 Control 1 4.5 2 Control 2 3.33 3 Control 3 2.08 4 Diet 1 5.33 5 Diet 2 3.92 6 Diet 3 2.25 7 DietEx 1 6.2 8 DietEx 2 6.1 9 DietEx 3 2.2 We can now create the time-series plot in Figure 4.4 using geom_line() to visualize how weight loss scores in each group changed during the study. Furthermore, we’ll use the labs() function in the ggplot2 package to add informative labels to the aes()thetic attributes of our plot. ggplot(WeeklyAverage_tidy, aes(x = month, y = weightloss, color = group)) + geom_line() + labs(x = &quot;Month&quot;, y = &quot;Weight Loss Score&quot;, color = &quot;Study group&quot;) FIGURE 4.4: Weight loss scores of control group. We see that the Diet + Exercise group had a higher average weight loss score in the first month than the other groups, but eventually the average weight loss scores converged in the third month. Note that if we forgot to include the names_transform argument specifying that month should be in the integer format, we would have gotten an error here since geom_line() wouldn’t have known how to sort the character values in month in the right order. Learning check (LC4.4) Read in the life expectancy data stored at https://moderndive.com/data/le_mess.csv and convert it to a “tidy” data frame. (LC4.5) Prepare line graphs displaying the average weight loss scores with time for each group using facets instead. Is the faceted plot more helpful for comparing the groups than the original plot? (LC4.6) Revise the code for the original plot to better visualize the sampling variation for each group and month. 4.3 tidyverse package Notice at the beginning of the chapter we loaded the following four packages, which are among four of the most frequently used R packages for data science: library(ggplot2) library(dplyr) library(readr) library(tidyr) Recall that ggplot2 is for data visualization, dplyr is for data wrangling, readr is for importing spreadsheet data into R, and tidyr is for converting data to “tidy” format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the tidyverse package. The tidyverse package acts as an “umbrella” package whereby installing/loading it will install/load multiple packages at once for you. After installing the tidyverse package as you would a normal package as seen in Section 1.3, running: library(tidyverse) would be the same as running: library(ggplot2) library(dplyr) library(readr) library(tidyr) library(purrr) library(tibble) library(stringr) library(forcats) The purrr, tibble, stringr, and forcats are left for a more advanced book; check out R for Data Science to learn about these packages. For the remainder of this book, we’ll start every chapter by running library(tidyverse), instead of loading the various component packages individually. The tidyverse “umbrella” package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames are in “tidy” format. This standardization of input and output data frames makes transitions between different functions in the different packages as seamless as possible. For more information, check out the tidyverse.org webpage for the package. 4.4 Conclusion 4.4.1 Additional resources If you want to learn more about using the readr and tidyr package, we suggest that you check out RStudio’s “Data Import Cheat Sheet.” In the current version of RStudio, you can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheat Sheets -&gt; “Browse Cheatsheets” -&gt; Scroll down the page to the “Data Import Cheat Sheet.” The first page of this cheatsheet has information on using the readr package to import data, while the second page has information on using the tidyr package to “tidy” data. You can see a preview of both cheatsheets in the figures below. FIGURE 4.5: Data Import cheatsheet (first page): readr package. FIGURE 4.6: Data Import cheatsheet (second page): tidyr package. 4.4.2 What’s to come? Congratulations! You’ve completed the “Data Science with tidyverse” portion of this book. We’ll now move to the “Statistical Inference with infer” portion of this book. Statistical inference is the science of inferring about some unknown quantity using sampling. The most well-known examples of sampling in practice involve polls. Because asking an entire population about their opinions would be a long and arduous task, pollsters often take a smaller sample that is hopefully representative of the population. Based on the results of this sample, pollsters hope to make claims about the entire population. As shown in Figure 4.7, once we’ve covered Chapter 5 on sampling, we’ll examine confidence intervals in Chapter 6 and hypothesis testing in Chapter 7. FIGURE 4.7: ModernDive for Life Scientists flowchart - on to Part II! References "],["5-sampling.html", "Chapter 5 Sampling 5.1 Sampling bowl activity 5.2 Virtual sampling 5.3 Sampling framework 5.4 Case study: Genetic crosses 5.5 Central Limit Theorem 5.6 Sampling scenarios 5.7 Conclusion", " Chapter 5 Sampling In this chapter, we kick off the second portion of this book on statistical inference by learning about sampling. The concepts behind sampling form the basis of confidence intervals and hypothesis testing, which we’ll cover in Chapters 6 and 7. We will see that the tools that you learned in the data science portion of this book, in particular data visualization and data wrangling, will also play an important role in the development of your understanding. As mentioned before, the concepts throughout this text all build into a culmination allowing you to “tell your story with data.” Chapter Learning Objectives At the end of this chapter, you should be able to… • Define and explain the basis of sampling error. • Describe how to produce a sampling distribution and explain why it approximates a normal distribution. • Explain how sample size affects the spread (standard deviation) of the sampling distribution. • Recognize when a sampling methodology is not representative, generalizable, or unbiased and explain why. • Describe how to conduct random sampling. • List sampling steps that can be used to improve the accuracy and precision of a point estimate. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 4.3 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) 5.1 Sampling bowl activity Let’s start with a hands-on activity. 5.1.1 What proportion of this bowl’s balls are red? Take a look at the bowl in Figure 5.1. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls. Let’s now ask ourselves, what proportion of this bowl’s balls are red? FIGURE 5.1: A bowl with red and white balls. One way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process. 5.1.2 Using the shovel once Instead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 5.2. Using the shovel, let’s remove \\(5 \\cdot 10 = 50\\) balls, as seen in Figure 5.3. FIGURE 5.2: Inserting a shovel into the bowl. FIGURE 5.3: Removing 50 balls from the bowl. Observe that 17 of the balls are red and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34% took much less time and energy to make. However, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe? What if we repeated this activity several times following the process shown in Figure 5.4? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition. 5.1.3 Using the shovel 33 times Each of our 33 groups of friends will do the following: Use the shovel to remove 50 balls each. Count the number of red balls and thus compute the proportion of the 50 balls that are red. Return the balls into the bowl. Mix the contents of the bowl a little to not let a previous group’s results influence the next group’s. FIGURE 5.4: Repeating sampling activity 33 times. Each of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then marks their proportion of their 50 balls that were red in the appropriate bin in a hand-drawn histogram as seen in Figure 5.5. FIGURE 5.5: Constructing a histogram of proportions. Recall from Section 2.6 that histograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in Figure 5.6. FIGURE 5.6: Hand-drawn histogram of first 10 out of 33 proportions. Observe the following in the histogram in Figure 5.6: At the low end, one group removed 50 balls from the bowl with proportion red between 0.20 and 0.25. At the high end, another group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red. However, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution. The shape of this distribution is somewhat bell-shaped. Let’s construct this same hand-drawn histogram in R using your data visualization skills that you honed in Chapter 2. We saved our 33 groups of friends’ results in the tactile_prop_red data frame included in the moderndive package. Run the following to display the first 10 of 33 rows: tactile_prop_red # A tibble: 33 × 4 group replicate red_balls prop_red &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 Ilyas, Yohan 1 21 0.42 2 Morgan, Terrance 2 17 0.34 3 Martin, Thomas 3 21 0.42 4 Clark, Frank 4 21 0.42 5 Riddhi, Karina 5 18 0.36 6 Andrew, Tyler 6 19 0.38 7 Julia 7 19 0.38 8 Rachel, Lauren 8 11 0.22 9 Daniel, Caroline 9 15 0.3 10 Josh, Maeve 10 17 0.34 # … with 23 more rows Observe for each group that we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. We also have a replicate variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red. Let’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05 in Figure 5.7. This is a computerized and complete version of the partially completed hand-drawn histogram you saw in Figure 5.6. Note that setting boundary = 0.4 indicates that we want a binning scheme such that one of the bins’ boundary is at 0.4. This helps us to more closely align this histogram with the hand-drawn histogram in Figure 5.6. ggplot(tactile_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 5.7: Distribution of 33 proportions based on 33 samples of size 50. 5.1.4 What did we just do? What we just demonstrated in this activity is the statistical concept of sampling. We would like to know the proportion of the bowl’s balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a sample of 50 balls using the shovel to make an estimate. Using this sample of 50 balls, we estimated the proportion of the bowl’s balls that are red to be 34%. Moreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure 5.7. This is known as the concept of sampling variation. The purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling: Understanding the effect of sampling variation. Understanding the effect of sample size on sampling variation. In Section 5.2, we’ll mimic the hands-on sampling activity we just performed on a computer. This will allow us not only to repeat the sampling exercise much more than 33 times, but it will also allow us to use shovels with different numbers of slots than just 50. Afterwards, we’ll present you with definitions, terminology, and notation related to sampling in Section 5.3. As in many disciplines, such necessary background knowledge may seem inaccessible and even confusing at first. However, as with many difficult topics, if you truly understand the underlying concepts and practice, practice, practice, you’ll be able to master them. To tie the contents of this chapter to the real world, we’ll present a biology-related example: genetic crosses. In Section 5.4 we’ll look at a particular genetic cross between curly-winged fruit flies. To close this chapter, we’ll generalize the “sampling from a bowl” exercise to other sampling scenarios and present a theoretical result known as the Central Limit Theorem. Learning check (LC5.1) Why was it important to mix the bowl before we sampled the balls? (LC5.2) Why is it that our 33 groups of friends did not all have the same numbers of balls that were red out of 50, and hence different proportions red? 5.2 Virtual sampling In the previous Section 5.1, we performed a tactile sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we’ll mimic this tactile sampling activity with a virtual sampling activity using a computer. In other words, we’ll use a virtual analog to the bowl of balls and a virtual analog to the shovel. 5.2.1 Using the virtual shovel once Let’s start by performing the virtual analog of the tactile sampling exercise we performed in Section 5.1. We first need a virtual analog of the bowl seen in Figure 5.1. To this end, we included a data frame named bowl in the moderndive package. The rows of bowl correspond exactly with the contents of the actual bowl. bowl # A tibble: 2,400 × 2 ball_ID color &lt;int&gt; &lt;chr&gt; 1 1 white 2 2 white 3 3 white 4 4 red 5 5 white 6 6 white 7 7 red 8 8 white 9 9 red 10 10 white # … with 2,390 more rows Observe that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable as discussed in Subsection 1.4.4; none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white. View the contents of the bowl in RStudio’s data viewer and scroll through the contents to convince yourself that bowl is indeed a virtual analog of the actual bowl in Figure 5.1. Now that we have a virtual analog of our bowl, we next need a virtual analog to the shovel seen in Figure 5.2 to generate virtual samples of 50 balls. We’re going to use the rep_sample_n() function included in the moderndive package. This function allows us to take repeated, or replicated, samples of size n. Observe that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable as discussed in Subsection 1.4.4; none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white. View the contents of the bowl in RStudio’s data viewer and scroll through the contents to convince yourself that bowl is indeed a virtual analog of the actual bowl in Figure 5.1. Now that we have a virtual analog of our bowl, we now need a virtual analog to the shovel seen in Figure 5.2 to generate virtual samples of 50 balls. We’re going to use the rep_sample_n() function included in the moderndive package. This function allows us to take repeated, or replicated, samples of size n. virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) virtual_shovel # A tibble: 50 × 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1970 white 2 1 842 red 3 1 2287 white 4 1 599 white 5 1 108 white 6 1 846 red 7 1 390 red 8 1 344 white 9 1 910 white 10 1 1485 white # … with 40 more rows Observe that virtual_shovel has 50 rows corresponding to our virtual sample of size 50. The ball_ID variable identifies which of the 2400 balls from bowl are included in our sample of 50 balls while color denotes its color. However, what does the replicate variable indicate? In virtual_shovel’s case, replicate is equal to 1 for all 50 rows. This is telling us that these 50 rows correspond to the first repeated/replicated use of the shovel, in our case our first sample. We’ll see shortly that when we “virtually” take 33 samples, replicate will take values between 1 and 33. Let’s compute the proportion of balls in our virtual sample that are red using the dplyr data wrangling verbs you learned in Chapter 3. First, for each of our 50 sampled balls, let’s identify if it is red or not using a test for equality with ==. Let’s create a new Boolean variable is_red using the mutate() function from Section 3.7: virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) # A tibble: 50 × 4 # Groups: replicate [1] replicate ball_ID color is_red &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; 1 1 1970 white FALSE 2 1 842 red TRUE 3 1 2287 white FALSE 4 1 599 white FALSE 5 1 108 white FALSE 6 1 846 red TRUE 7 1 390 red TRUE 8 1 344 white FALSE 9 1 910 white FALSE 10 1 1485 white FALSE # … with 40 more rows Observe that for every row where color == \"red\", the Boolean (logical) value TRUE is returned and for every row where color is not equal to \"red\", the Boolean FALSE is returned. Second, let’s compute the number of balls out of 50 that are red using the summarize() function. Recall from Section 3.5 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the mean() or median(). In this case, we use the sum(): virtual_shovel %&gt;% mutate(is_red = (color == &quot;red&quot;)) %&gt;% summarize(num_red = sum(is_red)) # A tibble: 1 × 2 replicate num_red &lt;int&gt; &lt;int&gt; 1 1 12 Why does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of balls where color is red. In our case, 12 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling. Third and lastly, let’s compute the proportion of the 50 sampled balls that are red by dividing num_red by 50: virtual_shovel %&gt;% mutate(is_red = color == &quot;red&quot;) %&gt;% summarize(num_red = sum(is_red)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 × 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 In other words, 24% of this virtual sample’s balls were red. Let’s make this code a little more compact and succinct by combining the first mutate() and the summarize() as follows: virtual_shovel %&gt;% summarize(num_red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 × 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 Great! 24% of virtual_shovel’s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the bowl’s balls that are red is 24%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 24% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure 5.6. We saw that these estimates varied. Let’s now perform the virtual analog of having 33 groups of students use the sampling shovel! 5.2.2 Using the virtual shovel 33 times Recall that in our tactile sampling exercise in Section 5.1, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated/replicated using the shovel 33 times. We can perform this repeated/replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument. This is telling R that we want to repeat the sampling 33 times. We’ll save these results in a data frame called virtual_samples. While we provide a preview of the first 10 rows of virtual_samples in what follows, we highly suggest you scroll through its contents using RStudio’s spreadsheet viewer by running View(virtual_samples). virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 33) virtual_samples # A tibble: 1,650 × 3 # Groups: replicate [33] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 875 white 2 1 1851 red 3 1 1548 red 4 1 1975 white 5 1 835 white 6 1 16 white 7 1 327 white 8 1 1803 red 9 1 740 red 10 1 179 red # … with 1,640 more rows Observe in the spreadsheet viewer that the first 50 rows of replicate are equal to 1 while the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\cdot\\) 50 = 1650 rows. Let’s now take virtual_samples and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as before, but this time with an additional group_by() of the replicate variable. Recall from Section 3.6 that by assigning the grouping variable “meta-data” before we summarize(), we’ll obtain 33 different proportions red. We display a preview of the first 10 out of 33 rows: virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) virtual_prop_red # A tibble: 33 × 3 replicate red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 23 0.46 2 2 19 0.38 3 3 18 0.36 4 4 19 0.38 5 5 15 0.3 6 6 21 0.42 7 7 21 0.42 8 8 16 0.32 9 9 24 0.48 10 10 14 0.28 # … with 23 more rows As with our 33 groups of friends’ tactile samples, there is variation in the resulting 33 virtual proportions red. Let’s visualize this variation in a histogram in Figure 5.8. Note that we add binwidth = 0.05 and boundary = 0.4 arguments as well. Recall that setting boundary = 0.4 ensures a binning scheme with one of the bins’ boundaries at 0.4. Since the binwidth = 0.05 is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 33 proportions red&quot;) FIGURE 5.8: Distribution of 33 proportions based on 33 samples of size 50. Observe that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 40% (for 11 out of 33 samples). Why do we have these differences in proportions red? Because of sampling variation. Let’s now compare our virtual results with our tactile results from the previous section in Figure 5.9. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped. FIGURE 5.9: Comparing 33 virtual and 33 tactile proportions red. Learning check (LC5.3) Why couldn’t we study the effects of sampling variation when we used the virtual shovel only once? Why did we need to take more than one virtual sample (in our case 33 virtual samples)? 5.2.3 Using the virtual shovel 1000 times Now say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000. We have two choices at this point. We could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. However, this would be a tedious and time-consuming task. This is where computers excel: automating long and repetitive tasks while performing them quite quickly. Thus, at this point we will abandon tactile sampling in favor of only virtual sampling. Let’s once again use the rep_sample_n() function with sample size set to be 50 once again, but this time with the number of replicates reps set to 1000. Be sure to scroll through the contents of virtual_samples in RStudio’s viewer. virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) virtual_samples # A tibble: 50,000 × 3 # Groups: replicate [1,000] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1236 red 2 1 1944 red 3 1 1939 white 4 1 780 white 5 1 1956 white 6 1 1003 white 7 1 2113 white 8 1 2213 white 9 1 782 white 10 1 898 white # … with 49,990 more rows Observe that now virtual_samples has 1000 \\(\\cdot\\) 50 = 50,000 rows, instead of the 33 \\(\\cdot\\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\cdot\\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls. virtual_prop_red &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) virtual_prop_red # A tibble: 1,000 × 3 replicate red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 18 0.36 2 2 19 0.38 3 3 20 0.4 4 4 15 0.3 5 5 17 0.34 6 6 16 0.32 7 7 23 0.46 8 8 23 0.46 9 9 15 0.3 10 10 18 0.36 # … with 990 more rows Observe that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 5.10. ggplot(virtual_prop_red, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Distribution of 1000 proportions red&quot;) FIGURE 5.10: Distribution of 1000 proportions based on 1000 samples of size 50. Once again, the most frequently occurring proportions of red balls occur between 35% and 40%. Every now and then, we obtain proportions as low as between 20% and 25%, and others as high as between 55% and 60%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, approximated well by what is called a “normal distribution” (see Appendix A.2). Learning check (LC5.4) Why did we not take 1000 “tactile” samples of 50 balls by hand? (LC5.5) Looking at Figure 5.10, would you say that sampling 50 balls where 30% of them were red is likely or not? What about sampling 50 balls where 10% of them were red? 5.2.4 The Normal Distribution Normal distributions are bell-shaped distributions that can be defined by two values: (1) the mean \\(\\mu\\) (“mu”) which locates the center of the distribution and (2) the standard deviation \\(\\sigma\\) (“sigma”) which indicates the variation or spread of the distribution. There are an infinite number of normal distributions that differ in their mean and/or standard deviation. Three examples of normal distributions are plotted in Figure 5.11: The normal distribution indicated by the solid line has mean \\(\\mu = 5\\) &amp; standard deviation \\(\\sigma = 2\\). The normal distribution indicated by the dotted line has mean \\(\\mu = 5\\) &amp; standard deviation \\(\\sigma = 5\\). The normal distribution indicated by the dashed line has mean \\(\\mu = 15\\) &amp; standard deviation \\(\\sigma = 2\\). The solid and dotted lines are normal distributions with the same center due to their common mean \\(\\mu\\) = 5. However, the normal distribution indicated by the dotted line is wider due to its larger standard deviation of \\(\\sigma\\) = 5. On the other hand, the normal distributions represented the solid and dashed lines have the same variation due to their common standard deviation \\(\\sigma\\) = 2, but they have different means and are therefore centered at different locations. FIGURE 5.11: Three normal distributions. So, what’s the big deal about the normal distribution? One thing is that if a variable follows a normal distribution, we can use a few rules of thumb to predict the distribution of values in a population based on its mean and standard deviation: 68% of values will lie within \\(\\pm\\) 1 standard deviation of the mean. 95% of values will lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations of the mean. 99.7% of values will lie within \\(\\pm\\) 3 standard deviations of the mean. See Appendix A.2 for a helpful illustration of these rules of thumbs and more information about the normal distribution. 5.2.5 Using different shovels Now say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100. FIGURE 5.12: Three shovels to extract three different sample sizes. If your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the “best” guess of the proportion of the bowl’s balls that are red. Let’s define some criteria for “best” in this subsection. Using our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size set to 25, 50, and 100, respectively, while keeping the number of repeated/replicated samples at 1000: Virtually use the appropriate shovel to generate 1000 samples with size balls. Compute the resulting 1000 replicates of the proportion of the shovel’s balls that are red. Visualize the distribution of these 1000 proportions red using a histogram. Run each of the following code segments individually and then compare the three resulting histograms. # Segment 1: sample size = 25 ------------------------------ # 1.a) Virtually use shovel 1000 times virtual_samples_25 &lt;- bowl %&gt;% rep_sample_n(size = 25, reps = 1000) # 1.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 25) # 1.c) Plot distribution via a histogram ggplot(virtual_prop_red_25, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 25 balls that were red&quot;, title = &quot;25&quot;) # Segment 2: sample size = 50 ------------------------------ # 2.a) Virtually use shovel 1000 times virtual_samples_50 &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) # 2.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) # 2.c) Plot distribution via a histogram ggplot(virtual_prop_red_50, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;50&quot;) # Segment 3: sample size = 100 ------------------------------ # 3.a) Virtually using shovel with 100 slots 1000 times virtual_samples_100 &lt;- bowl %&gt;% rep_sample_n(size = 100, reps = 1000) # 3.b) Compute resulting 1000 replicates of proportion red virtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 100) # 3.c) Plot distribution via a histogram ggplot(virtual_prop_red_100, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 100 balls that were red&quot;, title = &quot;100&quot;) For easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure 5.13. FIGURE 5.13: Comparing the distributions of proportion red for different sample sizes. Observe that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure 5.13, all three histograms appear to center around roughly 40%. We can be numerically explicit about the amount of variation in our three sets of 1000 values of prop_red using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable (see Appendix A.1 for a brief discussion on the properties of the standard deviation). For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function. # n = 25 virtual_prop_red_25 %&gt;% summarize(sd = sd(prop_red)) # n = 50 virtual_prop_red_50 %&gt;% summarize(sd = sd(prop_red)) # n = 100 virtual_prop_red_100 %&gt;% summarize(sd = sd(prop_red)) Let’s compare these three measures of distributional variation in Table 5.1. TABLE 5.1: Comparing standard deviations of proportions red for three different shovels Number of slots in shovel Standard deviation of proportions red 25 0.094 50 0.069 100 0.045 As we observed in Figure 5.13, as the sample size increases, the variation decreases. In other words, there is less variation in the 1000 values of the proportion red. So as the sample size increases, our guesses at the true proportion of the bowl’s balls that are red get more precise. Learning check (LC5.6) In Figure 5.13, we used shovels to take 1000 samples each, computed the resulting 1000 proportions of the shovel’s balls that were red, and then visualized the distribution of these 1000 proportions in a histogram. We did this for shovels with 25, 50, and 100 slots in them. As the size of the shovels increased, the histograms got narrower. In other words, as the size of the shovels increased from 25 to 50 to 100, did the 1000 proportions A. vary less, B. vary by the same amount, or C. vary more? (LC5.7) What summary statistic did we use to quantify how much the 1000 proportions red varied? A. The interquartile range B. The standard deviation C. The range: the largest value minus the smallest. 5.3 Sampling framework In both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to estimate the proportion of the bowl’s balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure 5.13 and Table 5.1: comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation: The effect of sampling variation on our estimates. The effect of sample size on sampling variation. Now that you have built some intuition relating to sampling, let’s now attach words and labels to the various concepts we’ve explored so far. Specifically in the next section, we’ll introduce terminology and notation as well as statistical definitions related to sampling. This will allow us to succinctly summarize and refer to the ideas behind sampling for the rest of this book. 5.3.1 Terminology and notation Let’s now attach words and labels to the various sampling concepts we’ve seen so far by introducing some terminology and mathematical notation. While they may seem daunting at first, we’ll make sure to tie each of them to sampling bowl activities you performed earlier. Furthermore, throughout this book we’ll give you plenty of opportunity for practice, as the best method for mastering these terms is repetition. The first set of terms and notation relate to populations: A population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case \\(N\\). A population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Canadians, the population parameter of interest is the population mean. A census is an exhaustive enumeration or counting of all \\(N\\) individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number \\(N\\) of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money). So in our sampling activities, the population is the collection of \\(N\\) = 2400 identically sized red and white balls in the bowl shown in Figure 5.1. Recall that we also represented the bowl “virtually” in the data frame bowl: bowl # A tibble: 2,400 × 2 ball_ID color &lt;int&gt; &lt;chr&gt; 1 1 white 2 2 white 3 3 white 4 4 red 5 5 white 6 6 white 7 7 red 8 8 white 9 9 red 10 10 white # … with 2,390 more rows The population parameter here is the proportion of the bowl’s balls that are red. Whenever we’re interested in a proportion of some value in a population, the population parameter has a specific name: the population proportion. We denote population proportions with the letter \\(p\\). We’ll see later on in Table 5.5 that we can also consider other types of population parameters, like population means and population regression slopes. In order to compute this population proportion \\(p\\) exactly, we need to first conduct a census by going through all \\(N\\) = 2400 and counting the number that are red. We then divide this count by 2400 to obtain the proportion red. You might be now asking yourself: “Wait. I understand that performing a census on the actual bowl would take a long time. But can’t we conduct a ‘virtual’ census using the virtual bowl?” You are absolutely correct! In fact when the authors of this book created the bowl data frame, they made its contents match the contents of actual bowl not by doing a census, but by reading the contents written on the box the bowl came in! Let’s conduct this “virtual” census by using the same dplyr verbs you used earlier to count the number of balls that are red: bowl %&gt;% summarize(red = sum(color == &quot;red&quot;)) # A tibble: 1 × 1 red &lt;int&gt; 1 900 Since 900 of the 2400 are red, the proportion is 900/2400 = 0.375 = 37.5%. So we know the value of the population parameter: in our case, the population proportion \\(p\\) is equal to 0.375. At this point, you might be further asking yourself: “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Great question! Normally, you wouldn’t do any sampling! However, the sampling activities we did this chapter are merely simulations of how sampling is done in real-life! We perform these simulations in order to study: The effect of sampling variation on our estimates. The effect of sample size on sampling variation. As we’ll see in Section 5.4 on genetic crosses, in real-life sampling not only will the population size \\(N\\) be very large making a census expensive, but sometimes we won’t even know how big the population is! For now however, we press on with our next set of terms and notation. The second set of terms and notation relate to samples: Sampling is the act of collecting a sample from the population, which we generally only do when we can’t perform a census. We mathematically denote the sample size using lower case \\(n\\), as opposed to upper case \\(N\\) which denotes the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). Thus sampling is a much cheaper alternative than performing a census. A point estimate, also known as a sample statistic, is a summary statistic computed from a sample that estimates the unknown population parameter. So previously we conducted sampling using a shovel with 50 slots to extract samples of size \\(n\\) = 50. To perform the virtual analog of this sampling, recall that we used the rep_sample_n() function as follows: virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) virtual_shovel # A tibble: 50 × 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1970 white 2 1 842 red 3 1 2287 white 4 1 599 white 5 1 108 white 6 1 846 red 7 1 390 red 8 1 344 white 9 1 910 white 10 1 1485 white # … with 40 more rows Using the sample of 50 balls contained in virtual_shovel, we generated an estimate of the proportion of the bowl’s balls that are red prop_red virtual_shovel %&gt;% summarize(num_red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = num_red / 50) # A tibble: 1 × 3 replicate num_red prop_red &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 12 0.24 So in our case, the value of prop_red is the point estimate of the population proportion \\(p\\) since it estimates the latter’s value. Furthermore, this point estimate has a specific name when considering proportions: the sample proportion. It is denoted using \\(\\widehat{p}\\) because it is a common convention in statistics to use a “hat” symbol to denote point estimates. The third set of terms relate to sampling methodology: the method used to collect samples. You’ll see here and throughout the rest of your book that the way you collect samples directly influences their quality. A sample is said to be representative if it roughly “looks like” the population. In other words, if the sample’s characteristics are a “good” representation of the population’s characteristics. We say a sample is generalizable if any results based on the sample can generalize to the population. In other words, if we can make “good” guesses about the population using the sample. We say a sampling procedure is biased if certain individuals in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every individual in a population has an equal chance of being sampled. We say a sample of \\(n\\) balls extracted using our shovel is representative of the population if it’s contents “roughly resemble” the contents of the bowl. If so, then the proportion of the shovel’s balls that are red can generalize to the proportion of the bowl’s \\(N\\) = 2400 balls that are red. Or expressed differently, \\(\\widehat{p}\\) is a “good guess” of \\(p\\). Now say we cheated when using the shovel and removed a number of white balls in favor of red balls. Then this sample would be biased towards red balls, and thus the sample would no longer be representative of the bowl. The fourth and final set of terms and notation relate to the goal of sampling: One way to ensure that a sample is unbiased and representative of the population is by using random sampling Inference is the act of “making a guess” about some unknown. Statistical inference is the act of making a guess about a population using a sample. In our case, since the rep_sample_n() function uses your computer’s random number generator, we were in fact performing random sampling. Let’s now put all four sets of terms and notation together, keeping our sampling activities in mind: Since we extracted a sample of \\(n\\) = 50 balls at random, we mixed all of the equally sized balls before using the shovel, then the contents of the shovel are unbiased and representative of the contents of the bowl, thus any result based on the shovel can generalize to the bowl, thus the sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus instead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel. What you have been performing is statistical inference. This is one of the most important concepts in all of statistics; so much so, this term is in the title of the original book: “Statistical Inference via Data Science”. More generally speaking, If the sampling of a sample of size \\(n\\) is done at random, then the sample is unbiased and representative of the population of size \\(N\\), thus any result based on the sample can generalize to the population, thus the point estimate is a “good guess” of the unknown population parameter, thus instead of performing a census, we can infer about the population using sampling. In the upcoming Chapter 6 on confidence intervals, we’ll introduce the infer package, which makes statistical inference “tidy” and transparent. It is why this second portion of the book is called “Statistical inference via infer.” Learning check (LC5.8) In the case of our bowl activity, what is the population parameter? Do we know its value? (LC5.9) What would performing a census in our bowl activity correspond to? Why did we not perform a census? (LC5.10) What purpose do point estimates serve in general? What is the name of the point estimate specific to our bowl activity? What is its mathematical notation? (LC5.11) How did we ensure that our tactile samples using the shovel were random? (LC5.12) Why is it important that sampling be done at random? (LC5.13) What are we inferring about the bowl based on the samples using the shovel? 5.3.2 Statistical definitions To further attach words and labels to the various sampling concepts we’ve seen so far, we also introduce some important statistical definitions related to sampling. As a refresher of our 1000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section 5.2, let’s display Figure 5.13 again as Figure 5.14. FIGURE 5.14: Previously seen three distributions of the sample proportion \\(\\widehat{p}\\). These types of distributions have a special name: sampling distributions of point estimates. Their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \\(\\widehat{p}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we can typically expect. Unfortunately, the term sampling distribution is often confused with a sample’s distribution which is merely the distribution of the values in a single sample. For example, observe the centers of all three sampling distributions: they are all roughly centered around 0.4 = 40%. Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of 0.2 = 20% when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions in Table 5.1, which we display again as Table 5.2: TABLE 5.2: Previously seen comparing standard deviations of proportions red for three different shovels Number of slots in shovel Standard deviation of proportions red 25 0.094 50 0.069 100 0.045 So as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: standard error of a point estimate. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel’s balls that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases. Similarly to confusion between sampling distributions with a sample’s distribution, people often confuse the standard error with the standard deviation. This is especially the case since a standard error is itself a kind of standard deviation. The best advice we can give is that a standard error is merely a kind of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error. To help reinforce these concepts, let’s re-display Figure 5.13 but using our new terminology, notation, and definitions relating to sampling in Figure 5.15. FIGURE 5.15: Three sampling distributions of the sample proportion \\(\\widehat{p}\\). Furthermore, let’s re-display Table 5.1 but using our new terminology, notation, and definitions relating to sampling in Table 5.3. TABLE 5.3: Standard errors of the sample proportion based on sample sizes of 25, 50, and 100 Sample size (n) Standard error of \\(\\widehat{p}\\) n = 25 0.094 n = 50 0.069 n = 100 0.045 Remember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error. Learning check (LC5.14) What purpose did the sampling distributions serve? (LC5.15) What does the standard error of the sample proportion \\(\\widehat{p}\\) quantify? 5.3.3 The moral of the story Let’s recap this section so far. We’ve seen that if a sample is generated at random, then the resulting point estimate is a “good guess” of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \\(\\widehat{p}\\) of the shovel’s balls that were red was a “good guess” of the population proportion \\(p\\) of the bowl’s balls that were red. However, what do we mean by our point estimate being a “good guess”? Sometimes, we’ll get an estimate that is less than the true value of the population parameter, while at other times we’ll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will “on average” be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion. In our sampling activities, sometimes our sample proportion \\(\\widehat{p}\\) was less than the true population proportion \\(p\\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \\(\\widehat{p}\\) were “on average” correct and thus were centered at the true value of the population proportion \\(p\\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an accurate estimate. Recall from earlier that the value of the population proportion \\(p\\) of the \\(N\\) = 2400 balls in the bowl was 900/2400 = 0.375 = 37.5%. We computed this value by performing a virtual census of bowl. Let’s re-display our sampling distributions from Figures 5.13 and 5.15, but now with a vertical red line marking the true population proportion \\(p\\) of balls that are red = 37.5% in Figure 5.16. We see that while there is a certain amount of error in the sample proportions \\(\\widehat{p}\\) for all three sampling distributions, on average the \\(\\widehat{p}\\) are centered at the true population proportion red \\(p\\). FIGURE 5.16: Three sampling distributions with population proportion \\(p\\) marked by vertical line. We also saw in this section that as your sample size \\(n\\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing standard error. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \\(\\widehat{p}\\) decreased. You can observe this behavior in Figure 5.16. This is also known as having a precise estimate. So random sampling ensures our point estimates are accurate, while on the other hand having a large sample size ensures our point estimates are precise. While the terms “accuracy” and “precision” may sound like they mean the same thing, there is a subtle difference. Accuracy describes how “on target” our estimates are, whereas precision describes how “consistent” our estimates are. Figure 5.17 illustrates the difference. FIGURE 5.17: Comparing accuracy and precision. At this point, you might be asking yourself: “Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn’t we be taking only one sample that’s as large as possible?”. If you did ask yourself these questions, your suspicion is correct! Recall from earlier when we asked ourselves “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Similarly, we took 1000 repeated samples as a simulation of how sampling is done in real-life! We used these simulations to study: The effect of sampling variation on our estimates. The effect of sample size on sampling variation. This is not how sampling is done in real life! In a real-life scenario, we wouldn’t take 1000 repeated/replicated samples, but rather a single sample that’s as large as we can afford. In Section 5.4, we’re going to study a real-life example of sampling: genetic cross results. Learning check (LC5.16) The table that follows is a version of Table 5.3 matching sample sizes \\(n\\) to different standard errors of the sample proportion \\(\\widehat{p}\\), but with the rows randomly re-ordered and the sample sizes removed. Fill in the table by matching the correct sample sizes to the correct standard errors. TABLE 5.4: Standard errors of \\(\\widehat{p}\\) based on n = 25, 50, 100 Sample size Standard error of \\(\\widehat{p}\\) n = 0.094 n = 0.045 n = 0.069 For the following four Learning checks, let the estimate be the sample proportion \\(\\widehat{p}\\): the proportion of a shovel’s balls that were red. It estimates the population proportion \\(p\\): the proportion of the bowl’s balls that were red. (LC5.17) What is the difference between an accurate and a precise estimate? (LC5.18) How do we ensure that an estimate is accurate? How do we ensure that an estimate is precise? (LC5.19) In a real-life situation, we would not take 1000 different samples to infer about a population, but rather only one. Then, what was the purpose of our exercises where we took 1000 different samples? (LC5.20) Figure 5.17 with the targets shows four combinations of “accurate versus precise” estimates. Draw four corresponding sampling distributions of the sample proportion \\(\\widehat{p}\\), like the one in the leftmost plot in Figure 5.16. 5.4 Case study: Genetic crosses Let’s now switch gears to a more realistic sampling scenario than our bowl activity: the results of a genetic cross. In practice, geneticists do not take 1000 repeated samples as we did in our previous sampling activities, but rather take only a single sample that’s as large as possible. The “curly wing” or Cy mutation causes the wings of Drosophila fruit flies to be curly. A cross between two curly wing flies produced 147 curly-winged and 70 normal-winged offspring. If curly-wings are due to a dominant mutation of a single gene, then according to Mendel’s law of equal segregation, we would expect three-quarters of the offspring to have curly-wings. Let’s tie elements of the real-life genetic results with our “tactile” and “virtual” bowl activity from Sections 5.1 and 5.2 using the terminology, notations, and definitions we learned in Section 5.3. You’ll see that our sampling activity with the bowl is an idealized version of what scientists are trying to estimate in real life. First, what is the (study) population of \\(N\\) individuals or observations of interest? Bowl: \\(N\\) = 2400 identically sized red and white balls Genetic cross: \\(N\\) = ? offspring of curly-winged flies Second, what is the population parameter? Bowl: The population proportion \\(p\\) of all the balls in the bowl that are red. Genetic cross: The population proportion \\(p\\) of all progeny resulting from curly-winged flies Third, what would a census look like? Bowl: Manually going over all \\(N\\) = 2400 balls and exactly computing the population proportion \\(p\\) of the balls that are red. Genetic cross: Counting all of the past, present, and future progeny of curly-winged flies. In this case, we don’t even know what the population size \\(N\\) is! Fourth, how do you perform sampling to obtain a sample of size \\(n\\)? Bowl: Using a shovel with \\(n\\) slots. Genetic cross: Cross one or more pairs of flies and count all of the progeny. In this case, the sample size of this genetic cross was \\(n = 217\\) offspring. Fifth, what is your point estimate also known as the sample statistic of the unknown population parameter? Bowl: The sample proportion \\(\\widehat{p}\\) of the balls in the shovel that were red. Genetic cross: The sample proportion \\(\\widehat{p}\\) of offspring in the sample that have curly wings. In this case, \\(\\widehat{p} = 147/217 = 0.677 = 67.7\\%\\). Sixth, is the sampling procedure representative? Bowl: Are the contents of the shovel representative of the contents of the bowl? Because we mixed the bowl before sampling, we can feel confident that they are. Genetic cross: Is the sample of \\(n = 217\\) offspring representative of all offspring of curly-winged flies? Presumably these flies were raised under standard conditions and are representative of offspring of all crosses between curly-winged parents. Seventh, are the samples generalizable to the greater population? Bowl: Is the sample proportion \\(\\widehat{p}\\) of the shovel’s balls that are red a “good guess” of the population proportion \\(p\\) of the bowl’s balls that are red? Given that the sample was representative, the answer is yes. Genetic cross: Is the sample proportion \\(\\widehat{p}\\) = 0.677 of the sample of offspring with curly wings a “good guess” of the population proportion \\(p\\) of all offspring of curly-winged parents? In other words, can we confidently say that roughly 68% of all offspring of curly-winged parents are curly-winged? Again, this depends on whether the sampling was random. Eighth, is the sampling procedure unbiased? In other words, do all observations have an equal chance of being included in the sample? Bowl: Since each ball was equally sized and we mixed the bowl before using the shovel, each ball had an equal chance of being included in a sample and hence the sampling was unbiased. Genetic cross: Did all offspring of a cross between curly-winged flies have an equal chance at being represented in this genetic cross? Again, this depends on whether the sampling was random and not biased. Ninth and lastly, was the sampling done at random? Bowl: As long as you mixed the bowl sufficiently before sampling, your samples would be random. Genetic cross: Was the sample conducted at random? Presumably so, but we can’t answer this question without knowing more about the sampling methodology . We’ll discuss this more at the end of this section. In other words, the genetic cross can be thought of as an instance of using the shovel to sample balls from the bowl. Furthermore, if another genetic cross between a pair of curly-winged flies were conducted under the same conditions, they would likely get a different estimate than 68%. This is due to sampling variation. Let’s now revisit the sampling paradigm from Subsection 5.3.1: In general: If the sampling of a sample of size \\(n\\) is done at random, then the sample is unbiased and representative of the population of size \\(N\\), thus any result based on the sample can generalize to the population, thus the point estimate is a “good guess” of the unknown population parameter, thus instead of performing a census, we can infer about the population using sampling. Specific to the bowl: Since we extracted a sample of \\(n\\) = 50 balls at random, in other words we mixed all of the equally sized balls before using the shovel, then the contents of the shovel are unbiased and representative of the contents of the bowl, thus any result based on the shovel can generalize to the bowl, thus the sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus instead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel. Specific to the genetic cross: If repeated a genetic cross between curly-winged flies under the same conditions and counted 241 offspring, then these 241 offspring would be an unbiased and representative sample of all offspring from curly-winged flies, thus any results based on this sample of 241 offspring can generalize to the entire population of all offspring of curly-winged parents, thus the observed frequency of 68% curly-winged offspring is a good guess of the true proportion of curly-winged offspring of infinite size, thus instead of performing a nearly infinite count of offspring of curly-winged parents, we can infer about all such offspring using this sample. So as you can see, it was critical for the sample obtained by this cross to be truly random in order to infer about all offspring of curly-winged parents. Was their sample truly random? It’s hard to answer such questions without knowing about the sampling methodology they used. For example, if not all offspring of curly-winged parents were counted because they died prior to pupation, then some offspring would be missing from our count and therefore not represented in the sample. Ensuring that our samples were random was easy to do in our sampling bowl exercises; however, in a real-life situation like the genetic cross, this can be harder to accomplish. Learning check Comment on the representativeness of the following sampling methodologies: (LC5.21) To attract females, the plumage of male birds is often much more colorful than females of the same species. You want to know what proportion of cardinals are males, so you spend the day sitting in a field. 5.5 Central Limit Theorem This chapter began with (virtual) access to a large bowl of balls (our population) and a desire to figure out the proportion of red balls. Despite having access to this population, in reality, you almost never will have access to the population, either because the population is too large, ever changing, or too expensive to take a census of. Accepting this reality means accepting that we need to use statistical inference. In Section 5.3.1, we stated that “statistical inference is the act of making a guess about a population using a sample.” But how do we do this inference? In the previous section, we defined the sampling framework only to state that in reality we take one large sample, instead of many samples as done in the sampling framework (which we modeled physically by taking many samples from the bowl). In reality, we take only one sample and use that one sample to make statements about the population parameter. This ability of making statements about the population is allowable by a famous theorem, or mathematically proven truth, called the Central Limit Theorem. What you visualized in Figures 5.13 and 5.15 and summarized in Tables 5.1 and 5.3 was a demonstration of this theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow. In other words, as our sample size gets larger (1) the sampling distribution of a point estimate (like a sample proportion) increasingly follows a normal distribution and (2) the variation of these sampling distributions gets smaller, as quantified by their standard errors. Shuyi Chiou, Casey Dunn, and Pathikrit Bhattacharyya created a 3-minute and 38-second video at https://youtu.be/jvoxEYmQHNM explaining this crucial statistical theorem using the average weight of wild bunny rabbits and the average wingspan of dragons as examples. Figure 5.18 shows a preview of this video. FIGURE 5.18: Preview of Central Limit Theorem video. Here’s what is so surprising about the Central Limit Theorem: regardless of the shape of the underlying population distribution, the sampling distribution of means (such as the sample mean of bunny weights or the sample mean of the length of dragon wings) and proportions (such as the sample proportion red in our shovels) will be normal. Normal distributions are defined by where they are centered and how wide they are, and the Central Limit Theorem gives us both: The sampling distribution of the point estimate is centered at the true population parameter We have an estimate for how wide the sampling distribution of the point estimate is, given by the standard error (which we will discuss further in Chapter 6) What the Central Limit Theorem creates for us is a ladder between a single sample and the population. By the Central Limit Theorem, we can say that (1) our sample’s point estimate is drawn from a normal distribution centered at the true population parameter and (2) that the width of that normal distribution is governed by the standard error of our point estimate. Relating this to our bowl, if we pull one sample and get the sample proportion of red balls \\(\\widehat{p}\\), this value of \\(\\widehat{p}\\) is drawn from the normal curve centered at the true population proportion of red balls \\(p\\) with the computed standard error. 5.6 Sampling scenarios In this chapter, we performed both tactile and virtual sampling exercises to infer about an unknown proportion. We also presented a case study of sampling in real life with genetic crosses. In each case, we used the sample proportion \\(\\widehat{p}\\) to estimate the population proportion \\(p\\). However, we are not just limited to scenarios related to proportions. In other words, we can use sampling to estimate other population parameters using other point estimates as well. We present four more such scenarios in Table 5.5. TABLE 5.5: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) In the rest of this book, we’ll cover all the remaining scenarios as follows: In Chapter 6, we’ll cover examples of statistical inference for Scenario 2: The mean length \\(\\mu\\) of all genes in the human genome. Scenario 3: The difference \\(p_1 - p_2\\) in the proportion of people who yawn when seeing someone else yawn first minus the proportion of people who yawn without seeing someone else yawn first. This is an example of two-sample inference. In Chapter 7, we’ll cover an example of statistical inference for Scenario 4: The difference \\(\\mu_1 - \\mu_2\\) in mean . This is another example of two-sample inference. In Chapter 9, we’ll cover an example of statistical inference for regression by revisiting the regression models for tooth growth as a function of various explanatory variables you’ll see in Chapter 8. Scenario 5: The slope \\(\\beta_1\\) of the population regression line. 5.7 Conclusion Chapter Learning Summary When a complete census of a population is impractical, a subset of individuals, or sample, can be used to estimate an unknown parameter, such as mean weight, of a population. Sampling can be performed tactically with random draws or virtually (and more quickly) with computer simulations. The subset of individuals chosen will vary between samples; consequently, the estimate will almost always differ between samples (sampling variation) and from the true population parameter (sampling error). Repeated sampling of a population produces a sampling distribution, where most sample estimates differ somewhat from the true population parameter, but some differ greatly. As the number of samples increases, the distribution of sample estimates more closely resembles a bell-shaped, or normal, distribution. The mean of the sampling distribution approximates the population parameter, and 95% of the sample estimates lie within ~2 standard deviations of this mean. As the size of each sample increases, the standard deviation of the sampling distribution (called the standard error) narrows; therefore, sample estimates very close to the population parameter become more likely. Random sampling, where each individual is equally likely to be chosen, ensures that a sample is unbiased and representative of the population. Accurate estimates are on target, while precise estimates are consistent. Random sampling and large sample sizes yield accurate and precise estimates. According to the Central Limit Theorem, the sampling distribution will become normal with increasing sample sizes; this property allows us a sample estimate to be used to infer a population parameter. 5.7.1 What’s to come? Recall in our Genetic cross case study in Section 5.4 that based on this particular sample, the best guess of the proportion of offspring of all curly-winged parents was 68%. However, this isn’t the end of the story. Most samples won’t produce an estimate that’s perfectly right; there will always be a certain amount of error caused by sampling variation. Public opinion pollsters refer to this uncertainty as the “margin of error.” As we’ll see in the next chapter, data scientists use a related term called the confidence interval, the interval that is likely to contain our true population proportion. "],["6-confidence-intervals.html", "Chapter 6 Bootstrapping and Confidence Intervals 6.1 Pennies activity 6.2 Computer simulation of resampling 6.3 Understanding confidence intervals 6.4 Constructing confidence intervals 6.5 Interpreting confidence intervals 6.6 Case study: Is yawning contagious? 6.7 Comparing bootstrap and sampling distributions 6.8 Theory-based confidence intervals 6.9 Conclusion", " Chapter 6 Bootstrapping and Confidence Intervals In Chapter 5, we studied sampling. We started with a “tactile” exercise where we wanted to know the proportion of balls in the sampling bowl in Figure 5.1 that are red. While we could have performed an exhaustive count, this would have been a tedious process. So instead, we used a shovel to extract a sample of 50 balls and used the resulting proportion that were red as an estimate. Furthermore, we made sure to mix the bowl’s contents before every use of the shovel. Because of the randomness created by the mixing, different uses of the shovel yielded different proportions red and hence different estimates of the proportion of the bowl’s balls that are red. We then mimicked this “tactile” sampling exercise with an equivalent “virtual” sampling exercise performed on the computer. Using our computer’s random number generator, we quickly mimicked the above sampling procedure a large number of times. In Subsection 5.2.5, we quickly repeated this sampling procedure 1000 times, using three different “virtual” shovels with 25, 50, and 100 slots. We visualized these three sets of 1000 estimates in Figure 5.16 and saw that as the sample size increased, the variation in the estimates decreased. In doing so, what we did was construct sampling distributions. The motivation for taking 1000 repeated samples and visualizing the resulting estimates was to study how these estimates varied from one sample to another; in other words, we wanted to study the effect of sampling variation. We quantified the variation of these estimates using their standard deviation, which has a special name: the standard error. In particular, we saw that as the sample size increased from 25 to 50 to 100, the standard error decreased and thus the sampling distributions narrowed. Larger sample sizes led to more precise estimates that varied less around the center. We then tied these sampling exercises to terminology and mathematical notation related to sampling in Subsection 5.3.1. Our study population was the large bowl with \\(N\\) = 2400 balls, while the population parameter, the unknown quantity of interest, was the population proportion \\(p\\) of the bowl’s balls that were red. Since performing a census would be expensive in terms of time and energy, we instead extracted a sample of size \\(n\\) = 50. The point estimate, also known as a sample statistic, used to estimate \\(p\\) was the sample proportion \\(\\widehat{p}\\) of these 50 sampled balls that were red. Furthermore, since the sample was obtained at random, it can be considered as unbiased and representative of the population. Thus any results based on the sample could be generalized to the population. Therefore, the proportion of the shovel’s balls that were red was a “good guess” of the proportion of the bowl’s balls that are red. In other words, we used the sample to infer about the population. However, as described in Section 5.2, both the tactile and virtual sampling exercises are not what one would do in real life; this was merely an activity used to study the effects of sampling variation. In a real-life situation, we would not take 1000 samples of size \\(n\\), but rather take a single representative sample that’s as large as possible. Additionally, we knew that the true proportion of the bowl’s balls that were red was 37.5%. In a real-life situation, we will not know what this value is. Because if we did, then why would we take a sample to estimate it? An example of a realistic sampling situation would be a genetic cross, like the one you saw in Section 5.4. A geneticist does not know the true proportion of all progeny of curly-winged flies, and thus they take a single sample of as many offspring as feasible to estimate this value. So how does one quantify the effects of sampling variation when you only have a single sample to work with? You cannot directly study the effects of sampling variation when you only have one sample. One common method to study this is bootstrapping resampling, which will be the focus of the earlier sections of this chapter. Furthermore, what if we would like not only a single estimate of the unknown population parameter, but also a range of highly plausible values? Going back to the genetic cross, the results indicated that 67.7% of the offsping had curly wings. But this is only a best guess of the true population proportion; other crosses would probably produce slightly different estimates. The range of plausible values for the true population proportion is what’s known as a confidence interval, which will be the focus of the later sections of this chapter. Chapter Learning Objectives At the end of this chapter, you should be able to… • Quantify the effects of sampling variation from a single sample using bootstrap resampling with replacement. • Calculate the confidence interval, range of highly probable values, for an unknown population parameter. • Explain each step of the infer workflow for calculating confidence intervals. • Perform exploratory data analysis by examining raw data values, computing summary statistics, and visualizing the data. • Calculate a confidence interval from a bootstrap distribution using the percentile method and standard error method. • Explain the effect of higher confidence levels and larger sample sizes on the width of a confidence interval. • Methods based on mathematical theories can also be used to compute the standard error and confidence interval if the sampling distribution is normally distributed. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 4.3 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to tidy format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(infer) 6.1 Pennies activity As we did in Chapter 5, we’ll begin with a hands-on tactile activity. 6.1.1 What is the average year on US pennies in 2019? Try to imagine all the pennies being used in the United States in 2019. That’s a lot of pennies! Now say we’re interested in the average year of minting of all these pennies. One way to compute this value would be to gather up all pennies being used in the US, record the year, and compute the average. However, this would be near impossible! So instead, let’s collect a sample of 50 pennies from a local bank in downtown Northampton, Massachusetts, USA as seen in Figure 6.1. FIGURE 6.1: Collecting a sample of 50 US pennies from a local bank. An image of these 50 pennies can be seen in Figure 6.2. For each of the 50 pennies starting in the top left, progressing row-by-row, and ending in the bottom right, we assigned an “ID” identification variable and marked the year of minting. FIGURE 6.2: 50 US pennies labelled. The moderndive package contains this data on our 50 sampled pennies in the pennies_sample data frame: pennies_sample # A tibble: 50 × 2 ID year &lt;int&gt; &lt;dbl&gt; 1 1 2002 2 2 1986 3 3 2017 4 4 1988 5 5 2008 6 6 1983 7 7 2008 8 8 1996 9 9 2004 10 10 2000 # … with 40 more rows The pennies_sample data frame has 50 rows corresponding to each penny with two variables. The first variable ID corresponds to the ID labels in Figure 6.2, whereas the second variable year corresponds to the year of minting saved as a numeric variable, also known as a double (dbl). Based on these 50 sampled pennies, what can we say about all US pennies in 2019? Let’s study some properties of our sample by performing an exploratory data analysis. When working with a new data set, an important first step is to perform an exploratory data analysis, or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA: Most crucially, look at the raw data values. Compute summary statistics, such as means, medians, and interquartile ranges. Create data visualizations. We’ve already performed the first common step in an exploratory data analysis: looking at the raw data values. Getting an early sense of what your raw data looks like can often prevent many larger issues down the road. Next, let’s visualize the distribution of the year of these 50 pennies using our data visualization tools from Chapter 2. Since year is a numerical variable, we use a histogram in Figure 6.3 to visualize its distribution. ggplot(pennies_sample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) FIGURE 6.3: Distribution of year on 50 US pennies. We observe a slightly left-skewed distribution, since most pennies fall between 1980 and 2010 with only a few pennies older than 1970. What is the average year for the 50 sampled pennies? Eyeballing the histogram it appears to be around 1990. Let’s now compute this value exactly using our data wrangling tools from Chapter 3. x_bar &lt;- pennies_sample %&gt;% summarize(mean_year = mean(year)) x_bar # A tibble: 1 × 1 mean_year &lt;dbl&gt; 1 1995.44 Thus, if we’re willing to assume that pennies_sample is a representative sample from all US pennies, a “good guess” of the average year of minting of all US pennies would be 1995.44. In other words, around 1995. This should all start sounding similar to what we did previously in Chapter 5! In Chapter 5, our study population was the bowl of \\(N\\) = 2400 balls. Our population parameter was the population proportion of these balls that were red, denoted by \\(p\\). In order to estimate \\(p\\), we extracted a sample of 50 balls using the shovel. We then computed the relevant point estimate: the sample proportion of these 50 balls that were red, denoted mathematically by \\(\\widehat{p}\\). Here our population is \\(N\\) = whatever the number of pennies are being used in the US, a value which we don’t know and probably never will. The population parameter of interest is now the population mean year of all these pennies, a value denoted mathematically by the Greek letter \\(\\mu\\) (pronounced “mu”). In order to estimate \\(\\mu\\), we went to the bank and obtained a sample of 50 pennies and computed the relevant point estimate: the sample mean year of these 50 pennies, denoted mathematically by \\(\\overline{x}\\) (pronounced “x-bar”). An alternative and more intuitive notation for the sample mean is \\(\\widehat{\\mu}\\). However, this is unfortunately not as commonly used, so in this book we’ll stick with convention and always denote the sample mean as \\(\\overline{x}\\). We summarize the correspondence between the sampling bowl exercise in Chapter 5 and our pennies exercise in Table 6.1, which are the first two rows of the previously seen Table 5.5. TABLE 6.1: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) Going back to our 50 sampled pennies in Figure 6.2, the point estimate of interest is the sample mean \\(\\overline{x}\\) of 1995.44. This quantity is an estimate of the population mean year of all US pennies \\(\\mu\\). Recall that we also saw in Chapter 5 that such estimates are prone to sampling variation. For example, in this particular sample in Figure 6.2, we observed three pennies with the year 1999. If we sampled another 50 pennies, would we observe exactly three pennies with the year 1999 again? More than likely not. We might observe none, one, two, or maybe even all 50! The same can be said for the other 26 unique years that are represented in our sample of 50 pennies. To study the effects of sampling variation in Chapter 5, we took many samples, something we could easily do with our shovel. In our case with pennies, however, how would we obtain another sample? By going to the bank and getting another roll of 50 pennies. Say we’re feeling lazy, however, and don’t want to go back to the bank. How can we study the effects of sampling variation using our single sample? We will do so using a technique known as bootstrap resampling with replacement, which we now illustrate. 6.1.2 Resampling once Step 1: Let’s print out identically sized slips of paper representing our 50 pennies as seen in Figure 6.4. FIGURE 6.4: Step 1: 50 slips of paper representing 50 US pennies. Step 2: Put the 50 slips of paper into a hat or tuque as seen in Figure 6.5. FIGURE 6.5: Step 2: Putting 50 slips of paper in a hat. Step 3: Mix the hat’s contents and draw one slip of paper at random as seen in Figure 6.6. Record the year. FIGURE 6.6: Step 3: Drawing one slip of paper at random. Step 4: Put the slip of paper back in the hat! In other words, replace it as seen in Figure 6.7. FIGURE 6.7: Step 4: Replacing slip of paper. Step 5: Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years. What we just performed was a resampling of the original sample of 50 pennies. We are not sampling 50 pennies from the population of all US pennies as we did in our trip to the bank. Instead, we are mimicking this act by resampling 50 pennies from our original sample of 50 pennies. Now ask yourselves, why did we replace our resampled slip of paper back into the hat in Step 4? Because if we left the slip of paper out of the hat each time we performed Step 4, we would end up with the same 50 original pennies! In other words, replacing the slips of paper induces sampling variation. Being more precise with our terminology, we just performed a resampling with replacement from the original sample of 50 pennies. Had we left the slip of paper out of the hat each time we performed Step 4, this would be resampling without replacement. Let’s study our 50 resampled pennies via an exploratory data analysis. First, let’s load the data into R by manually creating a data frame pennies_resample of our 50 resampled values. We’ll do this using the tibble() command from the dplyr package. Note that the 50 values you resample will almost certainly not be the same as ours given the inherent randomness. pennies_resample &lt;- tibble( year = c(1976, 1962, 1976, 1983, 2017, 2015, 2015, 1962, 2016, 1976, 2006, 1997, 1988, 2015, 2015, 1988, 2016, 1978, 1979, 1997, 1974, 2013, 1978, 2015, 2008, 1982, 1986, 1979, 1981, 2004, 2000, 1995, 1999, 2006, 1979, 2015, 1979, 1998, 1981, 2015, 2000, 1999, 1988, 2017, 1992, 1997, 1990, 1988, 2006, 2000) ) The 50 values of year in pennies_resample represent a resample of size 50 from the original sample of 50 pennies. We display the 50 resampled pennies in Figure 6.8. FIGURE 6.8: 50 resampled US pennies labelled. Let’s compare the distribution of the numerical variable year of our 50 resampled pennies with the distribution of the numerical variable year of our original sample of 50 pennies in Figure 6.9. ggplot(pennies_resample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) + labs(title = &quot;Resample of 50 pennies&quot;) ggplot(pennies_sample, aes(x = year)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) + labs(title = &quot;Original sample of 50 pennies&quot;) FIGURE 6.9: Comparing year in the resampled pennies_resample with the original sample pennies_sample. Observe in Figure 6.9 that while the general shapes of both distributions of year are roughly similar, they are not identical. Recall from the previous section that the sample mean of the original sample of 50 pennies from the bank was 1995.44. What about for our resample? Any guesses? Let’s have dplyr help us out as before: pennies_resample %&gt;% summarize(mean_year = mean(year)) # A tibble: 1 × 1 mean_year &lt;dbl&gt; 1 1996 We obtained a different mean year of 1996. This variation is induced by the resampling with replacement we performed earlier. What if we repeated this resampling exercise many times? Would we obtain the same mean year each time? In other words, would our guess at the mean year of all pennies in the US in 2019 be exactly 1996 every time? Just as we did in Chapter 5, let’s perform this resampling activity with the help of some of our friends: 35 friends in total. 6.1.3 Resampling 35 times Each of our 35 friends will repeat the same five steps: Start with 50 identically sized slips of paper representing the 50 pennies. Put the 50 small pieces of paper into a hat or beanie cap. Mix the hat’s contents and draw one slip of paper at random. Record the year in a spreadsheet. Replace the slip of paper back in the hat! Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years. Since we had 35 of our friends perform this task, we ended up with \\(35 \\cdot 50 = 1750\\) values. We recorded these values in a shared spreadsheet with 50 rows (plus a header row) and 35 columns. We display a snapshot of the first 10 rows and five columns of this shared spreadsheet in Figure 6.10. FIGURE 6.10: Snapshot of shared spreadsheet of resampled pennies. For your convenience, we’ve taken these 35 \\(\\cdot\\) 50 = 1750 values and saved them in pennies_resamples, a “tidy” data frame included in the moderndive package. We saw what it means for a data frame to be “tidy” in Subsection 4.1.1. pennies_resamples # A tibble: 1,750 × 3 # Groups: name [35] replicate name year &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 Arianna 1988 2 1 Arianna 2002 3 1 Arianna 2015 4 1 Arianna 1998 5 1 Arianna 1979 6 1 Arianna 1971 7 1 Arianna 1971 8 1 Arianna 2015 9 1 Arianna 1988 10 1 Arianna 1979 # … with 1,740 more rows What did each of our 35 friends obtain as the mean year? Once again, dplyr to the rescue! After grouping the rows by name, we summarize each group of 50 rows by their mean year: resampled_means &lt;- pennies_resamples %&gt;% group_by(name) %&gt;% summarize(mean_year = mean(year)) resampled_means # A tibble: 35 × 2 name mean_year &lt;chr&gt; &lt;dbl&gt; 1 Arianna 1992.5 2 Artemis 1996.42 3 Bea 1996.32 4 Camryn 1996.9 5 Cassandra 1991.22 6 Cindy 1995.48 7 Claire 1995.52 8 Dahlia 1998.48 9 Dan 1993.86 10 Eindra 1993.56 # … with 25 more rows Observe that resampled_means has 35 rows corresponding to the 35 means based on the 35 resamples. Furthermore, observe the variation in the 35 values in the variable mean_year. Let’s visualize this variation using a histogram in Figure 6.11. Recall that adding the argument boundary = 1990 to the geom_histogram() sets the binning structure so that one of the bin boundaries is at 1990 exactly. ggplot(resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;Sampled mean year&quot;) FIGURE 6.11: Distribution of 35 sample means from 35 resamples. Observe in Figure 6.11 that the distribution looks roughly normal and that we rarely observe sample mean years less than 1992 or greater than 2000. Also observe how the distribution is roughly centered at 1995, which is close to the sample mean of 1995.44 of the original sample of 50 pennies from the bank. 6.1.4 What did we just do? What we just demonstrated in this activity is the statistical procedure known as bootstrap resampling with replacement. We used resampling to mimic the sampling variation we studied in Chapter 5 on sampling. However, in this case, we did so using only a single sample from the population. In fact, the histogram of sample means from 35 resamples in Figure 6.11 is called the bootstrap distribution. It is an approximation to the sampling distribution of the sample mean, in the sense that both distributions will have a similar shape and similar spread. In fact in the upcoming Section 6.9, we’ll show you that this is the case. Using this bootstrap distribution, we can study the effect of sampling variation on our estimates. In particular, we’ll study the typical “error” of our estimates, known as the standard error. In Section 6.2 we’ll mimic our tactile resampling activity virtually on the computer, allowing us to quickly perform the resampling many more than 35 times. In Section 6.3 we’ll define the statistical concept of a confidence interval, which builds off the concept of bootstrap distributions. In Section 6.4, we’ll construct confidence intervals using the dplyr package, as well as a new package: the infer package for “tidy” and transparent statistical inference. We’ll introduce the “tidy” statistical inference framework that was the motivation for the infer package pipeline. The infer package will be the driving package throughout the rest of this book. As we did in Chapter 5, we’ll tie all these ideas together with a real-life case study in Section 6.6. This time we’ll look at data from an experiment about yawning from the US television show Mythbusters. 6.2 Computer simulation of resampling Let’s now mimic our tactile resampling activity virtually with a computer. 6.2.1 Virtually resampling once First, let’s perform the virtual analog of resampling once. Recall that the pennies_sample data frame included in the moderndive package contains the years of our original sample of 50 pennies from the bank. Furthermore, recall in Chapter 5 on sampling that we used the rep_sample_n() function as a virtual shovel to sample balls from our virtual bowl of 2400 balls as follows: virtual_shovel &lt;- bowl %&gt;% rep_sample_n(size = 50) Let’s modify this code to perform the resampling with replacement of the 50 slips of paper representing our original sample 50 pennies: virtual_resample &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE) Observe how we explicitly set the replace argument to TRUE in order to tell rep_sample_n() that we would like to sample pennies with replacement. Had we not set replace = TRUE, the function would’ve assumed the default value of FALSE and hence done resampling without replacement. Additionally, since we didn’t specify the number of replicates via the reps argument, the function assumes the default of one replicate reps = 1. Lastly, observe also that the size argument is set to match the original sample size of 50 pennies. Let’s look at only the first 10 out of 50 rows of virtual_resample: virtual_resample # A tibble: 50 × 3 # Groups: replicate [1] replicate ID year &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 37 1962 2 1 1 2002 3 1 45 1997 4 1 28 2006 5 1 50 2017 6 1 10 2000 7 1 16 2015 8 1 47 1982 9 1 23 1998 10 1 44 2015 # … with 40 more rows The replicate variable only takes on the value of 1 corresponding to us only having reps = 1, the ID variable indicates which of the 50 pennies from pennies_sample was resampled, and year denotes the year of minting. Let’s now compute the mean year in our virtual resample of size 50 using data wrangling functions included in the dplyr package: virtual_resample %&gt;% summarize(resample_mean = mean(year)) # A tibble: 1 × 2 replicate resample_mean &lt;int&gt; &lt;dbl&gt; 1 1 1996 As we saw when we did our tactile resampling exercise, the resulting mean year is different than the mean year of our 50 originally sampled pennies of 1995.44. 6.2.2 Virtually resampling 35 times Let’s now perform the virtual analog of our 35 friends’ resampling. Using these results, we’ll be able to study the variability in the sample means from 35 resamples of size 50. Let’s first add a reps = 35 argument to rep_sample_n() to indicate we would like 35 replicates. Thus, we want to repeat the resampling with the replacement of 50 pennies 35 times. virtual_resamples &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 35) virtual_resamples # A tibble: 1,750 × 3 # Groups: replicate [35] replicate ID year &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 21 1981 2 1 34 1985 3 1 4 1988 4 1 11 1994 5 1 26 1979 6 1 8 1996 7 1 19 1983 8 1 21 1981 9 1 49 2006 10 1 2 1986 # … with 1,740 more rows The resulting virtual_resamples data frame has 35 \\(\\cdot\\) 50 = 1750 rows corresponding to 35 resamples of 50 pennies. Let’s now compute the resulting 35 sample means using the same dplyr code as we did in the previous section, but this time adding a group_by(replicate): virtual_resampled_means &lt;- virtual_resamples %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) virtual_resampled_means # A tibble: 35 × 2 replicate mean_year &lt;int&gt; &lt;dbl&gt; 1 1 1995.58 2 2 1999.74 3 3 1993.7 4 4 1997.1 5 5 1999.42 6 6 1995.12 7 7 1994.94 8 8 1997.78 9 9 1991.26 10 10 1996.88 # … with 25 more rows Observe that virtual_resampled_means has 35 rows, corresponding to the 35 resampled means. Furthermore, observe that the values of mean_year vary. Let’s visualize this variation using a histogram in Figure 6.12. ggplot(virtual_resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;Resample mean year&quot;) FIGURE 6.12: Distribution of 35 sample means from 35 resamples. Let’s compare our virtually constructed bootstrap distribution with the one our 35 friends constructed via our tactile resampling exercise in Figure 6.13. Observe how they are somewhat similar, but not identical. FIGURE 6.13: Comparing distributions of means from resamples. Recall that in the “resampling with replacement” scenario we are illustrating here, both of these histograms have a special name: the bootstrap distribution of the sample mean. Furthermore, recall they are an approximation to the sampling distribution of the sample mean, a concept you saw in Chapter 5 on sampling. These distributions allow us to study the effect of sampling variation on our estimates of the true population mean, in this case the true mean year for all US pennies. However, unlike in Chapter 5 where we took multiple samples (something one would never do in practice), bootstrap distributions are constructed by taking multiple resamples from a single sample: in this case, the 50 original pennies from the bank. 6.2.3 Virtually resampling 1000 times Remember that one of the goals of resampling with replacement is to construct the bootstrap distribution, which is an approximation of the sampling distribution. However, the bootstrap distribution in Figure 6.12 is based only on 35 resamples and hence looks a little coarse. Let’s increase the number of resamples to 1000, so that we can hopefully better see the shape and the variability between different resamples. # Repeat resampling 1000 times virtual_resamples &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) # Compute 1000 sample means virtual_resampled_means &lt;- virtual_resamples %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) However, in the interest of brevity, going forward let’s combine these two operations into a single chain of pipe (%&gt;%) operators: virtual_resampled_means &lt;- pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) virtual_resampled_means # A tibble: 1,000 × 2 replicate mean_year &lt;int&gt; &lt;dbl&gt; 1 1 1992.6 2 2 1994.78 3 3 1994.74 4 4 1997.88 5 5 1990 6 6 1999.48 7 7 1990.26 8 8 1993.2 9 9 1994.88 10 10 1996.3 # … with 990 more rows In Figure 6.14 let’s visualize the bootstrap distribution of these 1000 means based on 1000 virtual resamples: ggplot(virtual_resampled_means, aes(x = mean_year)) + geom_histogram(binwidth = 1, color = &quot;white&quot;, boundary = 1990) + labs(x = &quot;sample mean&quot;) FIGURE 6.14: Bootstrap resampling distribution based on 1000 resamples. Note here that the bell shape is starting to become much more apparent. We now have a general sense for the range of values that the sample mean may take on. But where is this histogram centered? Let’s compute the mean of the 1000 resample means: virtual_resampled_means %&gt;% summarize(mean_of_means = mean(mean_year)) # A tibble: 1 × 1 mean_of_means &lt;dbl&gt; 1 1995.36 The mean of these 1000 means is 1995.36, which is quite close to the mean of our original sample of 50 pennies of 1995.44. This is the case since each of the 1000 resamples is based on the original sample of 50 pennies. Congratulations! You’ve just constructed your first bootstrap distribution! In the next section, you’ll see how to use this bootstrap distribution to construct confidence intervals. Learning check (LC6.1) What is the chief difference between a bootstrap distribution and a sampling distribution? (LC6.2) Looking at the bootstrap distribution for the sample mean in Figure 6.14, between what two values would you say most values lie? 6.3 Understanding confidence intervals Let’s start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand, you could use a spear, while on the other you could use a net. Using the net will probably allow you to catch more fish! Now think back to our pennies exercise where you are trying to estimate the true population mean year \\(\\mu\\) of all US pennies. Think of the value of \\(\\mu\\) as a fish. On the one hand, we could use the appropriate point estimate/sample statistic to estimate \\(\\mu\\), which we saw in Table 6.1 is the sample mean \\(\\overline{x}\\). Based on our sample of 50 pennies from the bank, the sample mean was 1995.44. Think of using this value as “fishing with a spear.” What would “fishing with a net” correspond to? Look at the bootstrap distribution in Figure 6.14 once more. Between which two years would you say that “most” sample means lie? While this question is somewhat subjective, saying that most sample means lie between 1992 and 2000 would not be unreasonable. Think of this interval as the “net.” What we’ve just illustrated is the concept of a confidence interval, which we’ll abbreviate with “CI” throughout this book. As opposed to a point estimate/sample statistic that estimates the value of an unknown population parameter with a single value, a confidence interval gives what can be interpreted as a range of plausible values. Going back to our analogy, point estimates/sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets. FIGURE 6.15: Analogy of difference between point estimates and confidence intervals. Our proposed interval of 1992 to 2000 was constructed by eye and was thus somewhat subjective. We now introduce two methods for constructing such intervals in a more exact fashion: the percentile method and the standard error method. Both methods for confidence interval construction share some commonalities. First, they are both constructed from a bootstrap distribution, as you constructed in Subsection 6.2.3 and visualized in Figure 6.14. Second, they both require you to specify the confidence level. Commonly used confidence levels include 90%, 95%, and 99%. All other things being equal, higher confidence levels correspond to wider confidence intervals, and lower confidence levels correspond to narrower confidence intervals. In this book, we’ll be mostly using 95% and hence constructing “95% confidence intervals for \\(\\mu\\)” for our pennies activity. 6.3.1 Percentile method One method to construct a confidence interval is to use the middle 95% of values of the bootstrap distribution. We can do this by computing the 2.5th and 97.5th percentiles, which are 1991.059 and 1999.283, respectively. This is known as the percentile method for constructing confidence intervals. For now, let’s focus only on the concepts behind a percentile method constructed confidence interval; we’ll show you the code that computes these values in the next section. Let’s mark these percentiles on the bootstrap distribution with vertical lines in Figure 6.16. About 95% of the mean_year variable values in virtual_resampled_means fall between 1991.059 and 1999.283, with 2.5% to the left of the leftmost line and 2.5% to the right of the rightmost line. FIGURE 6.16: Percentile method 95% confidence interval. Interval endpoints marked by vertical lines. 6.3.2 Standard error method Recall in Appendix A.2, we saw that if a numerical variable follows a normal distribution, or, in other words, the histogram of this variable is bell-shaped, then roughly 95% of values fall between \\(\\pm\\) 1.96 standard deviations of the mean. Given that our bootstrap distribution based on 1000 resamples with replacement in Figure 6.14 is normally shaped, let’s use this fact about normal distributions to construct a confidence interval in a different way. First, recall the bootstrap distribution has a mean equal to 1995.36. This value almost coincides exactly with the value of the sample mean \\(\\overline{x}\\) of our original 50 pennies of 1995.44. Second, let’s compute the standard deviation of the bootstrap distribution using the values of mean_year in the virtual_resampled_means data frame: virtual_resampled_means %&gt;% summarize(SE = sd(mean_year)) # A tibble: 1 × 1 SE &lt;dbl&gt; 1 2.15466 What is this value? Recall that the bootstrap distribution is an approximation to the sampling distribution. Recall also that the standard deviation of a sampling distribution has a special name: the standard error. Putting these two facts together, we can say that 2.155 is an approximation of the standard error of \\(\\overline{x}\\). Thus, using our 95% rule of thumb about normal distributions from Appendix A.2, we can use the following formula to determine the lower and upper endpoints of a 95% confidence interval for \\(\\mu\\): \\[ \\begin{aligned} \\overline{x} \\pm 1.96 \\cdot SE &amp;= (\\overline{x} - 1.96 \\cdot SE, \\overline{x} + 1.96 \\cdot SE)\\\\ &amp;= (1995.44 - 1.96 \\cdot 2.15, 1995.44 + 1.96 \\cdot 2.15)\\\\ &amp;= (1991.15, 1999.73) \\end{aligned} \\] Let’s now add the SE method confidence interval with dashed lines in Figure 6.17. FIGURE 6.17: Comparing two 95% confidence interval methods. We see that both methods produce nearly identical 95% confidence intervals for \\(\\mu\\) with the percentile method yielding \\((1991.06, 1999.28)\\) while the standard error method produces \\((1991.22, 1999.66)\\). However, recall that we can only use the standard error rule when the bootstrap distribution is roughly normally shaped. Now that we’ve introduced the concept of confidence intervals and laid out the intuition behind two methods for constructing them, let’s explore the code that allows us to construct them. Learning check (LC6.3) What condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method? (LC6.4) Say we wanted to construct a 68% confidence interval instead of a 95% confidence interval for \\(\\mu\\). Describe what changes are needed to make this happen. Hint: we suggest you look at Appendix A.2 on the normal distribution. 6.4 Constructing confidence intervals Recall that the process of resampling with replacement we performed by hand in Section 6.1 and virtually in Section 6.2 is known as bootstrapping. The term bootstrapping originates in the expression of “pulling oneself up by their bootstraps,” meaning to “succeed only by one’s own efforts or abilities.” From a statistical perspective, bootstrapping alludes to succeeding in being able to study the effects of sampling variation on estimates from the “effort” of a single sample. Or more precisely, it refers to constructing an approximation to the sampling distribution using only one sample. To perform this resampling with replacement virtually in Section 6.2, we used the rep_sample_n() function, making sure that the size of the resamples matched the original sample size of 50. In this section, we’ll build off these ideas to construct confidence intervals using a new package: the infer package for “tidy” and transparent statistical inference. 6.4.1 Original workflow Recall that in Section 6.2, we virtually performed bootstrap resampling with replacement to construct bootstrap distributions. Such distributions are approximations to the sampling distributions we saw in Chapter 5, but are constructed using only a single sample. Let’s revisit the original workflow using the %&gt;% pipe operator. First, we used the rep_sample_n() function to resample size = 50 pennies with replacement from the original sample of 50 pennies in pennies_sample by setting replace = TRUE. Furthermore, we repeated this resampling 1000 times by setting reps = 1000: pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) Second, since for each of our 1000 resamples of size 50, we wanted to compute a separate sample mean, we used the dplyr verb group_by() to group observations/rows together by the replicate variable… pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) … followed by using summarize() to compute the sample mean() year for each replicate group: pennies_sample %&gt;% rep_sample_n(size = 50, replace = TRUE, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarize(mean_year = mean(year)) For this simple case, we can get by with using the rep_sample_n() function and a couple of dplyr verbs to construct the bootstrap distribution. However, using only dplyr verbs only provides us with a limited set of tools. For more complicated situations, we’ll need a little more firepower. Let’s repeat this using the infer package. 6.4.2 infer package workflow The infer package is an R package for statistical inference. It makes efficient use of the %&gt;% pipe operator we introduced in Section 3.1 to spell out the sequence of steps necessary to perform statistical inference in a “tidy” and transparent fashion. Furthermore, just as the dplyr package provides functions with verb-like names to perform data wrangling, the infer package provides functions with intuitive verb-like names to perform statistical inference. Let’s go back to our pennies. Previously, we computed the value of the sample mean \\(\\overline{x}\\) using the dplyr function summarize(): pennies_sample %&gt;% summarize(stat = mean(year)) We’ll see that we can also do this using infer functions specify() and calculate(): pennies_sample %&gt;% specify(response = year) %&gt;% calculate(stat = &quot;mean&quot;) You might be asking yourself: “Isn’t the infer code longer? Why would I use that code?”. While not immediately apparent, you’ll see that there are three chief benefits to the infer workflow as opposed to the dplyr workflow. First, the infer verb names better align with the overall resampling framework you need to understand to construct confidence intervals and to conduct hypothesis tests (in Chapter 7). We’ll see flowchart diagrams of this framework in the upcoming Figure 6.23 and in Chapter 7 with Figure 7.14. Second, you can jump back and forth seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This will become apparent in Subsection 7.3.2 when we’ll compare the infer code for both of these inferential methods. Third, the infer workflow is much simpler for conducting inference when you have more than one variable. We’ll see two such situations. We’ll first see situations of two-sample inference where the sample data is collected from two groups, such as in Section 6.6 where we study the contagiousness of yawning and in Section 7.1 where we compare parasite susceptibility of two groups of birds. Then in Section 9.4, we’ll see situations of inference for regression using the regression models you fit in Chapter 8. Let’s now illustrate the sequence of verbs necessary to construct a confidence interval for \\(\\mu\\), the population mean year of minting of all US pennies in 2019. 1. specify variables FIGURE 6.18: Diagram of the specify() verb. As shown in Figure 6.18, the specify() function is used to choose which variables in a data frame will be the focus of our statistical inference. We do this by specifying the response argument. For example, in our pennies_sample data frame of the 50 pennies sampled from the bank, the variable of interest is year: pennies_sample %&gt;% specify(response = year) Response: year (numeric) # A tibble: 50 × 1 year &lt;dbl&gt; 1 2002 2 1986 3 2017 4 1988 5 2008 6 1983 7 2008 8 1996 9 2004 10 2000 # … with 40 more rows Notice how the data itself doesn’t change, but the Response: year (numeric) meta-data does. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Section 3.6. We can also specify which variables will be the focus of our statistical inference using a formula = y ~ x. This is the same formula notation you will see in Chapters 8 and 10 on regression models: the response variable y is separated from the explanatory variable x by a ~ (“tilde”). The following use of specify() with the formula argument yields the same result seen previously: pennies_sample %&gt;% specify(formula = year ~ NULL) Since in the case of pennies we only have a response variable and no explanatory variable of interest, we set the x on the right-hand side of the ~ to be NULL. While in the case of the pennies either specification works just fine, we’ll see examples later on where the formula specification is simpler. In particular, this comes up in the upcoming Section 6.6 on comparing two proportions and Section 9.4 on inference for regression. 2. generate replicates FIGURE 6.19: Diagram of generate() replicates. After we specify() the variables of interest, we pipe the results into the generate() function to generate replicates. Figure 6.19 shows how this is combined with specify() to start the pipeline. In other words, repeat the resampling process a large number of times. Recall in Sections 6.2.2 and 6.2.3 we did this 35 and 1000 times. The generate() function’s first argument is reps, which sets the number of replicates we would like to generate. Since we want to resample the 50 pennies in pennies_sample with replacement 1000 times, we set reps = 1000. The second argument type determines the type of computer simulation we’d like to perform. We set this to type = \"bootstrap\" indicating that we want to perform bootstrap resampling. You’ll see different options for type in Chapter 7. pennies_sample %&gt;% specify(response = year) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: year (numeric) # A tibble: 50,000 × 2 # Groups: replicate [1,000] replicate year &lt;int&gt; &lt;dbl&gt; 1 1 1981 2 1 1988 3 1 2006 4 1 2016 5 1 2002 6 1 1985 7 1 1979 8 1 2000 9 1 2006 10 1 2016 # … with 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 pennies with replacement 1000 times and 50,000 = 50 \\(\\cdot\\) 1000. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. The default value of the type argument is \"bootstrap\" in this scenario, so if the last line was written as generate(reps = 1000), we’d obtain the same results. Comparing with original workflow: Note that the steps of the infer workflow so far produce the same results as the original workflow using the rep_sample_n() function we saw earlier. In other words, the following two code chunks produce similar results: # infer workflow: # Original workflow: pennies_sample %&gt;% pennies_sample %&gt;% specify(response = year) %&gt;% rep_sample_n(size = 50, replace = TRUE, generate(reps = 1000) reps = 1000) 3. calculate summary statistics FIGURE 6.20: Diagram of calculate() summary statistics. After we generate() many replicates of bootstrap resampling with replacement, we next want to summarize each of the 1000 resamples of size 50 to a single sample statistic value. As seen in the diagram, the calculate() function does this. In our case, we want to calculate the mean year for each bootstrap resample of size 50. To do so, we set the stat argument to \"mean\". You can also set the stat argument to a variety of other common summary statistics, like \"median\", \"sum\", \"sd\" (standard deviation), and \"prop\" (proportion). To see a list of all possible summary statistics you can use, type ?calculate and read the help file. Let’s save the result in a data frame called bootstrap_distribution and explore its contents: bootstrap_distribution &lt;- pennies_sample %&gt;% specify(response = year) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = &quot;mean&quot;) bootstrap_distribution # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 1995.7 2 2 1994.04 3 3 1993.62 4 4 1994.5 5 5 1994.08 6 6 1993.6 7 7 1995.26 8 8 1996.64 9 9 1994.3 10 10 1995.94 # … with 990 more rows Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 replicate values. It also has the mean year for each bootstrap resample saved in the variable stat. Comparing with original workflow: You may have recognized at this point that the calculate() step in the infer workflow produces the same output as the group_by() %&gt;% summarize() steps in the original workflow. # infer workflow: # Original workflow: pennies_sample %&gt;% pennies_sample %&gt;% specify(response = year) %&gt;% rep_sample_n(size = 50, replace = TRUE, generate(reps = 1000) %&gt;% reps = 1000) %&gt;% calculate(stat = &quot;mean&quot;) group_by(replicate) %&gt;% summarize(stat = mean(year)) 4. visualize the results FIGURE 6.21: Diagram of visualize() results. The visualize() verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical stat variable’s values. The pipeline of the main infer verbs used for exploring bootstrap distribution results is shown in Figure 6.21. visualize(bootstrap_distribution) FIGURE 6.22: Bootstrap distribution. Comparing with original workflow: In fact, visualize() is a wrapper function for the ggplot() function that uses a geom_histogram() layer. Recall that we illustrated the concept of a wrapper function in Figure 8.5 in Subsection 8.1.2. # infer workflow: # Original workflow: visualize(bootstrap_distribution) ggplot(bootstrap_distribution, aes(x = stat)) + geom_histogram() The visualize() function can take many other arguments which we’ll see momentarily to customize the plot further. It also works with helper functions to do the shading of the histogram values corresponding to the confidence interval values. Let’s recap the steps of the infer workflow for constructing a bootstrap distribution and then visualizing it in Figure 6.23. FIGURE 6.23: infer package workflow for confidence intervals. Recall how we introduced two different methods for constructing 95% confidence intervals for an unknown population parameter in Section 6.3: the percentile method and the standard error method. Let’s now check out the infer package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the infer package! 6.4.3 Percentile method with infer Recall the percentile method for constructing 95% confidence intervals we introduced in Subsection 6.3.1. This method sets the lower endpoint of the confidence interval at the 2.5th percentile of the bootstrap distribution and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95% of the values of the sample mean in the bootstrap distribution. We can compute the 95% confidence interval by piping bootstrap_distribution into the get_confidence_interval() function from the infer package, with the confidence level set to 0.95 and the confidence interval type to be \"percentile\". Let’s save the results in percentile_ci. percentile_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 1991.24 1999.42 Alternatively, we can visualize the interval (1991.24, 1999.42) by piping the bootstrap_distribution data frame into the visualize() function and adding a shade_confidence_interval() layer. We set the endpoints argument to be percentile_ci. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = percentile_ci) FIGURE 6.24: Percentile method 95% confidence interval shaded corresponding to potential values. Observe in Figure 6.24 that 95% of the sample means stored in the stat variable in bootstrap_distribution fall between the two endpoints marked with the darker lines, with 2.5% of the sample means to the left of the shaded area and 2.5% of the sample means to the right. You also have the option to change the colors of the shading using the color and fill arguments. You can also use the shorter named function shade_ci() and the results will be the same. This is for folks who don’t want to type out all of confidence_interval and prefer to type out ci instead. Try out the following code! visualize(bootstrap_distribution) + shade_ci(endpoints = percentile_ci, color = &quot;hotpink&quot;, fill = &quot;khaki&quot;) 6.4.4 Standard error method with infer Recall the standard error method for constructing 95% confidence intervals we introduced in Subsection 6.3.2. For any distribution that is normally shaped, roughly 95% of the values lie within two standard deviations of the mean. In the case of the bootstrap distribution, the standard deviation has a special name: the standard error. So in our case, 95% of values of the bootstrap distribution will lie within \\(\\pm 1.96\\) standard errors of \\(\\overline{x}\\). Thus, a 95% confidence interval is \\[\\overline{x} \\pm 1.96 \\cdot SE = (\\overline{x} - 1.96 \\cdot SE, \\, \\overline{x} + 1.96 \\cdot SE).\\] Computation of the 95% confidence interval can once again be done by piping the bootstrap_distribution data frame we created into the get_confidence_interval() function. However, this time we set the first type argument to be \"se\". Second, we must specify the point_estimate argument in order to set the center of the confidence interval. We set this to be the sample mean of the original sample of 50 pennies of 1995.44 we saved in x_bar earlier. standard_error_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(type = &quot;se&quot;, point_estimate = x_bar) Using `level = 0.95` to compute confidence interval. standard_error_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 1991.35 1999.53 If we would like to visualize the interval (1991.35, 1999.53), we can once again pipe the bootstrap_distribution data frame into the visualize() function and add a shade_confidence_interval() layer to our plot. We set the endpoints argument to be standard_error_ci. The resulting standard-error method based on a 95% confidence interval for \\(\\mu\\) can be seen in Figure 6.25. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = standard_error_ci) FIGURE 6.25: Standard-error-method 95% confidence interval. As noted in Section 6.3, both methods produce similar confidence intervals: Percentile method: (1991.24, 1999.42) Standard error method: (1991.35, 1999.53) Learning check (LC6.5) Construct a 95% confidence interval for the median year of minting of all US pennies. Use the percentile method and, if appropriate, then use the standard-error method. 6.5 Interpreting confidence intervals Now that we’ve shown you how to construct confidence intervals using a sample drawn from a population, let’s now focus on how to interpret their effectiveness. The effectiveness of a confidence interval is judged by whether or not it contains the true value of the population parameter. Going back to our fishing analogy in Section 6.3, this is like asking, “Did our net capture the fish?”. So, for example, does our percentile-based confidence interval of (1991.24, 1999.42) “capture” the true mean year \\(\\mu\\) of all US pennies? Alas, we’ll never know, because we don’t know what the true value of \\(\\mu\\) is. After all, we’re sampling to estimate it! In order to interpret a confidence interval’s effectiveness, we need to know what the value of the population parameter is. That way we can say whether or not a confidence interval “captured” this value. Let’s revisit our sampling bowl from Chapter 5. What proportion of the bowl’s 2400 balls are red? Let’s compute this: bowl %&gt;% summarize(p_red = mean(color == &quot;red&quot;)) # A tibble: 1 × 1 p_red &lt;dbl&gt; 1 0.375 In this case, we know what the value of the population parameter is: we know that the population proportion \\(p\\) is 0.375. In other words, we know that 37.5% of the bowl’s balls are red. As we stated in Subsection 5.3.3, the sampling bowl exercise doesn’t really reflect how sampling is done in real life, but rather was an idealized activity. In real life, we won’t know what the true value of the population parameter is, hence the need for estimation. Let’s now construct confidence intervals for \\(p\\) using our 33 groups of friends’ samples from the bowl in Chapter 5. We’ll then see if the confidence intervals “captured” the true value of \\(p\\), which we know to be 37.5%. That is to say, “Did the net capture the fish?”. 6.5.1 Did the net capture the fish? Recall that we had 33 groups of friends each take samples of size 50 from the bowl and then compute the sample proportion of red balls \\(\\widehat{p}\\). This resulted in 33 such estimates of \\(p\\). Let’s focus on Ilyas and Yohan’s sample, which is saved in the bowl_sample_1 data frame in the moderndive package: bowl_sample_1 # A tibble: 50 × 1 color &lt;chr&gt; 1 white 2 white 3 red 4 red 5 white 6 white 7 red 8 white 9 white 10 white # … with 40 more rows They observed 21 red balls out of 50 and thus their sample proportion \\(\\widehat{p}\\) was 21/50 = 0.42 = 42%. Think of this as the “spear” from our fishing analogy. Let’s now follow the infer package workflow from Subsection 6.4.2 to create a percentile-method-based 95% confidence interval for \\(p\\) using Ilyas and Yohan’s sample. Think of this as the “net.” 1. specify variables First, we specify() the response variable of interest color: bowl_sample_1 %&gt;% specify(response = color) Error: A level of the response variable `color` needs to be specified for the `success` argument in `specify()`. Whoops! We need to define which event is of interest! red or white balls? Since we are interested in the proportion red, let’s set success to be \"red\": bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) Response: color (factor) # A tibble: 50 × 1 color &lt;fct&gt; 1 white 2 white 3 red 4 red 5 white 6 white 7 red 8 white 9 white 10 white # … with 40 more rows 2. generate replicates Second, we generate() 1000 replicates of bootstrap resampling with replacement from bowl_sample_1 by setting reps = 1000 and type = \"bootstrap\". bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: color (factor) # A tibble: 50,000 × 2 # Groups: replicate [1,000] replicate color &lt;int&gt; &lt;fct&gt; 1 1 white 2 1 white 3 1 white 4 1 white 5 1 red 6 1 white 7 1 white 8 1 white 9 1 white 10 1 red # … with 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 balls with replacement 1000 times and thus 50,000 = 50 \\(\\cdot\\) 1000. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. 3. calculate summary statistics Third, we summarize each of the 1000 resamples of size 50 with the proportion of successes. In other words, the proportion of the balls that are \"red\". We can set the summary statistic to be calculated as the proportion by setting the stat argument to be \"prop\". Let’s save the result as sample_1_bootstrap: sample_1_bootstrap &lt;- bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) sample_1_bootstrap # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.32 2 2 0.42 3 3 0.44 4 4 0.4 5 5 0.44 6 6 0.52 7 7 0.38 8 8 0.44 9 9 0.34 10 10 0.42 # … with 990 more rows Observe there are 1000 rows in this data frame and thus 1000 values of the variable stat. These 1000 values of stat represent our 1000 replicated values of the proportion, each based on a different resample. 4. visualize the results Fourth and lastly, let’s compute the resulting 95% confidence interval. percentile_ci_1 &lt;- sample_1_bootstrap %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci_1 # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.3 0.56 Let’s visualize the bootstrap distribution along with the percentile_ci_1 percentile-based 95% confidence interval for \\(p\\) in Figure 6.26. We’ll adjust the number of bins to better see the resulting shape. Furthermore, we’ll add a dashed vertical line at Ilyas and Yohan’s observed \\(\\widehat{p}\\) = 21/50 = 0.42 = 42% using geom_vline(). sample_1_bootstrap %&gt;% visualize(bins = 15) + shade_confidence_interval(endpoints = percentile_ci_1) + geom_vline(xintercept = 0.42, linetype = &quot;dashed&quot;) FIGURE 6.26: Bootstrap distribution. Did Ilyas and Yohan’s net capture the fish? Did their 95% confidence interval for \\(p\\) based on their sample contain the true value of \\(p\\) of 0.375? Yes! 0.375 is between the endpoints of their confidence interval (0.3, 0.56). However, will every 95% confidence interval for \\(p\\) capture this value? In other words, if we had a different sample of 50 balls and constructed a different confidence interval, would it necessarily contain \\(p\\) = 0.375 as well? Let’s see! Let’s first take a different sample from the bowl, this time using the computer as we did in Chapter 5: bowl_sample_2 &lt;- bowl %&gt;% rep_sample_n(size = 50) bowl_sample_2 # A tibble: 50 × 3 # Groups: replicate [1] replicate ball_ID color &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 1 1665 red 2 1 1312 red 3 1 2105 red 4 1 810 white 5 1 189 white 6 1 1429 white 7 1 2294 red 8 1 1233 white 9 1 1951 white 10 1 2061 white # … with 40 more rows Let’s reapply the same infer functions on bowl_sample_2 to generate a different 95% confidence interval for \\(p\\). First, we create the new bootstrap distribution and save the results in sample_2_bootstrap: sample_2_bootstrap &lt;- bowl_sample_2 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) sample_2_bootstrap # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.48 2 2 0.38 3 3 0.32 4 4 0.32 5 5 0.34 6 6 0.26 7 7 0.3 8 8 0.36 9 9 0.44 10 10 0.36 # … with 990 more rows We once again compute a percentile-based 95% confidence interval for \\(p\\): percentile_ci_2 &lt;- sample_2_bootstrap %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci_2 # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.2 0.48 Does this new net capture the fish? In other words, does the 95% confidence interval for \\(p\\) based on the new sample contain the true value of \\(p\\) of 0.375? Yes again! 0.375 is between the endpoints of our confidence interval (0.2, 0.48). Let’s now repeat this process 100 more times: we take 100 virtual samples from the bowl and construct 100 95% confidence intervals. Let’s visualize the results in Figure 6.27 where: We mark the true value of \\(p = 0.375\\) with a vertical line. We mark each of the 100 95% confidence intervals with horizontal lines. These are the “nets.” The horizontal line is colored grey if the confidence interval “captures” the true value of \\(p\\) marked with the vertical line. The horizontal line is colored black otherwise. FIGURE 6.27: 100 percentile-based 95% confidence intervals for \\(p\\). Of the 100 95% confidence intervals, 95 of them captured the true value \\(p = 0.375\\), whereas 5 of them didn’t. In other words, 95 of our nets caught the fish, whereas 5 of our nets didn’t. This is where the “95% confidence level” we defined in Section 6.3 comes into play: for every 100 95% confidence intervals, we expect that 95 of them will capture \\(p\\) and that five of them won’t. Note that “expect” is a probabilistic statement referring to a long-run average. In other words, for every 100 confidence intervals, we will observe about 95 confidence intervals that capture \\(p\\), but not necessarily exactly 95. In Figure 6.27 for example, 95 of the confidence intervals capture \\(p\\). To further accentuate our point about confidence levels, let’s generate a figure similar to Figure 6.27, but this time constructing 80% standard-error method based confidence intervals instead. Let’s visualize the results in Figure 6.28 with the scale on the x-axis being the same as in Figure 6.27 to make comparison easy. Furthermore, since all standard-error method confidence intervals for \\(p\\) are centered at their respective point estimates \\(\\widehat{p}\\), we mark this value on each line with dots. FIGURE 6.28: 100 SE-based 80% confidence intervals for \\(p\\) with point estimate center marked with dots. Observe how the 80% confidence intervals are narrower than the 95% confidence intervals, reflecting our lower degree of confidence. Think of this as using a smaller “net.” We’ll explore other determinants of confidence interval width in the upcoming Subsection 6.5.3. Furthermore, observe that of the 100 80% confidence intervals, 82 of them captured the population proportion \\(p\\) = 0.375, whereas 18 of them did not. Since we lowered the confidence level from 95% to 80%, we now have a much larger number of confidence intervals that failed to “catch the fish.” 6.5.2 Precise and shorthand interpretation Let’s return our attention to 95% confidence intervals. The precise and mathematically correct interpretation of a 95% confidence interval is a little long-winded: Precise interpretation: If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter. This is what we observed in Figure 6.27. Our confidence interval construction procedure is 95% reliable. That is to say, we can expect our confidence intervals to include the true population parameter about 95% of the time. A common but incorrect interpretation is: “There is a 95% probability that the confidence interval contains \\(p\\).” Looking at Figure 6.27, each of the confidence intervals either does or doesn’t contain \\(p\\). In other words, the probability is either a 1 or a 0. So if the 95% confidence level only relates to the reliability of the confidence interval construction procedure and not to a given confidence interval itself, what insight can be derived from a given confidence interval? For example, going back to the pennies example, we found that the percentile method 95% confidence interval for \\(\\mu\\) was (1991.24, 1999.42), whereas the standard error method 95% confidence interval was (1991.35, 1999.53). What can be said about these two intervals? Loosely speaking, we can think of these intervals as our “best guess” of a plausible range of values for the mean year \\(\\mu\\) of all US pennies. For the rest of this book, we’ll use the following shorthand summary of the precise interpretation. Short-hand interpretation: We are 95% “confident” that a 95% confidence interval captures the value of the population parameter. We use quotation marks around “confident” to emphasize that while 95% relates to the reliability of our confidence interval construction procedure, ultimately a constructed confidence interval is our best guess of an interval that contains the population parameter. In other words, it’s our best net. So returning to our pennies example and focusing on the percentile method, we are 95% “confident” that the true mean year of pennies in circulation in 2019 is somewhere between 1991.24 and 1999.42. 6.5.3 Width of confidence intervals Now that we know how to interpret confidence intervals, let’s go over some factors that determine their width. Impact of confidence level One factor that determines confidence interval widths is the pre-specified confidence level. For example, in Figures 6.27 and 6.28, we compared the widths of 95% and 80% confidence intervals and observed that the 95% confidence intervals were wider. The quantification of the confidence level should match what many expect of the word “confident.” In order to be more confident in our best guess of a range of values, we need to widen the range of values. To elaborate on this, imagine we want to guess the forecasted high temperature in Seoul, South Korea on August 15th. Given Seoul’s temperate climate with four distinct seasons, we could say somewhat confidently that the high temperature would be between 50°F - 95°F (10°C - 35°C). However, if we wanted a temperature range we were absolutely confident about, we would need to widen it. We need this wider range to allow for the possibility of anomalous weather, like a freak cold spell or an extreme heat wave. So a range of temperatures we could be near certain about would be between 32°F - 110°F (0°C - 43°C). On the other hand, if we could tolerate being a little less confident, we could narrow this range to between 70°F - 85°F (21°C - 30°C). Let’s revisit our sampling bowl from Chapter 5. Let’s compare \\(10 \\cdot 3 = 30\\) confidence intervals for \\(p\\) based on three different confidence levels: 80%, 95%, and 99%. Specifically, we’ll first take 30 different random samples of size \\(n\\) = 50 balls from the bowl. Then we’ll construct 10 percentile-based confidence intervals using each of the three different confidence levels. Finally, we’ll compare the widths of these intervals. We visualize the resulting confidence intervals in Figure 6.29 along with a vertical line marking the true value of \\(p\\) = 0.375. FIGURE 6.29: Ten 80, 95, and 99% confidence intervals for \\(p\\) based on \\(n = 50\\). Observe that as the confidence level increases from 80% to 95% to 99%, the confidence intervals tend to get wider as seen in Table 6.2 where we compare their average widths. TABLE 6.2: Average width of 80, 95, and 99% confidence intervals Confidence level Mean width 80% 0.162 95% 0.262 99% 0.338 So in order to have a higher confidence level, our confidence intervals must be wider. Ideally, we would have both a high confidence level and narrow confidence intervals. However, we cannot have it both ways. If we want to be more confident, we need to allow for wider intervals. Conversely, if we would like a narrow interval, we must tolerate a lower confidence level. The moral of the story is: Higher confidence levels tend to produce wider confidence intervals. When looking at Figure 6.29 it is important to keep in mind that we kept the sample size fixed at \\(n\\) = 50. Thus, all \\(10 \\cdot 3 = 30\\) random samples from the bowl had the same sample size. What happens if instead we took samples of different sizes? Recall that we did this in Subsection 5.2.5 using virtual shovels with 25, 50, and 100 slots. Impact of sample size This time, let’s fix the confidence level at 95%, but consider three different sample sizes for \\(n\\): 25, 50, and 100. Specifically, we’ll first take 10 different random samples of size 25, 10 different random samples of size 50, and 10 different random samples of size 100. We’ll then construct 95% percentile-based confidence intervals for each sample. Finally, we’ll compare the widths of these intervals. We visualize the resulting 30 confidence intervals in Figure 6.30. Note also the vertical line marking the true value of \\(p\\) = 0.375. FIGURE 6.30: Ten 95% confidence intervals for \\(p\\) with \\(n = 25, 50,\\) and \\(100\\). Observe that as the confidence intervals are constructed from larger and larger sample sizes, they tend to get narrower. Let’s compare the average widths in Table 6.3. TABLE 6.3: Average width of 95% confidence intervals based on \\(n = 25\\), \\(50\\), and \\(100\\) Sample size Mean width n = 25 0.380 n = 50 0.268 n = 100 0.189 The moral of the story is: Larger sample sizes tend to produce narrower confidence intervals. Recall that this was a key message in Subsection 5.3.3. As we used larger and larger shovels for our samples, the sample proportions red \\(\\widehat{p}\\) tended to vary less. In other words, our estimates got more and more precise. Recall that we visualized these results in Figure 5.16, where we compared the sampling distributions for \\(\\widehat{p}\\) based on samples of size \\(n\\) equal 25, 50, and 100. We also quantified the sampling variation of these sampling distributions using their standard deviation, which has that special name: the standard error. So as the sample size increases, the standard error decreases. In fact, the standard error is another related factor in determining confidence interval width. We’ll explore this fact in Subsection 6.8 when we discuss theory-based methods for constructing confidence intervals using mathematical formulas. Such methods are an alternative to the computer-based methods we’ve been using so far. 6.6 Case study: Is yawning contagious? Let’s apply our knowledge of confidence intervals to answer the question: “Is yawning contagious?”. If you see someone else yawn, are you more likely to yawn? In an episode of the US show Mythbusters, the hosts conducted an experiment to answer this question. The episode is available to view in the United States on the Discovery Network website here and more information about the episode is also available on IMDb. 6.6.1 Mythbusters study data Fifty adult participants who thought they were being considered for an appearance on the show were interviewed by a show recruiter. In the interview, the recruiter either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the Mythbusters team watched the participants using a hidden camera to see if they yawned. The data frame containing the results of their experiment is available in the mythbusters_yawn data frame included in the moderndive package: mythbusters_yawn # A tibble: 50 × 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no 7 7 seed yes 8 8 control no 9 9 control no 10 10 seed no # … with 40 more rows The variables are: subj: The participant ID with values 1 through 50. group: A binary treatment variable indicating whether the participant was exposed to yawning. \"seed\" indicates the participant was exposed to yawning while \"control\" indicates the participant was not. yawn: A binary response variable indicating whether the participant ultimately yawned. Recall that you learned about treatment and response variables in Subsection 8.3.1 in our discussion on confounding variables. Let’s use some data wrangling to obtain counts of the four possible outcomes: mythbusters_yawn %&gt;% group_by(group, yawn) %&gt;% summarize(count = n()) # A tibble: 4 × 3 # Groups: group [2] group yawn count &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 control no 12 2 control yes 4 3 seed no 24 4 seed yes 10 Let’s first focus on the \"control\" group participants who were not exposed to yawning. 12 such participants did not yawn, while 4 such participants did. So out of the 16 people who were not exposed to yawning, 4/16 = 0.25 = 25% did yawn. Let’s now focus on the \"seed\" group participants who were exposed to yawning where 24 such participants did not yawn, while 10 such participants did yawn. So out of the 34 people who were exposed to yawning, 10/34 = 0.294 = 29.4% did yawn. Comparing these two percentages, the participants who were exposed to yawning yawned 29.4% - 25% = 4.4% more often than those who were not. 6.6.2 Sampling scenario Let’s review the terminology and notation related to sampling we studied in Subsection 5.3.1. In Chapter 5 our study population was the bowl of \\(N\\) = 2400 balls. Our population parameter of interest was the population proportion of these balls that were red, denoted mathematically by \\(p\\). In order to estimate \\(p\\), we extracted a sample of 50 balls using the shovel and computed the relevant point estimate: the sample proportion that were red, denoted mathematically by \\(\\widehat{p}\\). Who is the study population here? All humans? All the people who watch the show Mythbusters? It’s hard to say! This question can only be answered if we know how the show’s hosts recruited participants! In other words, what was the sampling methodology used by the Mythbusters to recruit participants? We alas are not provided with this information. Only for the purposes of this case study, however, we’ll assume that the 50 participants are a representative sample of all Americans given the popularity of this show. Thus, we’ll be assuming that any results of this experiment will generalize to all \\(N\\) = 327 million Americans (2018 population). Just like with our sampling bowl, the population parameter here will involve proportions. However, in this case it will be the difference in population proportions \\(p_{seed} - p_{control}\\), where \\(p_{seed}\\) is the proportion of all Americans who if exposed to yawning will yawn themselves, and \\(p_{control}\\) is the proportion of all Americans who if not exposed to yawning still yawn themselves. Correspondingly, the point estimate/sample statistic based the Mythbusters’ sample of participants will be the difference in sample proportions \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\). Let’s extend Table 5.5 of scenarios of sampling for inference to include our latest scenario. TABLE 6.4: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) This is known as a two-sample inference situation since we have two separate samples. Based on their two-samples of size \\(n_{seed}\\) = 34 and \\(n_{control}\\) = 16, the point estimate is \\[ \\widehat{p}_{seed} - \\widehat{p}_{control} = \\frac{24}{34} - \\frac{12}{16} = 0.04411765 \\approx 4.4\\% \\] However, say the Mythbusters repeated this experiment. In other words, say they recruited 50 new participants and exposed 34 of them to yawning and 16 not. Would they obtain the exact same estimated difference of 4.4%? Probably not, again, because of sampling variation. How does this sampling variation affect their estimate of 4.4%? In other words, what would be a plausible range of values for this difference that accounts for this sampling variation? We can answer this question with confidence intervals! Furthermore, since the Mythbusters only have a single two-sample of 50 participants, they would have to construct a 95% confidence interval for \\(p_{seed} - p_{control}\\) using bootstrap resampling with replacement. We make a couple of important notes. First, for the comparison between the \"seed\" and \"control\" groups to make sense, however, both groups need to be independent from each other. Otherwise, they could influence each other’s results. This means that a participant being selected for the \"seed\" or \"control\" group has no influence on another participant being assigned to one of the two groups. As an example, if there were a mother and her child as participants in the study, they wouldn’t necessarily be in the same group. They would each be assigned randomly to one of the two groups of the explanatory variable. Second, the order of the subtraction in the difference doesn’t matter so long as you are consistent and tailor your interpretations accordingly. In other words, using a point estimate of \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) or \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\) does not make a material difference, you just need to stay consistent and interpret your results accordingly. 6.6.3 Constructing the confidence interval As we did in Subsection 6.4.2, let’s first construct the bootstrap distribution for \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) and then use this to construct 95% confidence intervals for \\(p_{seed} - p_{control}\\). We’ll do this using the infer workflow again. However, since the difference in proportions is a new scenario for inference, we’ll need to use some new arguments in the infer functions along the way. 1. specify variables Let’s take our mythbusters_yawn data frame and specify() which variables are of interest using the y ~ x formula interface where: Our response variable is yawn: whether or not a participant yawned. It has levels \"yes\" and \"no\". The explanatory variable is group: whether or not a participant was exposed to yawning. It has levels \"seed\" (exposed to yawning) and \"control\" (not exposed to yawning). mythbusters_yawn %&gt;% specify(formula = yawn ~ group) Error: A level of the response variable `yawn` needs to be specified for the `success` argument in `specify()`. Alas, we got an error message similar to the one from Subsection 6.5.1: infer is telling us that one of the levels of the categorical variable yawn needs to be defined as the success. Recall that we define success to be the event of interest we are trying to count and compute proportions of. Are we interested in those participants who \"yes\" yawned or those who \"no\" didn’t yawn? This isn’t clear to R or someone just picking up the code and results for the first time, so we need to set the success argument to \"yes\" as follows to improve the transparency of the code: mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) Response: yawn (factor) Explanatory: group (factor) # A tibble: 50 × 2 yawn group &lt;fct&gt; &lt;fct&gt; 1 yes seed 2 yes control 3 no seed 4 yes seed 5 no seed 6 no control 7 yes seed 8 no control 9 no control 10 no seed # … with 40 more rows 2. generate replicates Our next step is to perform bootstrap resampling with replacement like we did with the slips of paper in our pennies activity in Section 6.1. We saw how it works with both a single variable in computing bootstrap means in Section 6.4 and in computing bootstrap proportions in Section 6.5, but we haven’t yet worked with bootstrapping involving multiple variables. In the infer package, bootstrapping with multiple variables means that each row is potentially resampled. Let’s investigate this by focusing only on the first six rows of mythbusters_yawn: first_six_rows &lt;- head(mythbusters_yawn) first_six_rows # A tibble: 6 × 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 2 control yes 3 3 seed no 4 4 seed yes 5 5 seed no 6 6 control no When we bootstrap this data, we are potentially pulling the subject’s readings multiple times. Thus, we could see the entries of \"seed\" for group and \"no\" for yawn together in a new row in a bootstrap sample. This is further seen by exploring the sample_n() function in dplyr on this smaller 6-row data frame comprised of head(mythbusters_yawn). The sample_n() function can perform this bootstrapping procedure and is similar to the rep_sample_n() function in infer, except that it is not repeated, but rather only performs one sample with or without replacement. first_six_rows %&gt;% sample_n(size = 6, replace = TRUE) # A tibble: 6 × 3 subj group yawn &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 1 seed yes 2 6 control no 3 1 seed yes 4 5 seed no 5 4 seed yes 6 4 seed yes We can see that in this bootstrap sample generated from the first six rows of mythbusters_yawn, we have some rows repeated. The same is true when we perform the generate() step in infer as done in what follows. Using this fact, we generate 1000 replicates, or, in other words, we bootstrap resample the 50 participants with replacement 1000 times. mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) Response: yawn (factor) Explanatory: group (factor) # A tibble: 50,000 × 3 # Groups: replicate [1,000] replicate yawn group &lt;int&gt; &lt;fct&gt; &lt;fct&gt; 1 1 yes seed 2 1 yes control 3 1 no control 4 1 no control 5 1 yes seed 6 1 yes seed 7 1 yes seed 8 1 yes seed 9 1 no seed 10 1 yes seed # … with 49,990 more rows Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 participants with replacement 1000 times and 50,000 = 1000 \\(\\cdot\\) 50. The variable replicate indicates which resample each row belongs to. So it has the value 1 50 times, the value 2 50 times, all the way through to the value 1000 50 times. 3. calculate summary statistics After we generate() many replicates of bootstrap resampling with replacement, we next want to summarize the bootstrap resamples of size 50 with a single summary statistic, the difference in proportions. We do this by setting the stat argument to \"diff in props\": mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;) Error: Statistic is based on a difference; specify the `order` in which to subtract the levels of the explanatory variable. We see another error here. We need to specify the order of the subtraction. Is it \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) or \\(\\widehat{p}_{control} - \\widehat{p}_{seed}\\). We specify it to be \\(\\widehat{p}_{seed} - \\widehat{p}_{control}\\) by setting order = c(\"seed\", \"control\"). Note that you could’ve also set order = c(\"control\", \"seed\"). As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly. Let’s save the output in a data frame bootstrap_distribution_yawning: bootstrap_distribution_yawning &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) bootstrap_distribution_yawning # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.0357143 2 2 0.229167 3 3 0.00952381 4 4 0.0106952 5 5 0.00483092 6 6 0.00793651 7 7 -0.0845588 8 8 -0.00466200 9 9 0.164686 10 10 0.124777 # … with 990 more rows Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 replicate ID’s and the 1000 differences in proportions for each bootstrap resample in stat. 4. visualize the results In Figure 6.31 we visualize() the resulting bootstrap resampling distribution. Let’s also add a vertical line at 0 by adding a geom_vline() layer. visualize(bootstrap_distribution_yawning) + geom_vline(xintercept = 0) FIGURE 6.31: Bootstrap distribution. First, let’s compute the 95% confidence interval for \\(p_{seed} - p_{control}\\) using the percentile method, in other words, by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped. bootstrap_distribution_yawning %&gt;% get_confidence_interval(type = &quot;percentile&quot;, level = 0.95) # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 -0.238276 0.302464 Second, since the bootstrap distribution is roughly bell-shaped, we can construct a confidence interval using the standard error method as well. Recall that to construct a confidence interval using the standard error method, we need to specify the center of the interval using the point_estimate argument. In our case, we need to set it to be the difference in sample proportions of 4.4% that the Mythbusters observed. We can also use the infer workflow to compute this value by excluding the generate() 1000 bootstrap replicates step. In other words, do not generate replicates, but rather use only the original sample data. We can achieve this by commenting out the generate() line, telling R to ignore it: obs_diff_in_props &lt;- mythbusters_yawn %&gt;% specify(formula = yawn ~ group, success = &quot;yes&quot;) %&gt;% # generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;seed&quot;, &quot;control&quot;)) obs_diff_in_props Response: yawn (factor) Explanatory: group (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 0.0441176 We thus plug this value in as the point_estimate argument. myth_ci_se &lt;- bootstrap_distribution_yawning %&gt;% get_confidence_interval(type = &quot;se&quot;, point_estimate = obs_diff_in_props) Using `level = 0.95` to compute confidence interval. myth_ci_se # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 -0.227291 0.315526 Let’s visualize both confidence intervals in Figure 6.32, with the percentile-method interval marked with black lines and the standard-error-method marked with grey lines. Observe that they are both similar to each other. FIGURE 6.32: Two 95% confidence intervals: percentile method (black) and standard error method (grey). 6.6.4 Interpreting the confidence interval Given that both confidence intervals are quite similar, let’s focus our interpretation to only the percentile-method confidence interval of (-0.238, 0.302). Recall from Subsection 6.5.2 that the precise statistical interpretation of a 95% confidence interval is: if this construction procedure is repeated 100 times, then we expect about 95 of the confidence intervals to capture the true value of \\(p_{seed} - p_{control}\\). In other words, if we gathered 100 samples of \\(n\\) = 50 participants from a similar pool of people and constructed 100 confidence intervals each based on each of the 100 samples, about 95 of them will contain the true value of \\(p_{seed} - p_{control}\\) while about five won’t. Given that this is a little long winded, we use the shorthand interpretation: we’re 95% “confident” that the true difference in proportions \\(p_{seed} - p_{control}\\) is between (-0.238, 0.302). There is one value of particular interest that this 95% confidence interval contains: zero. If \\(p_{seed} - p_{control}\\) were equal to 0, then there would be no difference in proportion yawning between the two groups. This would suggest that there is no associated effect of being exposed to a yawning recruiter on whether you yawn yourself. In our case, since the 95% confidence interval includes 0, we cannot conclusively say if either proportion is larger. Of our 1000 bootstrap resamples with replacement, sometimes \\(\\widehat{p}_{seed}\\) was higher and thus those exposed to yawning yawned themselves more often. At other times, the reverse happened. Say, on the other hand, the 95% confidence interval was entirely above zero. This would suggest that \\(p_{seed} - p_{control} &gt; 0\\), or, in other words \\(p_{seed} &gt; p_{control}\\), and thus we’d have evidence suggesting those exposed to yawning do yawn more often. 6.7 Comparing bootstrap and sampling distributions Let’s talk more about the relationship between sampling distributions and bootstrap distributions. Recall back in Subsection 5.2.3, we took 1000 virtual samples from the bowl using a virtual shovel, computed 1000 values of the sample proportion red \\(\\widehat{p}\\), then visualized their distribution in a histogram. Recall that this distribution is called the sampling distribution of \\(\\widehat{p}\\). Furthermore, the standard deviation of the sampling distribution has a special name: the standard error. We also mentioned that this sampling activity does not reflect how sampling is done in real life. Rather, it was an idealized version of sampling so that we could study the effects of sampling variation on estimates, like the proportion of the shovel’s balls that are red. In real life, however, one would take a single sample that’s as large as possible, much like in the genetic cross we saw in Section 5.4. But how can we get a sense of the effect of sampling variation on estimates if we only have one sample and thus only one estimate? Don’t we need many samples and hence many estimates? The workaround to having a single sample was to perform bootstrap resampling with replacement from the single sample. We did this in the resampling activity in Section 6.1 where we focused on the mean year of minting of pennies. We used pieces of paper representing the original sample of 50 pennies from the bank and resampled them with replacement from a hat. We had 35 of our friends perform this activity and visualized the resulting 35 sample means \\(\\overline{x}\\) in a histogram in Figure 6.11. This distribution was called the bootstrap distribution of \\(\\overline{x}\\). We stated at the time that the bootstrap distribution is an approximation to the sampling distribution of \\(\\overline{x}\\) in the sense that both distributions will have a similar shape and similar spread. Thus the standard error of the bootstrap distribution can be used as an approximation to the standard error of the sampling distribution. Let’s show you that this is the case by now comparing these two types of distributions. Specifically, we’ll compare the sampling distribution of \\(\\widehat{p}\\) based on 1000 virtual samples from the bowl from Subsection 5.2.3 to the bootstrap distribution of \\(\\widehat{p}\\) based on 1000 virtual resamples with replacement from Ilyas and Yohan’s single sample bowl_sample_1 from Subsection 6.5.1. Sampling distribution Here is the code you saw in Subsection 5.2.3 to construct the sampling distribution of \\(\\widehat{p}\\) shown again in Figure 6.33, with some changes to incorporate the statistical terminology relating to sampling from Subsection 5.3.1. # Take 1000 virtual samples of size 50 from the bowl: virtual_samples &lt;- bowl %&gt;% rep_sample_n(size = 50, reps = 1000) # Compute the sampling distribution of 1000 values of p-hat sampling_distribution &lt;- virtual_samples %&gt;% group_by(replicate) %&gt;% summarize(red = sum(color == &quot;red&quot;)) %&gt;% mutate(prop_red = red / 50) # Visualize sampling distribution of p-hat ggplot(sampling_distribution, aes(x = prop_red)) + geom_histogram(binwidth = 0.05, boundary = 0.4, color = &quot;white&quot;) + labs(x = &quot;Proportion of 50 balls that were red&quot;, title = &quot;Sampling distribution&quot;) FIGURE 6.33: Previously seen sampling distribution of sample proportion red for \\(n = 1000\\). An important thing to keep in mind is the default value for replace is FALSE when using rep_sample_n(). This is because when sampling 50 balls with a shovel, we are extracting 50 balls one-by-one without replacing them. This is in contrast to bootstrap resampling with replacement, where we resample a ball and put it back, and repeat this process 50 times. Let’s quantify the variability in this sampling distribution by calculating the standard deviation of the prop_red variable representing 1000 values of the sample proportion \\(\\widehat{p}\\). Remember that the standard deviation of the sampling distribution is the standard error, frequently denoted as se. sampling_distribution %&gt;% summarize(se = sd(prop_red)) # A tibble: 1 × 1 se &lt;dbl&gt; 1 0.0673987 Bootstrap distribution Here is the code you previously saw in Subsection 6.5.1 to construct the bootstrap distribution of \\(\\widehat{p}\\) based on Ilyas and Yohan’s original sample of 50 balls saved in bowl_sample_1. bootstrap_distribution &lt;- bowl_sample_1 %&gt;% specify(response = color, success = &quot;red&quot;) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;prop&quot;) FIGURE 6.34: Bootstrap distribution of proportion red for \\(n = 1000\\). bootstrap_distribution %&gt;% summarize(se = sd(stat)) # A tibble: 1 × 1 se &lt;dbl&gt; 1 0.0712212 Comparison Now that we have computed both the sampling distribution and the bootstrap distributions, let’s compare them side-by-side in Figure 6.35. We’ll make both histograms have matching scales on the x- and y-axes to make them more comparable. Furthermore, we’ll add: To the sampling distribution on the top: a solid line denoting the proportion of the bowl’s balls that are red \\(p\\) = 0.375. To the bootstrap distribution on the bottom: a dashed line at the sample proportion \\(\\widehat{p}\\) = 21/50 = 0.42 = 42% that Ilyas and Yohan observed. FIGURE 6.35: Comparing the sampling and bootstrap distributions of \\(\\widehat{p}\\). There is a lot going on in Figure 6.35, so let’s break down all the comparisons slowly. First, observe how the sampling distribution on top is centered at \\(p\\) = 0.375. This is because the sampling is done at random and in an unbiased fashion. So the estimates \\(\\widehat{p}\\) are centered at the true value of \\(p\\). However, this is not the case with the following bootstrap distribution. The bootstrap distribution is centered at 0.42, which is the proportion red of Ilyas and Yohan’s 50 sampled balls. This is because we are resampling from the same sample over and over again. Since the bootstrap distribution is centered at the original sample’s proportion, it doesn’t necessarily provide a better estimate of \\(p\\) = 0.375. This leads us to our first lesson about bootstrapping: The bootstrap distribution will likely not have the same center as the sampling distribution. In other words, bootstrapping cannot improve the quality of an estimate. Second, let’s now compare the spread of the two distributions: they are somewhat similar. In the previous code, we computed the standard deviations of both distributions as well. Recall that such standard deviations have a special name: standard errors. Let’s compare them in Table 6.5. TABLE 6.5: Comparing standard errors Distribution type Standard error Sampling distribution 0.067 Bootstrap distribution 0.071 Notice that the bootstrap distribution’s standard error is a rather good approximation to the sampling distribution’s standard error. This leads us to our second lesson about bootstrapping: Even if the bootstrap distribution might not have the same center as the sampling distribution, it will likely have very similar shape and spread. In other words, bootstrapping will give you a good estimate of the standard error. Thus, using the fact that the bootstrap distribution and sampling distributions have similar spreads, we can build confidence intervals using bootstrapping as we’ve done all throughout this chapter! 6.8 Theory-based confidence intervals So far in this chapter, we’ve constructed confidence intervals using two methods: the percentile method and the standard error method. Recall also from Subsection 6.3.2 that we can only use the standard-error method if the bootstrap distribution is bell-shaped (i.e., normally distributed). In a similar vein, if the sampling distribution is normally shaped, there is another method for constructing confidence intervals that does not involve using your computer. You can use a theory-based method involving mathematical formulas! The formula uses the rule of thumb we saw in Appendix A.2 that 95% of values in a normal distribution are within \\(\\pm 1.96\\) standard deviations of the mean. In the case of sampling and bootstrap distributions, recall that the standard deviation has a special name: the standard error. Theory-based method for computing standard errors There exists in many cases a formula that approximates the standard error! In the case of our bowl where we used the sample proportion red \\(\\widehat{p}\\) to estimate the proportion of the bowl’s balls that are red, the formula that approximates the standard error is: \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] For example, recall from bowl_sample_1 that Yohan and Ilyas sampled \\(n = 50\\) balls and observed a sample proportion \\(\\widehat{p}\\) of 21/50 = 0.42. So, using the formula, an approximation of the standard error of \\(\\widehat{p}\\) is \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{50}} = \\sqrt{0.004872} = 0.0698 \\approx 0.070\\] The key observation to make here is that there is an \\(n\\) in the denominator. So as the sample size \\(n\\) increases, the standard error decreases. We’ve demonstrated this fact using our virtual shovels in Subsection 5.3.3. If you don’t recall this demonstration, we highly recommend you go back and read that subsection. Let’s compare this theory-based standard error to the standard error of the sampling and bootstrap distributions you computed previously in Subsection 6.7 in Table 6.6. Notice how they are all similar! TABLE 6.6: Comparing standard errors Distribution type Standard error Sampling distribution 0.067 Bootstrap distribution 0.071 Formula approximation 0.070 Going back to Yohan and Ilyas’ sample proportion of \\(\\widehat{p}\\) of 21/50 = 0.42, say this were based on a sample of size \\(n\\) = 100 instead of 50. Then the standard error would be: \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{0.42(1-0.42)}{100}} = \\sqrt{0.002436} = 0.0494\\] Observe that the standard error has gone down from 0.0698 to 0.0494. In other words, the “typical” error of our estimates using \\(n\\) = 100 will go down and hence be more precise. Recall that we illustrated the difference between accuracy and precision of estimates in Figure 5.17. Why is this formula true? Unfortunately, we don’t have the tools at this point to prove this; you’ll need to take a more advanced course in probability and statistics. (It is related to the concepts of Bernoulli and Binomial Distributions. You can read more about its derivation here if you like.) Theory-based method for constructing confidence intervals Using these theory-based standard errors, let’s present a theory-based method for constructing 95% confidence intervals that does not involve using a computer, but rather mathematical formulas. Note that this theory-based method only holds if the sampling distribution is normally shaped, so that we can use the 95% rule of thumb about normal distributions discussed in Appendix A.2. Collect a single representative sample of size \\(n\\) that’s as large as possible. Compute the point estimate: the sample proportion \\(\\widehat{p}\\). Think of this as the center of your “net.” Compute the approximation to the standard error \\[\\text{SE}_{\\widehat{p}} \\approx \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Compute a quantity known as the margin of error (more on this later after we list the five steps): \\[\\text{MoE}_{\\widehat{p}} = 1.96 \\cdot \\text{SE}_{\\widehat{p}} = 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Compute both endpoints of the confidence interval. The lower end-point. Think of this as the left end-point of the net: \\[\\widehat{p} - \\text{MoE}_{\\widehat{p}} = \\widehat{p} - 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} - 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] The upper endpoint. Think of this as the right end-point of the net: \\[\\widehat{p} + \\text{MoE}_{\\widehat{p}} = \\widehat{p} + 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} + 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] Alternatively, you can succinctly summarize a 95% confidence interval for \\(p\\) using the \\(\\pm\\) symbol: \\[\\widehat{p} \\pm \\text{MoE}_{\\widehat{p}} = \\widehat{p} \\pm (1.96 \\cdot \\text{SE}_{\\widehat{p}}) = \\widehat{p} \\pm \\left( 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}} \\right)\\] So going back to Yohan and Ilyas’ sample of \\(n = 50\\) balls that had 21 red balls, the 95% confidence interval for \\(p\\) is \\[ \\begin{aligned} 0.41 \\pm 1.96 \\cdot 0.0698 &amp;= 0.41 \\, \\pm \\, 0.137 \\\\ &amp;= (0.41 - 0.137, \\, 0.41 + 0.137) \\\\ &amp;= (0.273, \\, 0.547). \\end{aligned} \\] Yohan and Ilyas are 95% “confident” that the true proportion red of the bowl’s balls is between 28.3% and 55.7%. Given that the true population proportion \\(p\\) was 0.375, in this case they successfully captured the fish. In Step 4, we defined a statistical quantity known as the margin of error. You can think of this quantity as how much the net extends to the left and to the right of the center of our net. The 1.96 multiplier is rooted in the 95% rule of thumb we introduced earlier and the fact that we want the confidence level to be 95%. The value of the margin of error entirely determines the width of the confidence interval. Recall from Subsection 6.5.3 that confidence interval widths are determined by an interplay of the confidence level, the sample size \\(n\\), and the standard error. Confidence intervals based on 33 tactile samples Let’s revisit our 33 friends’ samples from the bowl from Subsection 5.1.3. We’ll use their 33 samples to construct 33 theory-based 95% confidence intervals for \\(p\\). Recall this data was saved in the tactile_prop_red data frame included in the moderndive package: rename() the variable prop_red to p_hat, the statistical name of the sample proportion \\(\\widehat{p}\\). mutate() a new variable n making explicit the sample size of 50. mutate() other new variables computing: The standard error SE for \\(\\widehat{p}\\) using the previous formula. The margin of error MoE by multiplying the SE by 1.96 The left endpoint of the confidence interval lower_ci The right endpoint of the confidence interval upper_ci conf_ints &lt;- tactile_prop_red %&gt;% rename(p_hat = prop_red) %&gt;% mutate( n = 50, SE = sqrt(p_hat * (1 - p_hat) / n), MoE = 1.96 * SE, lower_ci = p_hat - MoE, upper_ci = p_hat + MoE ) # A tibble: 33 × 9 group replicate red_balls p_hat n SE MoE lower_ci upper_ci &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Ilyas, … 1 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 2 Morgan,… 2 17 0.34 50 0.0669925 0.131305 0.208695 0.471305 3 Martin,… 3 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 4 Clark, … 4 21 0.42 50 0.0697997 0.136807 0.283193 0.556807 5 Riddhi,… 5 18 0.36 50 0.0678823 0.133049 0.226951 0.493049 6 Andrew,… 6 19 0.38 50 0.0686440 0.134542 0.245458 0.514542 7 Julia 7 19 0.38 50 0.0686440 0.134542 0.245458 0.514542 8 Rachel,… 8 11 0.22 50 0.0585833 0.114823 0.105177 0.334823 9 Daniel,… 9 15 0.3 50 0.0648074 0.127023 0.172977 0.427023 10 Josh, M… 10 17 0.34 50 0.0669925 0.131305 0.208695 0.471305 # … with 23 more rows In Figure 6.36, let’s plot the 33 confidence intervals for \\(p\\) saved in conf_ints along with a vertical line at \\(p\\) = 0.375 indicating the true proportion of the bowl’s balls that are red. Furthermore, let’s mark the sample proportions \\(\\widehat{p}\\) with dots since they represent the centers of these confidence intervals. FIGURE 6.36: 33 confidence intervals at the 95% level based on 33 tactile samples of size \\(n = 50\\). Observe that 31 of the 33 confidence intervals “captured” the true value of \\(p\\), for a success rate of 31 / 33 = 93.94%. While this is not quite 95%, recall that we expect about 95% of such confidence intervals to capture \\(p\\). The actual observed success rate will vary slightly. Theory-based methods like this have largely been used in the past because we didn’t have the computing power to perform simulation-based methods such as bootstrapping. They are still commonly used, however, and if the sampling distribution is normally distributed, we have access to an alternative method for constructing confidence intervals as well as performing hypothesis tests as we will see in Chapter 7. The kind of computer-based statistical inference we’ve seen so far has a particular name in the field of statistics: simulation-based inference. This is because we are performing statistical inference using computer simulations. In our opinion, two large benefits of simulation-based methods over theory-based methods are that (1) they are easier for people new to statistical inference to understand and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist. 6.9 Conclusion Chapter Learning Summary Bootstrap resampling with replacement is used to produce a bootstrap distribution that approximates a sampling distribution. The bootstrap distribution can be used to estimate the standard error (i.e., the standard deviation of the sampling distribution) and confidence intervals. Resampling can be performed tactically with random draws or virtually (and more quickly) with computer simulations. The percentile method calculates the 95% confidence interval by identifying the range that includes the middle 95% of values in the bootstrap distribution. The standard error method calculates the 95% confidence interval as the mean of the bootstrap distribution +/- 1.96 times its standard deviation. Confidence intervals become wider as the confidence level is increased and the sample size is decreased. Two-sample inference compares the proportion (or mean or other point estimate) of two separate samples. If the 95% (or other pre-specified) confidence interval does not include zero then there is sufficient evidence that the two groups differ. Before powerful computers, confidence intervals were calculated using methods based on mathematical theories, which use formulas to approximate the standard error; however, these theory-based methods require that certain conditions (such as a normal sampling distribution) be met for their conclusions to be trusted. 6.9.1 Additional resources If you want more examples of the infer workflow to construct confidence intervals, we suggest you check out the infer package homepage, in particular, a series of example analyses available at https://infer.netlify.app/articles/. 6.9.2 What’s to come? Now that we’ve equipped ourselves with confidence intervals, in Chapter 7 we’ll cover the other common tool for statistical inference: hypothesis testing. Just like confidence intervals, hypothesis tests are used to infer about a population using a sample. However, we’ll see that the framework for making such inferences is slightly different. "],["7-hypothesis-testing.html", "Chapter 7 Hypothesis Testing 7.1 Clutch size and malaria resistance 7.2 Understanding hypothesis tests 7.3 Conducting hypothesis tests 7.4 Interpreting hypothesis tests 7.5 Case study: Do living lizards have longer horns than dead lizards? 7.6 Theory-based hypothesis tests 7.7 Problems with p-values 7.8 Conclusion", " Chapter 7 Hypothesis Testing Now that we’ve studied confidence intervals in Chapter 6, let’s study another commonly used method for statistical inference: hypothesis testing. Hypothesis tests allow us to take a sample of data from a population and infer about the plausibility of competing hypotheses. For example, in the upcoming “clutch size” example in Section 7.1, you’ll study the data collected from a study to investigate whether their was an association between egg production and a female’s susceptibility to parasites. The good news is we’ve already covered many of the necessary concepts to understand hypothesis testing in Chapters 5 and 6. We will expand further on these ideas here and also provide a general framework for understanding hypothesis tests. By understanding this general framework, you’ll be able to adapt it to many different scenarios. The same can be said for confidence intervals. There was one general framework that applies to all confidence intervals and the infer package was designed around this framework. While the specifics may change slightly for different types of confidence intervals, the general framework stays the same. We believe that this approach is much better for long-term learning than focusing on specific details for specific confidence intervals using theory-based approaches. As you’ll now see, we prefer this general framework for hypothesis tests as well. If you’d like more practice or you’re curious to see how this framework applies to different scenarios, you can find fully-worked out examples for many common hypothesis tests and their corresponding confidence intervals in Appendix B. We recommend that you carefully review these examples as they also cover how the general frameworks apply to traditional theory-based methods like the \\(t\\)-test and normal-theory confidence intervals. You’ll see there that these traditional methods are just approximations for the computer-based methods we’ve been focusing on. However, they also require conditions to be met for their results to be valid. Computer-based methods using randomization, simulation, and bootstrapping have much fewer restrictions. Furthermore, they help develop your computational thinking, which is one big reason they are emphasized throughout this book. Chapter Learning Objectives At the end of this chapter, you should be able to… • State useful null and alternative statistical hypotheses. • Perform hypothesis testing in R using computer-based and theory-based methods. • Properly interpret p-values to reach statistical conclusions. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 4.3 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages Before you load the following packages, first check that they are installed by looking in RStudio’s Packages tab. The abd package also requires the lattice, grid, mosaic, and nlme packages, so look for these too. If any of the needed packages are missing, read Section 1.3 for information on how to install R packages. library(tidyverse) library(infer) library(moderndive) library(abd) library(ggmosaic) 7.1 Clutch size and malaria resistance Let’s begin with a study investigating if there is an association between susceptibility of birds to parasites following increased egg production. 7.1.1 Is there a tradeoff between reproduction and immune defense? What limits the number of offspring that an individual can produce? One idea is that there is a trade-off between devoting resources to reproduction versus devoting resources to systems for self-preservation, including defenses against parasites and other pathogens. To test this idea, Oppliger et al. (1996) removed the first two eggs laid in half the nests of a breeding population of the bird species Paus major. To compensate, the females in these nests produced one more egg, on average, than females in the control group, whose nests were left intact. Two weeks after egg hatching, the presence of malaria in each female parent was assessed. A summary of the results can be found in the GreatTitMalaria dataset of the abd package. Let’s have a look. GreatTitMalaria treatment response 1 Control Malaria 2 Control Malaria 3 Control Malaria 4 Control Malaria 5 Control Malaria 6 Control Malaria 7 Control Malaria 8 Control No Malaria 9 Control No Malaria 10 Control No Malaria 11 Control No Malaria 12 Control No Malaria 13 Control No Malaria 14 Control No Malaria 15 Control No Malaria 16 Control No Malaria 17 Control No Malaria 18 Control No Malaria 19 Control No Malaria 20 Control No Malaria 21 Control No Malaria 22 Control No Malaria 23 Control No Malaria 24 Control No Malaria 25 Control No Malaria 26 Control No Malaria 27 Control No Malaria 28 Control No Malaria 29 Control No Malaria 30 Control No Malaria 31 Control No Malaria 32 Control No Malaria 33 Control No Malaria 34 Control No Malaria 35 Control No Malaria 36 Egg removal Malaria 37 Egg removal Malaria 38 Egg removal Malaria 39 Egg removal Malaria 40 Egg removal Malaria 41 Egg removal Malaria 42 Egg removal Malaria 43 Egg removal Malaria 44 Egg removal Malaria 45 Egg removal Malaria 46 Egg removal Malaria 47 Egg removal Malaria 48 Egg removal Malaria 49 Egg removal Malaria 50 Egg removal Malaria 51 Egg removal No Malaria 52 Egg removal No Malaria 53 Egg removal No Malaria 54 Egg removal No Malaria 55 Egg removal No Malaria 56 Egg removal No Malaria 57 Egg removal No Malaria 58 Egg removal No Malaria 59 Egg removal No Malaria 60 Egg removal No Malaria 61 Egg removal No Malaria 62 Egg removal No Malaria 63 Egg removal No Malaria 64 Egg removal No Malaria 65 Egg removal No Malaria Let’s perform an exploratory data analysis of the relationship between the two categorical variables malaria and treatment. Recall that we saw in Subsection 2.8.2 that one way we can visualize such a relationship is by using a stacked barplot. ggplot(GreatTitMalaria, aes(x = treatment, fill = response)) + geom_bar() + labs(x = &quot;Treatment&quot;) FIGURE 7.1: Barplot relating bird treatment to malaria infection. Observe in Figure 7.1 that while some of the females on the control nests were infected with the malaria parasite, a greater fraction of females on the experimentally-manipulated nests were infected. Because of the difference in the number of females in the two treatment groups, it’s a little difficult to compare the rates of infection. To see this better, let’s introduce another type of plot called a mosaic plot, which uses geom_mosaic() from the ggmosaic package. ggplot(GreatTitMalaria) + geom_mosaic(aes(x = product(treatment), fill = response)) + labs(x = &quot;Treatment&quot;, y = &quot;Relative frequency&quot;) FIGURE 7.2: Mosaic plot relating bird treatment to frequency of malaria infection. While the bars are also stacked in a mosaic plot, the width of each bar on the x-axis indicates the size of each group. Furthermore, proportions are displayed on the y-axis, rather than counts. In this mosaic plot, it’s easier to see that a larger proportion of the experimentally treated females were infected with malaria compared to the control females. To summarize, of the 35 control females, 7 became infected with the malaria parasite, for a proportion of 7/35 = 0.2 = 20%. On the other hand, of the 30 females in the experimental group, 15 became infected with the malaria parasite, for a proportion of 15/30 = 0.5 = 50%. Comparing these two rates of malaria parasite infection, it appears that experimentally-manipulated females were infected at a rate 0.5 - 0.2 = 0.3 = 30% higher than control females. This is highly suggestive of an increased sensitivity of treated females to malaria infection. The question is, however, does this provide conclusive evidence that there is difference in susceptibility to parasite infection as a result of this experimental manipulation? Could a difference in infection rates of 30% still occur by chance, even in a hypothetical world where egg removal didn’t increase the risk of malaria infection? In other words, what is the role of sampling variation in this hypothesized world? To answer this question, we’ll again rely on a computer to run simulations. 7.1.2 Shuffling once To run the simulations, we will work with the original GreatTitMalaria dataset. To understand the probability of the observed results, first try to imagine a hypothetical universe where egg removal didn’t affect the likelihood of a malaria infection. In such a hypothetical universe, the nest status of a female bird would have no bearing on their chances of becoming infected by a parasite. Bringing things back to our GreatTitMalaria data frame, the treatment variable would thus be an irrelevant label for predicting malaria infection. If these treatment labels were irrelevant, then we could randomly reassign them by “shuffling” them to no consequence! To illustrate this idea, in Table 7.1 we narrow our focus to 6 arbitrarily chosen females of the 65 . The malaria column shows that 3 females were infected while 3 weren’t. The treatment column shows the nest treatment, control or egg removal. However, in our hypothesized universe of no effect of egg removal, treatment is irrelevant and thus it is of no consequence to randomly “shuffle” the values of treatment. The shuffled_treatment column shows one such possible random shuffling. Observe in the last column how the number of females in the Egg removal group and the Control group remains the same at 3 each, but they are now listed in a different order. TABLE 7.1: One example of shuffling treatment variable treatment response shuffled_treatment Egg removal Malaria Egg removal Egg removal Malaria Control Control Malaria Egg removal Control No Malaria Control Control No Malaria Control Egg removal No Malaria Egg removal Again, such random shuffling of the treatment label only makes sense in our hypothesized universe of no treatment effect. How could we extend this shuffling of the treatment variable to all 65 birds by hand? One way would be by using standard decks of playing cards, like the one displayed in Figure 7.3. FIGURE 7.3: Standard deck of 52 playing cards. In our shuffling simulation, we will start with 35 red cards, the number of birds in the control group, and 30 black cards, the number of birds in the egg removal group. After shuffling these 65 cards as seen in Figure 7.4, we can flip the cards over one-by-one, assigning Control to the shuffled treatment variable for each red card and Egg removal to the shuffled treatment variable for each black card. FIGURE 7.4: Shuffling a deck of cards. We’ve saved one such shuffling in the GreatTitMalaria_shuffled data frame. If you compare the original GreatTitMalaria and the shuffled GreatTitMalaria_shuffled data frames, you’ll see that while the malaria variable is identical, the shuffled_treatment variable differs from treatment. Let’s repeat the same exploratory data analysis we did for the original GreatTitMalaria data on our GreatTitMalaria_shuffled data frame. Let’s create a bar plot visualizing the relationship between malaria and the new shuffled treatment variable and compare this to the original unshuffled version in Figure 7.5. ggplot(GreatTitMalaria_shuffled, aes(x = shuffled_treatment, fill = response)) + geom_bar() + labs(x = &quot;Shuffled treatment&quot;) FIGURE 7.5: Barplots of relationship of response with treatment (left) and shuffled treatment (right). In the original data, in the left barplot, the rate of malaria (red) in females following egg removal is clearly much greater than the rate in control females. In comparison, with the new “shuffled” data visualized in the right barplot, the rates of malaria appear pretty similar between the females in the two groups. Let’s also compute the proportion of female birds with malaria for each shuffled_treatment group: GreatTitMalaria_shuffled %&gt;% group_by(shuffled_treatment, response) %&gt;% tally() # Same as summarize(n = n()) # A tibble: 4 × 3 # Groups: shuffled_treatment [2] shuffled_treatment response n &lt;fct&gt; &lt;fct&gt; &lt;int&gt; 1 Control Malaria 13 2 Control No Malaria 22 3 Egg removal Malaria 9 4 Egg removal No Malaria 21 So in this hypothetical universe of no effect of egg removal, 9/30 = 0.3 = 30% of females following egg removal became infected with malaria. On the other hand, 13/35 = 0.371 = 37.1% of control females became infected with malaria. Let’s next compare these two values. In this hypothetical universe of no effect of egg removal, it appears that females following egg removal were infected with malaria at a rate that was 0.3 - 0.371 = 0.071 = 7.1% different than control females. Confirming what we visualized in the stacked barplot above, the malaria rate of female birds in the two groups is very similar. But why didn’t we see exactly the same malaria rates between the two groups. This is due to sampling variation. How can we better understand the effect of this sampling variation? By repeating this shuffling many more times! 7.1.3 Shuffling 99 more times Imagine that we repeated the shuffling process above 99 more times and recorded the values in 99 more shuffled__treatment columns. The shuffled_results dataset includes the results of one such scenario For each of these 100 columns of shuffles, we computed the difference in malaria rates between control and experimentally treated birds, and in Figure 7.6 we display their distribution in a histogram. We also mark the observed difference in malaria rate that occurred in real life of 0.3 = 30% with a dark line. FIGURE 7.6: Distribution of shuffled differences in GreatTitMalaria. Before we discuss the distribution of the histogram, we emphasize the key thing to remember: this histogram represents differences in malaria rates that one would observe in our hypothesized universe where egg removal has no effect on susceptibility to the malaria parasite. Observe first that the histogram is roughly centered at 0. Saying that the difference in malaria rates is 0 is equivalent to saying that female birds in both groups had the same malaria rate. In other words, the center of these 100 values is consistent with what we would expect in our hypothesized universe of no effect of treatment on immune response. However, while the values are centered at 0, there is variation about 0. This is because even in a hypothesized universe of no treatment effect, you will still likely observe small differences in malaria rates because of chance sampling variation. Looking at the histogram in Figure 7.6, such differences could even be as extreme as -0.319 or 0.238. Turning our attention to what we observed in real life: the difference of 0.3 = 30% is marked with a vertical dark line. Ask yourself: in a hypothesized world of no effect of the treatment, how likely would it be that we observe this difference? Clearly, not often! Now ask yourself: what do these results say about our hypothesized universe of no effect of egg removal on susceptibility to malaria? 7.1.4 What did we just do? What we just demonstrated in this activity is the statistical procedure known as hypothesis testing using a permutation test. The term “permutation” is the mathematical term for “shuffling”: taking a series of values and reordering them randomly, as you did with the playing cards. In fact, permutations are another form of resampling, like the bootstrap method you performed in Chapter 6. While the bootstrap method involves resampling with replacement, permutation methods involve resampling without replacement. Think of our exercise involving the slips of paper representing pennies and the hat in Section 6.1: after sampling a penny, you put it back in the hat. Now think of our deck of cards. After drawing a card, you laid it out in front of you, recorded the color, and then you did not put it back in the deck. In our previous example, we tested the validity of the hypothesized universe of no effect of egg removal on malaria susceptibility. The evidence contained in our observed sample of 65 female birds was somewhat inconsistent with our hypothesized universe. Thus, we would be inclined to reject this hypothesized universe and declare that the evidence suggests there is an effect of egg removal on malaria susceptibility. Recall our case study on whether yawning is contagious from Section 6.6. The previous example involves inference about an unknown difference of population proportions as well. This time, it will be \\(p_{e} - p_{c}\\), where \\(p_{e}\\) is the population proportion of birds after egg removal becoming infected with malaria and \\(p_{c}\\) is the equivalent for birds on control nests. Recall that this is one of the scenarios for inference we’ve seen so far in Table 7.2. TABLE 7.2: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) So, based on our sample of \\(n_w\\) = 30 experimental females and \\(n_c\\) = 35 control females, the point estimate for \\(p_{e} - p_{c}\\) is the difference in sample proportions \\(\\widehat{p}_{e} -\\widehat{p}_{c}\\) = 0.5 - 0.2 = 0.3 = 30%. This difference in susceptibility of experimentally-treated females of 0.3 is greater than 0, suggesting that increased reproductive effort due to egg removal has an effect on malaria susceptibility. However, the question we asked ourselves was “is this difference meaningfully greater than 0?”. In other words, is that difference indicative of a true effect, or can we just attribute it to sampling variation? Hypothesis testing allows us to make such distinctions. 7.2 Understanding hypothesis tests Much like the terminology, notation, and definitions relating to sampling you saw in Section 5.3, there are a lot of terminology, notation, and definitions related to hypothesis testing as well. Learning these may seem like a very daunting task at first. However, with practice, practice, and more practice, anyone can master them. First, a hypothesis is a statement about the value of an unknown population parameter. In the bird study, the population parameter of interest is the difference in population proportions \\(p_{e} - p_{c}\\). Hypothesis tests can involve any of the population parameters in Table 5.5 of the five inference scenarios we’ll cover in this book and also more advanced types we won’t cover here. Second, a hypothesis test consists of a test between two competing hypotheses: (1) a null hypothesis \\(H_0\\) (pronounced “H-naught”) versus (2) an alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\)). Generally, the null hypothesis is a claim that there is “no effect” or “no difference of interest.” In many cases, the null hypothesis represents the status quo or a situation that nothing interesting is happening. Furthermore, generally the alternative hypothesis is the claim that the experimenter or researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\). In the bird study, an appropriate hypothesis test would be: \\[ \\begin{aligned} H_0 &amp;: \\text{control and experimental birds are infected at the same rate}\\\\ \\text{vs } H_A &amp;: \\text{experimental birds are infected at a higher rate than control birds} \\end{aligned} \\] Note some of the choices we have made. First, we set the null hypothesis \\(H_0\\) to be that there is no difference in infection rate and the “challenger” alternative hypothesis \\(H_A\\) to be that there is a difference. While it would not be wrong in principle to reverse the two, it is a convention in statistical inference that the null hypothesis is set to reflect a “null” situation where “nothing is going on.” As we discussed earlier, in this case, \\(H_0\\) corresponds to there being no difference in infection rates. Furthermore, we set \\(H_A\\) to be that experimental birds are infected at a higher rate, a subjective choice reflecting a prior suspicion we have that this is the case. We call such alternative hypotheses one-sided alternatives. If someone else, however, does not share such suspicions and only wants to investigate that there is a difference, whether higher or lower, they would set what is known as a two-sided alternative. We can re-express the formulation of the alternative hypothesis test using the mathematical notation for our population parameter of interest, the difference in population proportions \\(p_{e} - p_{c}\\): \\[ \\begin{aligned} H_0 &amp;: p_{e} - p_{c} = 0\\\\ \\text{vs } H_A&amp;: p_{e} - p_{c} &gt; 0 \\end{aligned} \\] Observe how the alternative hypothesis \\(H_A\\) is one-sided with \\(p_{e} - p_{c} &gt; 0\\). Had we opted for a two-sided alternative, we would have set \\(p_{e} - p_{c} \\neq 0\\). To keep things simple for now, we’ll stick with the simpler one-sided alternative. We’ll present an example of a two-sided alternative in Section 7.5. Third, a test statistic is a point estimate/sample statistic formula used for hypothesis testing. Note that a sample statistic is merely a summary statistic based on a sample of observations. Recall from Section 3.5 that a summary statistic takes in many values and returns only one. Here, the samples would be the \\(n_e\\) = 30 experimentally-treated females and the \\(n_c\\) = 35 control females. Hence, the point estimate of interest is the difference in sample proportions \\(\\widehat{p}_{e} - \\widehat{p}_{c}\\). Fourth, the observed test statistic is the value of the test statistic that we observed in real life. In our case, we computed this value using the data saved in the GreatTitMalaria data frame. It was the observed difference of \\(\\widehat{p}_{e} -\\widehat{p}_{c} = 0.5 - 0.2 = 0.3 = 30\\%\\) in favor of control birds. Fifth, the null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true. Ooof! That’s a long one! Let’s unpack it slowly. The key to understanding the null distribution is that the null hypothesis \\(H_0\\) is assumed to be true. We’re not saying that \\(H_0\\) is true at this point, we’re only assuming it to be true for hypothesis testing purposes. In our case, this corresponds to our hypothesized universe of no effect of egg removal on infection rates. Assuming the null hypothesis \\(H_0\\), also stated as “Under \\(H_0\\),” how does the test statistic vary due to sampling variation? In our case, how will the difference in sample proportions \\(\\widehat{p}_{c} - \\widehat{p}_{e}\\) vary due to sampling under \\(H_0\\)? Recall from Subsection 5.3.2 that distributions displaying how point estimates vary due to sampling variation are called sampling distributions. The only additional thing to keep in mind about null distributions is that they are sampling distributions assuming the null hypothesis \\(H_0\\) is true. In our case, we previously visualized a null distribution in Figure 7.6, which we re-display in Figure 7.7 using our new notation and terminology. It is the distribution of the 100 differences in sample proportions our friends computed assuming a hypothetical universe of no treatment effect. We also mark the value of the observed test statistic of 0.3 with a vertical line. FIGURE 7.7: Null distribution and observed test statistic. Sixth, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. Double ooof! Let’s unpack this slowly as well. You can think of the \\(p\\)-value as a quantification of “surprise”: assuming \\(H_0\\) is true, how surprised are we with what we observed? Or in our case, in our hypothesized universe of no effect of the egg removal treatment, how surprised are we that we observed a difference in infection rates of 0.3 from our collected samples assuming \\(H_0\\) is true? Very surprised? Somewhat surprised? The \\(p\\)-value quantifies this probability, or in the case of our 100 differences in sample proportions in Figure 7.7, what proportion had a more “extreme” result? Here, extreme is defined in terms of the alternative hypothesis \\(H_A\\) that control birds are infected at a lower rate than experimental birds. In other words, how often was the infection rate of experimental birds even more pronounced than \\(0.5 - 0.2 = 0.3 = 30\\%\\)? In this case, 100 times out of 100, we obtained a difference in proportion greater than or equal to the observed difference of 0.3 = 30%. A very rare (in fact, not occurring) outcome! Given the rarity of such a pronounced difference in infection rates in our hypothesized universe of no treatment effect, we’re inclined to reject our hypothesized universe. Instead, we favor the hypothesis stating there is an effect of egg removal on susceptibility to malaria infection. In other words, we reject \\(H_0\\) in favor of \\(H_A\\). Seventh and lastly, in many hypothesis testing procedures, it is commonly recommended to set the significance level of the test beforehand. It is denoted by the Greek letter \\(\\alpha\\) (pronounced “alpha”). This value acts as a cutoff on the \\(p\\)-value, where if the \\(p\\)-value falls below \\(\\alpha\\), we would “reject the null hypothesis \\(H_0\\).” Alternatively, if the \\(p\\)-value does not fall below \\(\\alpha\\), we would “fail to reject \\(H_0\\).” Note the latter statement is not quite the same as saying we “accept \\(H_0\\).” This distinction is rather subtle and not immediately obvious. So we’ll revisit it later in Section 7.4. While different fields tend to use different values of \\(\\alpha\\), some commonly used values for \\(\\alpha\\) are 0.1, 0.01, and 0.05; with 0.05 being the choice people often make without putting much thought into it. We’ll talk more about \\(\\alpha\\) significance levels in Section 7.4, but first let’s fully conduct the hypothesis test corresponding to our GreatTitMalaria activity using the infer package. 7.3 Conducting hypothesis tests In Section 6.4, we showed you how to construct confidence intervals. We first illustrated how to do this using dplyr data wrangling verbs and the rep_sample_n() function from Subsection 5.2.3 which we used as a virtual shovel. In particular, we constructed confidence intervals by resampling with replacement by setting the replace = TRUE argument to the rep_sample_n() function. We then showed you how to perform the same task using the infer package workflow. While both workflows resulted in the same bootstrap distribution from which we can construct confidence intervals, the infer package workflow emphasizes each of the steps in the overall process in Figure 7.8. It does so using function names that are intuitively named with verbs: specify() the variables of interest in your data frame. generate() replicates of bootstrap resamples with replacement. calculate() the summary statistic of interest. visualize() the resulting bootstrap distribution and confidence interval. FIGURE 7.8: Confidence intervals with the infer package. In this section, we’ll now show you how to seamlessly modify the previously seen infer code for constructing confidence intervals to conduct hypothesis tests. You’ll notice that the basic outline of the workflow is almost identical, except for an additional hypothesize() step between the specify() and generate() steps, as can be seen in Figure 7.9. FIGURE 7.9: Hypothesis testing with the infer package. Furthermore, we’ll use a pre-specified significance level \\(\\alpha\\) = 0.05 for this hypothesis test. Let’s leave discussion on the choice of this \\(\\alpha\\) value until later on in Section 7.4. 7.3.1 infer package workflow 1. specify variables Recall that we use the specify() verb to specify the response variable and, if needed, any explanatory variables for our study. In this case, since we are interested in any potential effects of treatment on infection rates, we set response as the response variable and treatment as the explanatory variable. We do so using formula = response ~ explanatory where response is the name of the response variable in the data frame and treatment is the name of the explanatory variable. So in our case it is response ~ treatment. Furthermore, since we are interested in the proportion of female birds with \"Malaria\", and not the proportion of birds with \"No Malaria\", we set the argument success to \"Malaria\". GreatTitMalaria %&gt;% specify(formula = response ~ treatment, success = &quot;Malaria&quot;) Response: response (factor) Explanatory: treatment (factor) # A tibble: 65 × 2 response treatment &lt;fct&gt; &lt;fct&gt; 1 Malaria Control 2 Malaria Control 3 Malaria Control 4 Malaria Control 5 Malaria Control 6 Malaria Control 7 Malaria Control 8 No Malaria Control 9 No Malaria Control 10 No Malaria Control # … with 55 more rows Again, notice how the GreatTitMalaria data itself doesn’t change, but the Response: response (factor) and Explanatory: treatment (factor) meta-data do. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Section 3.6. 2. hypothesize the null In order to conduct hypothesis tests using the infer workflow, we need a new step not present for confidence intervals: hypothesize(). Recall from Section 7.2 that our hypothesis test was \\[ \\begin{aligned} H_0 &amp;: p_{e} - p_{c} = 0\\\\ \\text{vs. } H_A&amp;: p_{e} - p_{c} &gt; 0 \\end{aligned} \\] In other words, the null hypothesis \\(H_0\\) corresponding to our “hypothesized universe” stated that there was no difference in treatment-based infection rates. We set this null hypothesis \\(H_0\\) in our infer workflow using the null argument of the hypothesize() function to either: \"point\" for hypotheses involving a single sample or \"independence\" for hypotheses involving two samples. In our case, since we have two samples (the birds on control nests and those on nests with egg removal), we set null = \"independence\". GreatTitMalaria %&gt;% specify(formula = response ~ treatment, success = &quot;Malaria&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) Response: response (factor) Explanatory: treatment (factor) Null Hypothesis: independence # A tibble: 65 × 2 response treatment &lt;fct&gt; &lt;fct&gt; 1 Malaria Control 2 Malaria Control 3 Malaria Control 4 Malaria Control 5 Malaria Control 6 Malaria Control 7 Malaria Control 8 No Malaria Control 9 No Malaria Control 10 No Malaria Control # … with 55 more rows Again, the data has not changed yet. This will occur at the upcoming generate() step; we’re merely setting meta-data for now. Where do the terms \"point\" and \"independence\" come from? These are two technical statistical terms. The term “point” relates from the fact that for a single group of observations, you will test the value of a single point. Going back to the pennies example from Chapter 6, say we wanted to test if the mean year of all US pennies was equal to 1993 or not. We would be testing the value of a “point” \\(\\mu\\), the mean year of all US pennies, as follows \\[ \\begin{aligned} H_0 &amp;: \\mu = 1993\\\\ \\text{vs } H_A&amp;: \\mu \\neq 1993 \\end{aligned} \\] The term “independence” relates to the fact that for two groups of observations, you are testing whether or not the response variable is independent of the explanatory variable that assigns the groups. In our case, we are testing whether the malaria response variable is “independent” of the explanatory variable treatment that assigns each female bird to either of the two groups. 3. generate replicates After we hypothesize() the null hypothesis, we generate() replicates of “shuffled” datasets assuming the null hypothesis is true. We do this by repeating the shuffling exercise you performed in Section 7.1 several times. Instead of merely doing it 100 times as our groups of friends did, let’s use the computer to repeat this 1000 times by setting reps = 1000 in the generate() function. However, unlike for confidence intervals where we generated replicates using type = \"bootstrap\" resampling with replacement, we’ll now perform shuffles/permutations by setting type = \"permute\". Recall that shuffles/permutations are a kind of resampling, but unlike the bootstrap method, they involve resampling without replacement. GreatTitMalaria_generate &lt;- GreatTitMalaria %&gt;% specify(formula = response ~ treatment, success = &quot;Malaria&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) nrow(GreatTitMalaria_generate) [1] 65000 Observe that the resulting data frame has 65,000 rows. This is because we performed shuffles/permutations for each of the 65 rows 1000 times and \\(65,000 = 1000 \\cdot 65\\). If you explore the GreatTitMalaria_generate data frame with View(), you’ll notice that the variable replicate indicates which resample each row belongs to. So it has the value 1 65 times, the value 2 65 times, all the way through to the value 1000, again 65 times. 4. calculate summary statistics Now that we have generated 1000 replicates of “shuffles” assuming the null hypothesis is true, let’s calculate() the appropriate summary statistic for each of our 1000 shuffles. From Section 7.2, point estimates related to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the difference in population proportions \\(p_{e} - p_{c}\\), the test statistic here is the difference in sample proportions \\(\\widehat{p}_{e} - \\widehat{p}_{c}\\). For each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"diff in props\". Furthermore, since we are interested in \\(\\widehat{p}_{e} - \\widehat{p}_{c}\\) we set order = c(\"Egg removal\", \"Control\"). As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly. Let’s save the result in a data frame called null_distribution: null_distribution &lt;- GreatTitMalaria %&gt;% specify(formula = response ~ treatment, success = &quot;Malaria&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;Egg removal&quot;, &quot;Control&quot;)) null_distribution Response: response (factor) Explanatory: treatment (factor) Null Hypothesis: independence # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.176190 2 2 -0.0714286 3 3 -0.319048 4 4 -0.0714286 5 5 -0.0714286 6 6 0.0523810 7 7 -0.195238 8 8 0.0523810 9 9 -0.0714286 10 10 0.114286 # … with 990 more rows Observe that we have 1000 values of stat, each representing one instance of \\(\\widehat{p}_{e} - \\widehat{p}_{c}\\) in a hypothesized world of no treatment effect. Observe as well that we chose the name of this data frame carefully: null_distribution. Recall once again from Section 7.2 that sampling distributions when the null hypothesis \\(H_0\\) is assumed to be true have a special name: the null distribution. What was the observed difference in infection rates? In other words, what was the observed test statistic \\(\\widehat{p}_{e} - \\widehat{p}_{c}\\)? Recall from Section 7.1 that we computed this observed difference by hand to be 0.5 - 0.2 = 0.3 = 30%. We can also compute this value using the previous infer code but with the hypothesize() and generate() steps removed. Let’s save this in obs_diff_prop: obs_diff_prop &lt;- GreatTitMalaria %&gt;% specify(response ~ treatment, success = &quot;Malaria&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;Egg removal&quot;, &quot;Control&quot;)) obs_diff_prop Response: response (factor) Explanatory: treatment (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 0.3 5. visualize the p-value The final step is to measure how surprised we are by a malaria infection difference of 30% in a hypothesized universe of no treatment effect. If the observed difference of 0.3 is highly unlikely, then we would be inclined to reject the validity of our hypothesized universe. We start by visualizing the null distribution of our 1000 values of \\(\\widehat{p}_{c} - \\widehat{p}_{e}\\) using visualize() in Figure 7.10. Recall that these are values of the difference in infection rates assuming \\(H_0\\) is true. This corresponds to being in our hypothesized universe of no treatment effect. visualize(null_distribution, bins = 10) FIGURE 7.10: Null distribution. Let’s now add what happened in real life to Figure 7.10, the observed difference in infection rates of 0.5 - 0.2 = 0.3 = 30%. However, instead of merely adding a vertical line using geom_vline(), let’s use the shade_p_value() function with obs_stat set to the observed test statistic value we saved in obs_diff_prop. Furthermore, we’ll set the direction = \"right\" reflecting our alternative hypothesis \\(H_A: p_{e} - p_{c} &gt; 0\\). Recall our alternative hypothesis \\(H_A\\) is that \\(p_{e} - p_{c} &gt; 0\\), stating that there is a higher infection rate in experimental females compared to control females. “More extreme” here corresponds to differences that are “greater” or “more positive” or “more to the right” Hence we set the direction argument of shade_p_value() to be \"right\". On the other hand, had our alternative hypothesis \\(H_A\\) been the other possible one-sided alternative \\(p_{e} - p_{c} &lt; 0\\), suggesting that the experimental treatment decreases susceptibility to infection, we would’ve set direction = \"left\". Had our alternative hypothesis \\(H_A\\) been two-sided \\(p_{e} - p_{c} \\neq 0\\), suggesting an effect in either direction, we would’ve set direction = \"both\". visualize(null_distribution, bins = 10) + shade_p_value(obs_stat = obs_diff_prop, direction = &quot;right&quot;) FIGURE 7.11: Shaded histogram to show \\(p\\)-value. In the resulting Figure 7.11, the solid dark line marks 0.3 = 30%. However, what does the shaded-region correspond to? This is the \\(p\\)-value. Recall the definition of the \\(p\\)-value from Section 7.2: A \\(p\\)-value is the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. So judging by the shaded region in Figure 7.11, it seems we would rarely observe differences in infection rates of 0.3 = 30% or more in a hypothesized universe of no treatment effect. In other words, the \\(p\\)-value is very small. Hence, we would be inclined to reject this hypothesized universe, or using statistical language we would “reject \\(H_0\\).” What fraction of the null distribution is shaded? In other words, what is the exact value of the \\(p\\)-value? We can compute it using the get_p_value() function with the same arguments as the previous shade_p_value() code: null_distribution %&gt;% get_p_value(obs_stat = obs_diff_prop, direction = &quot;right&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0.007 Keeping the definition of a \\(p\\)-value in mind, the probability of observing a difference in infection rates as large as 0.3 = 30% due to sampling variation alone in the null distribution is 0.007 = 0.7%. Since this \\(p\\)-value is smaller than our pre-specified significance level \\(\\alpha\\) = 0.05, we reject the null hypothesis \\(H_0: p_{e} - p_{c} = 0\\). In other words, this \\(p\\)-value is sufficiently small to reject our hypothesized universe of no treatment effect. We instead have enough evidence to change our mind in favor of the egg removal treatment having an effect on susceptibility to malaria infection. Observe that whether we reject the null hypothesis \\(H_0\\) or not depends in large part on our choice of significance level \\(\\alpha\\). We’ll discuss this more in Subsection 7.4.3. 7.3.2 Comparison with confidence intervals One of the great things about the infer package is that we can jump seamlessly between conducting hypothesis tests and constructing confidence intervals with minimal changes! Recall the code from the previous section that creates the null distribution, which in turn is needed to compute the \\(p\\)-value: null_distribution &lt;- GreatTitMalaria %&gt;% specify(formula = response ~ treatment, success = &quot;Malaria&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;Egg removal&quot;, &quot;Control&quot;)) To create the corresponding bootstrap distribution needed to construct a 95% confidence interval for \\(p_{e} - p_{c}\\), we only need to make two changes. First, we remove the hypothesize() step since we are no longer assuming a null hypothesis \\(H_0\\) is true. We can do this by deleting or commenting out the hypothesize() line of code. Second, we switch the type of resampling in the generate() step to be \"bootstrap\" instead of \"permute\". bootstrap_distribution &lt;- GreatTitMalaria %&gt;% specify(formula = response ~ treatment, success = &quot;Malaria&quot;) %&gt;% # Change 1 - Remove hypothesize(): # hypothesize(null = &quot;independence&quot;) %&gt;% # Change 2 - Switch type from &quot;permute&quot; to &quot;bootstrap&quot;: generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;Egg removal&quot;, &quot;Control&quot;)) Using this bootstrap_distribution, let’s first compute the percentile-based confidence intervals, as we did in Section 6.4: percentile_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0700739 0.505840 Using our shorthand interpretation for 95% confidence intervals from Subsection 6.5.2, we are 95% “confident” that the true difference in population proportions \\(p_{e} - p_{c}\\) is between (0.07, 0.506). Let’s visualize bootstrap_distribution and this percentile-based 95% confidence interval for \\(p_{e} - p_{c}\\) in Figure 7.12. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = percentile_ci) FIGURE 7.12: Percentile-based 95% confidence interval. Notice a key value that is not included in the 95% confidence interval for \\(p_{e} - p_{c}\\): the value 0. In other words, a difference of 0 is not included in our net, suggesting that \\(p_{e}\\) and \\(p_{c}\\) are truly different! Furthermore, observe how the entirety of the 95% confidence interval for \\(p_{e} - p_{c}\\) lies above 0, suggesting that this difference is positive for the experimental birds. Since the bootstrap distribution appears to be roughly normally shaped, we can also use the standard error method as we did in Section 6.4. In this case, we must specify the point_estimate argument as the observed difference in infection rates 0.3 = 30% saved in obs_diff_prop. This value acts as the center of the confidence interval. se_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;se&quot;, point_estimate = obs_diff_prop) se_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0752680 0.524732 Let’s visualize bootstrap_distribution again, but now the standard error based 95% confidence interval for \\(p_{e} - p_{c}\\) in Figure 7.13. Again, notice how the value 0 is not included in our confidence interval, again suggesting that \\(p_{e}\\) and \\(p_{c}\\) are truly different! visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = se_ci) FIGURE 7.13: Standard error-based 95% confidence interval. Learning check (LC7.1) Why does the following code produce an error? In other words, what about the response and predictor variables make this not a possible computation with the infer package? library(moderndive) library(infer) null_distribution_cean &lt;- GreatTitMalaria %&gt;% specify(formula = response ~ treatment, success = &quot;Malaria&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;Egg removal&quot;, &quot;Control&quot;)) (LC7.2) Why are we relatively confident that the distributions of the sample proportions will be good approximations of the population distributions of infection proportions for the two treatment groups? (LC7.3) Using the definition of p-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the infection rates for control and experimental birds. 7.3.3 “There is only one test” Let’s recap the steps necessary to conduct a hypothesis test using the terminology, notation, and definitions related to sampling you saw in Section 7.2 and the infer workflow from Subsection 7.3.1: specify() the variables of interest in your data frame. hypothesize() the null hypothesis \\(H_0\\). In other words, set a “model for the universe” assuming \\(H_0\\) is true. generate() shuffles assuming \\(H_0\\) is true. In other words, simulate data assuming \\(H_0\\) is true. calculate() the test statistic of interest, both for the observed data and your simulated data. visualize() the resulting null distribution and compute the \\(p\\)-value by comparing the null distribution to the observed test statistic. While this is a lot to digest, especially the first time you encounter hypothesis testing, the nice thing is that once you understand this general framework, then you can understand any hypothesis test. In a famous blog post, computer scientist Allen Downey called this the “There is only one test” framework, for which he created the flowchart displayed in Figure 7.14. FIGURE 7.14: Allen Downey’s hypothesis testing framework. Notice its similarity with the “hypothesis testing with infer” diagram you saw in Figure 7.9. That’s because the infer package was explicitly designed to match the “There is only one test” framework. So if you can understand the framework, you can easily generalize these ideas for all hypothesis testing scenarios. Whether for population proportions \\(p\\), for population means \\(\\mu\\),for differences in population proportions \\(p_1 - p_2\\), and as you’ll see later, for differences in population means \\(\\mu_1 - \\mu_2\\) (Section 7.5), and for population regression slopes \\(\\beta_1\\) (Chapter 9). In fact, it applies more generally even than just these examples to more complicated hypothesis tests and test statistics as well, many of which are explored in “Full infer Pipeline Examples”. Learning check (LC7.4) Describe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between the infection rate of control and experimental birds using this study. 7.4 Interpreting hypothesis tests Interpreting the results of hypothesis tests is one of the more challenging aspects of this method for statistical inference. In this section, we’ll focus on ways to help with deciphering the process and address some common misconceptions. 7.4.1 Two possible outcomes In Section 7.2, we mentioned that given a pre-specified significance level \\(\\alpha\\) there are two possible outcomes of a hypothesis test: If the \\(p\\)-value is less than \\(\\alpha\\), then we reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\). If the \\(p\\)-value is greater than or equal to \\(\\alpha\\), we fail to reject the null hypothesis \\(H_0\\). Unfortunately, the latter result is often misinterpreted as “accepting the null hypothesis \\(H_0\\).” While at first glance it may seem that the statements “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent, there actually is a subtle difference. Saying that we “accept the null hypothesis \\(H_0\\)” is equivalent to stating that “we think the null hypothesis \\(H_0\\) is true.” However, saying that we “fail to reject the null hypothesis \\(H_0\\)” is saying something else: “While \\(H_0\\) might still be false, we don’t have enough evidence to say so.” In other words, there is an absence of enough proof. However, the absence of proof is not proof of absence. To further shed light on this distinction, let’s use the United States criminal justice system as an analogy. A criminal trial in the United States is a similar situation to hypothesis tests whereby a choice between two contradictory claims must be made about a defendant who is on trial: The defendant is truly either “innocent” or “guilty.” The defendant is presumed “innocent until proven guilty.” The defendant is found guilty only if there is strong evidence that the defendant is guilty. The phrase “beyond a reasonable doubt” is often used as a guideline for determining a cutoff for when enough evidence exists to find the defendant guilty. The defendant is found to be either “not guilty” or “guilty” in the ultimate verdict. In other words, not guilty verdicts are not suggesting the defendant is innocent, but instead that “while the defendant may still actually be guilty, there wasn’t enough evidence to prove this fact.” Now let’s make the connection with hypothesis tests: Either the null hypothesis \\(H_0\\) or the alternative hypothesis \\(H_A\\) is true. Hypothesis tests are conducted assuming the null hypothesis \\(H_0\\) is true. We reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\) only if the evidence found in the sample suggests that \\(H_A\\) is true. The significance level \\(\\alpha\\) is used as a guideline to set the threshold on just how strong of evidence we require. We ultimately decide to either “fail to reject \\(H_0\\)” or “reject \\(H_0\\).” So while gut instinct may suggest “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent statements, they are not. “Accepting \\(H_0\\)” is equivalent to finding a defendant innocent. However, courts do not find defendants “innocent,” but rather they find them “not guilty.” Putting things differently, defense attorneys do not need to prove that their clients are innocent, rather they only need to prove that clients are not “guilty beyond a reasonable doubt”. So going back to our bird study in Section 7.3, recall that our hypothesis test was \\(H_0: p_{e} - p_{c} = 0\\) versus \\(H_A: p_{e} - p_{c} &gt; 0\\) and that we used a pre-specified significance level of \\(\\alpha\\) = 0.05. We found a \\(p\\)-value of 0.007. Since the \\(p\\)-value was smaller than \\(\\alpha\\) = 0.05, we rejected \\(H_0\\). In other words, we found needed levels of evidence in this particular sample to say that \\(H_0\\) is false at the \\(\\alpha\\) = 0.05 significance level. We also state this conclusion using non-statistical language: we found enough evidence in this data to suggest that there was a treatment effect at play. 7.4.2 Types of errors Unfortunately, there is some chance a jury or a judge can make an incorrect response in a criminal trial by reaching the wrong verdict. For example, finding a truly innocent defendant “guilty”. Or on the other hand, finding a truly guilty defendant “not guilty.” This can often stem from the fact that prosecutors don’t have access to all the relevant evidence, but instead are limited to whatever evidence the police can find. The same holds for hypothesis tests. We can make incorrect decisions about a population parameter because we only have a sample of data from the population and thus sampling variation can lead us to incorrect conclusions. There are two possible erroneous conclusions in a criminal trial: either (1) a truly innocent person is found guilty or (2) a truly guilty person is found not guilty. Similarly, there are two possible errors in a hypothesis test: either (1) rejecting \\(H_0\\) when in fact \\(H_0\\) is true, called a Type I error or (2) failing to reject \\(H_0\\) when in fact \\(H_0\\) is false, called a Type II error. Another term used for “Type I error” is “false positive,” while another term for “Type II error” is “false negative.” This risk of error is the price researchers pay for basing inference on a sample instead of performing a census on the entire population. But as we’ve seen in our numerous examples and activities so far, censuses are often very expensive and other times impossible, and thus researchers have no choice but to use a sample. Thus in any hypothesis test based on a sample, we have no choice but to tolerate some chance that a Type I error will be made and some chance that a Type II error will occur. To help understand the concepts of Type I error and Type II errors, we apply these terms to our criminal justice analogy in Figure 7.15. FIGURE 7.15: Type I and Type II errors in criminal trials. Thus a Type I error corresponds to incorrectly putting a truly innocent person in jail, whereas a Type II error corresponds to letting a truly guilty person go free. Let’s show the corresponding table in Figure 7.16 for hypothesis tests. FIGURE 7.16: Type I and Type II errors in hypothesis tests. 7.4.3 How do we choose alpha? If we are using a sample to make inferences about a population, we run the risk of making errors. For confidence intervals, a corresponding “error” would be constructing a confidence interval that does not contain the true value of the population parameter. For hypothesis tests, this would be making either a Type I or Type II error. Obviously, we want to minimize the probability of either error; we want a small probability of making an incorrect conclusion: The probability of a Type I Error occurring is denoted by \\(\\alpha\\). The value of \\(\\alpha\\) is called the significance level of the hypothesis test, which we defined in Section 7.2. The probability of a Type II Error is denoted by \\(\\beta\\). The value of \\(1-\\beta\\) is known as the power of the hypothesis test. In other words, \\(\\alpha\\) corresponds to the probability of incorrectly rejecting \\(H_0\\) when in fact \\(H_0\\) is true. On the other hand, \\(\\beta\\) corresponds to the probability of incorrectly failing to reject \\(H_0\\) when in fact \\(H_0\\) is false. Ideally, we want \\(\\alpha = 0\\) and \\(\\beta = 0\\), meaning that the chance of making either error is 0. However, this can never be the case in any situation where we are sampling for inference. There will always be the possibility of making either error when we use sample data. Furthermore, these two error probabilities are inversely related. As the probability of a Type I error goes down, the probability of a Type II error goes up. What is typically done in practice is to fix the probability of a Type I error by pre-specifying a significance level \\(\\alpha\\) and then try to minimize \\(\\beta\\). In other words, we will tolerate a certain fraction of incorrect rejections of the null hypothesis \\(H_0\\), and then try to minimize the fraction of incorrect non-rejections of \\(H_0\\). So for example if we used \\(\\alpha\\) = 0.01, we would be using a hypothesis testing procedure that in the long run would incorrectly reject the null hypothesis \\(H_0\\) one percent of the time. This is analogous to setting the confidence level of a confidence interval. So what value should you use for \\(\\alpha\\)? Different fields have different conventions, but some commonly used values include 0.10, 0.05, 0.01, and 0.001. However, it is important to keep in mind that if you use a relatively small value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have a harder time being less than \\(\\alpha\\). Thus we would reject the null hypothesis less often. In other words, we would reject the null hypothesis \\(H_0\\) only if we have very strong evidence to do so. This is known as a “conservative” test. On the other hand, if we used a relatively large value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have an easier time being less than \\(\\alpha\\). Thus we would reject the null hypothesis more often. In other words, we would reject the null hypothesis \\(H_0\\) even if we only have mild evidence to do so. This is known as a “liberal” test. Learning check (LC7.5) What is wrong about saying, “The defendant is innocent.” based on the US system of criminal trials? (LC7.6) What is the purpose of hypothesis testing? (LC7.7) What are some flaws with hypothesis testing? How could we alleviate them? (LC7.8) Consider two \\(\\alpha\\) significance levels of 0.1 and 0.01. Of the two, which would lead to a more liberal hypothesis testing procedure? In other words, one that will, all things being equal, lead to more rejections of the null hypothesis \\(H_0\\). 7.5 Case study: Do living lizards have longer horns than dead lizards? Let’s apply our knowledge of hypothesis testing to answer the question: “Are lizards with longer horns more likely to survive?”. Horned lizards possess an elaborate crown of bony horns that were presumed to provide protection against predators. To test this hypothesis, a team of herpetologists examined flat-tailed horned lizards (Phrynosoma mcalli), which are often eaten by loggerhead shrikes (such a pretty bird – and yet quite lethal!). The researchers examined the length of horns of both living and shrike-killed horned lizards to see if there was an association between horn length and predation status. 7.5.1 Horned lizards dataset The HornedLizards dataset in the abd package contains information on 184 horned lizards. The group variable indicates the predation status (living or killed) and the horn.length variable indicates the squamosal horn length (in mm) of each lizard. We are interested in whether living horned lizards have a greater horn.length on average than killed ones. HornedLizards %&gt;% tibble() # A tibble: 185 × 2 horn.length group &lt;dbl&gt; &lt;fct&gt; 1 25.2 living 2 26.9 living 3 26.6 living 4 25.6 living 5 25.7 living 6 25.9 living 7 27.3 living 8 25.1 living 9 30.3 living 10 25.6 living # … with 175 more rows Let’s perform an exploratory data analysis of this data. Recall from Subsection 2.7.1 that a boxplot is a visualization we can use to show the relationship between a numerical and a categorical variable. Another option you saw in Section 2.5 would be to use a faceted histogram. However, in the interest of brevity, let’s only present the boxplot in Figure 7.17. ggplot(data = HornedLizards, aes(x = group, y = horn.length)) + geom_boxplot() + labs(y = &quot;Squamosal horn length&quot;) FIGURE 7.17: Boxplot of squamosal horn length vs. group. Eyeballing Figure 7.17, living horned lizards have a greater median horn length than killed. Do we have reason to believe, however, that there is a significant difference between the mean horn.length for killed horned lizards compared to living horned lizards? Although the boxplot does show that the median sample horn length is higher for living horned lizards, it’s hard to conclude if this difference is statistically significant or due to random chance in the sample of horned lizards studied. After all, there is a fair amount of overlap between the boxes. Recall that the median isn’t necessarily the same as the mean either, depending on whether the distribution is skewed. Did you notice the warning above that one row contains “non-finite” values? Apparently one row has missing values – which explains why the dataset has 185 rows instead of the expected 184 observations. Before we proceed further, let’s use tidyr::drop_na to drop the row with missing data to potentially avoid trouble later on. HornedLizards &lt;- HornedLizards %&gt;% tidyr::drop_na() Let’s calculate some summary statistics – the number of horned lizards, the mean horn length, and the standard deviation – split by the binary categorical variable group. We’ll do this using dplyr data wrangling verbs. Notice in particular how we count the number of each type of lizard using the n() summary function. HornedLizards %&gt;% group_by(group) %&gt;% summarize(n = n(), mean_length = mean(horn.length), std_dev = sd(horn.length)) # A tibble: 2 × 4 group n mean_length std_dev &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 killed 30 21.9867 2.70946 2 living 154 24.2812 2.63078 Observe that we have 154 living HornedLizards with an average horn.length of 24.281 mm and 30 killed HornedLizards with an average horn.length of 21.987 mm. The difference in these average ratings is thus 24.281 - 21.987 = 2.295. So there appears to be an edge of 2.295 mm in favor of living horned lizards. The question is, however, are these results indicative of a true difference for all living and killed horned lizards? Or could we attribute this difference to chance sampling variation? 7.5.2 Sampling scenario Let’s now revisit this study in terms of terminology and notation related to sampling we studied in Subsection 5.3.1. The study population is all horned lizards, either living or killed. The sample from this population is the 184 HornedLizards included in the HornedLizards dataset. Since this sample was randomly taken from the population HornedLizards, it is presumably representative of all living and killed horned lizards. Thus, any analysis and results based on HornedLizards can generalize to the entire population. What are the relevant population parameter and point estimates? We introduce the fourth sampling scenario in Table 7.3. TABLE 7.3: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) So, whereas the sampling bowl exercise in Section 5.1 concerned proportions, the pennies exercise in Section 6.1 concerned means, the case study on whether yawning is contagious in Section 6.6 and the GreatTitMalaria activity in Section 7.1 concerned differences in proportions, we are now concerned with differences in means. In other words, the population parameter of interest is the difference in population mean ratings \\(\\mu_k - \\mu_l\\), where \\(\\mu_k\\) is the mean horn length of all killed horned lizards and similarly \\(\\mu_l\\) is the mean horn length of all living horned lizards. Additionally the point estimate/sample statistic of interest is the difference in sample means \\(\\overline{x}_k - \\overline{x}_l\\), where \\(\\overline{x}_k\\) is the mean horn.length of the \\(n_k\\) = 30 horned lizards in our sample and \\(\\overline{x}_l\\) is the mean horn length of the \\(n_l\\) = 154 in our sample. Based on our earlier exploratory data analysis, our estimate \\(\\overline{x}_k - \\overline{x}_l\\) is \\(21.987 - 24.281 = -2.295\\). So there appears to be a difference of -2.295 in favor of living horned lizards. The question is, however, could this difference of -2.295 be merely due to chance and sampling variation? Or are these results indicative of a true difference in mean ratings for all living and killed horned lizards? To answer this question, we’ll use hypothesis testing. 7.5.3 Conducting the hypothesis test We’ll be testing: \\[ \\begin{aligned} H_0 &amp;: \\mu_k - \\mu_l = 0\\\\ \\text{vs } H_A&amp;: \\mu_k - \\mu_l \\neq 0 \\end{aligned} \\] In other words, the null hypothesis \\(H_0\\) suggests that both living and killed horned lizards have the same mean horn.length. This is the “hypothesized universe” we’ll assume is true. On the other hand, the alternative hypothesis \\(H_A\\) suggests that there is a difference. Unlike the one-sided alternative we used in the GreatTitMalaria exercise \\(H_A: p_e - p_c &gt; 0\\), we are now considering a two-sided alternative of \\(H_A: \\mu_k - \\mu_l \\neq 0\\). Furthermore, we’ll pre-specify a low significance level of \\(\\alpha\\) = 0.01. By setting this value low, all things being equal, there is a lower chance that the \\(p\\)-value will be less than \\(\\alpha\\). Thus, there is a lower chance that we’ll reject the null hypothesis \\(H_0\\) in favor of the alternative hypothesis \\(H_A\\). In other words, we’ll reject the hypothesis that there is no difference in mean ratings for all killed and living horned lizards, only if we have quite strong evidence. 1. specify variables Let’s now perform all the steps of the infer workflow. We first specify() the variables of interest in the HornedLizards data frame using the formula horn.length ~ group. This tells infer that the numerical variable horn.length is the outcome variable, while the binary variable group is the explanatory variable. Note that unlike previously when we were interested in proportions, since we are now interested in the mean of a numerical variable, we do not need to set the success argument. HornedLizards %&gt;% specify(formula = horn.length ~ group) Response: horn.length (numeric) Explanatory: group (factor) # A tibble: 184 × 2 horn.length group &lt;dbl&gt; &lt;fct&gt; 1 25.2 living 2 26.9 living 3 26.6 living 4 25.6 living 5 25.7 living 6 25.9 living 7 27.3 living 8 25.1 living 9 30.3 living 10 25.6 living # … with 174 more rows Observe at this point that the data in HornedLizards has not changed. The only change so far is the newly defined Response: horn.length (numeric) and Explanatory: group (factor) meta-data. 2. hypothesize the null We set the null hypothesis \\(H_0: \\mu_k - \\mu_l = 0\\) by using the hypothesize() function. Since we have two samples, killed and living horned lizards, we set null to be \"independence\" as we described in Section 7.3. HornedLizards %&gt;% specify(formula = horn.length ~ group) %&gt;% hypothesize(null = &quot;independence&quot;) Response: horn.length (numeric) Explanatory: group (factor) Null Hypothesis: independence # A tibble: 184 × 2 horn.length group &lt;dbl&gt; &lt;fct&gt; 1 25.2 living 2 26.9 living 3 26.6 living 4 25.6 living 5 25.7 living 6 25.9 living 7 27.3 living 8 25.1 living 9 30.3 living 10 25.6 living # … with 174 more rows 3. generate replicates After we have set the null hypothesis, we generate “shuffled” replicates assuming the null hypothesis is true by repeating the shuffling/permutation exercise you performed in Section 7.1. We’ll repeat this resampling without replacement of type = \"permute\" a total of reps = 1000 times. Feel free to run the code below to check out what the generate() step produces. HornedLizards %&gt;% specify(formula = horn.length ~ group) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% View() 4. calculate summary statistics Now that we have 1000 replicated “shuffles” assuming the null hypothesis \\(H_0\\) that both killed and living HornedLizards on average have the same horn lengths, let’s calculate() the appropriate summary statistic for these 1000 replicated shuffles. From Section 7.2, summary statistics relating to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the difference in population means \\(\\mu_{k} - \\mu_{l}\\), the test statistic of interest here is the difference in sample means \\(\\overline{x}_{k} - \\overline{x}_{l}\\). For each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"diff in means\". Furthermore, since we are interested in \\(\\overline{x}_{k} - \\overline{x}_{l}\\), we set order = c(\"killed\", \"living\"). Let’s save the results in a data frame called null_distribution_HornedLizards: null_distribution_HornedLizards &lt;- HornedLizards %&gt;% specify(formula = horn.length ~ group) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;killed&quot;, &quot;living&quot;)) null_distribution_HornedLizards # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.545152 2 2 0.700476 3 3 0.489394 4 4 0.0512987 5 5 0.334069 6 6 0.0353680 7 7 -0.494329 8 8 -0.104026 9 9 0.999177 10 10 0.616840 # … with 990 more rows Observe that we have 1000 values of stat, each representing one instance of \\(\\overline{x}_{k} - \\overline{x}_{l}\\). The 1000 values form the null distribution, which is the technical term for the sampling distribution of the difference in sample means \\(\\overline{x}_{k} - \\overline{x}_{l}\\) assuming \\(H_0\\) is true. What happened in real life? What was the observed difference in horn lengths? What was the observed test statistic \\(\\overline{x}_{k} - \\overline{x}_{l}\\)? Recall from our earlier data wrangling, this observed difference in means was \\(21.987 - 24.281 = -2.295\\). We can also achieve this using the code that constructed the null distribution null_distribution_HornedLizards but with the hypothesize() and generate() steps removed. Let’s save this in obs_diff_means: obs_diff_means &lt;- HornedLizards %&gt;% specify(formula = horn.length ~ group) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;killed&quot;, &quot;living&quot;)) obs_diff_means Response: horn.length (numeric) Explanatory: group (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -2.29450 5. visualize the p-value Lastly, in order to compute the \\(p\\)-value, we have to assess how “extreme” the observed difference in means of -2.295 is. We do this by comparing -2.295 to our null distribution, which was constructed in a hypothesized universe of no true difference in average horn lengths between the two groups. Let’s visualize both the null distribution and the \\(p\\)-value in Figure 7.18. Unlike our example in Subsection 7.3.1 involving GreatTitMalaria, since we have a two-sided \\(H_A: \\mu_k - \\mu_l \\neq 0\\), we have to allow for both possibilities for more extreme, so we set direction = \"both\". visualize(null_distribution_HornedLizards, bins = 10) + shade_p_value(obs_stat = obs_diff_means, direction = &quot;both&quot;) FIGURE 7.18: Null distribution, observed test statistic, and \\(p\\)-value. Let’s go over the elements of this plot. First, the histogram is the null distribution. Second, the solid line is the observed test statistic, or the difference in sample means we observed in real life of \\(21.987 - 24.281 = -2.295\\). Third, the two shaded areas of the histogram form the \\(p\\)-value, or the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. What proportion of the null distribution is shaded? In other words, what is the numerical value of the \\(p\\)-value? We use the get_p_value() function to compute this value: null_distribution_HornedLizards %&gt;% get_p_value(obs_stat = obs_diff_means, direction = &quot;both&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0 This \\(p\\)-value of 0 is extremely small. In other words, there is an extremely small chance that we’d observe a difference of 21.987 - 24.281 = -2.295 in a hypothesized universe where there was truly no difference in ratings. In particular, this \\(p\\)-value is less than our pre-specified \\(\\alpha\\) significance level of 0.01. Thus, we are inclined to reject the null hypothesis \\(H_0: \\mu_k - \\mu_l = 0\\). In non-statistical language, the conclusion is: we have the evidence needed in this sample of data to suggest that we should reject the hypothesis that there is no difference in mean horn lengths between living and killed horned lizards. We, thus, can say that a difference exists in living and killed horned lizards, on average, for all horned lizards. Learning check (LC7.9) Conduct the same analysis comparing killed horned lizards versus living HornedLizards using the median horn.length instead of the mean horn.length. What was different and what was the same? (LC7.10) What conclusions, if any, can you make from viewing the faceted histogram looking at horn.length versus group that you couldn’t see when looking at the boxplot? (LC7.11) Describe in a paragraph how we used Allen Downey’s diagram to conclude if a statistical difference existed between mean horned lizards for killed and living horned lizards. (LC7.12) Why are we relatively confident that the distributions of the sample horn lengths will be good approximations of the population distributions of horn lengths for the two groups? (LC7.13) Using the definition of \\(p\\)-value, write in words what the \\(p\\)-value represents for the hypothesis test comparing the mean horn length of living to killed horned lizards. (LC7.14) What is the value of the \\(p\\)-value for the hypothesis test comparing the mean horn length of living to killed horned lizards? 7.6 Theory-based hypothesis tests Much as we did in Subsection 6.8 when we showed you a theory-based method for constructing confidence intervals that involved mathematical formulas, we now present an example of a traditional theory-based method to conduct hypothesis tests. This method relies on probability models, probability distributions, and a few assumptions to construct the null distribution. This is in contrast to the approach we’ve been using throughout this book where we relied on computer simulations to construct the null distribution. These traditional theory-based methods have been used for decades mostly because researchers didn’t have access to computers that could run thousands of calculations quickly and efficiently. Now that computing power is much cheaper and more accessible, simulation-based methods are much more feasible. However, researchers in many fields continue to use theory-based methods. Hence, we make it a point to include an example here. As we’ll show in this section, any theory-based method is ultimately an approximation to the simulation-based method. The theory-based method we’ll focus on is known as the two-sample \\(t\\)-test for testing differences in sample means. However, the test statistic we’ll use won’t be the difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\), but rather the related two-sample \\(t\\)-statistic. The data we’ll use will once again be the HornedLizards data of killed and living horned lizards from Section 7.5. Two-sample t-statistic A common task in statistics is the process of “standardizing a variable.” By standardizing different variables, we make them more comparable. For example, say you are interested in studying the distribution of temperature recordings from Portland, Oregon, USA and comparing it to that of the temperature recordings in Montreal, Quebec, Canada. Given that US temperatures are generally recorded in degrees Fahrenheit and Canadian temperatures are generally recorded in degrees Celsius, how can we make them comparable? One approach would be to convert degrees Fahrenheit into Celsius, or vice versa. Another approach would be to convert them both to a common “standardized” scale, like Kelvin units of temperature. One common method for standardizing a variable from probability and statistics theory is to compute the \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\] where \\(x\\) represents one value of a variable, \\(\\mu\\) represents the mean of that variable, and \\(\\sigma\\) represents the standard deviation of that variable. You first subtract the mean \\(\\mu\\) from each value of \\(x\\) and then divide \\(x - \\mu\\) by the standard deviation \\(\\sigma\\). These operations will have the effect of re-centering your variable around 0 and re-scaling your variable \\(x\\) so that they have what are known as “standard units.” Thus for every value that your variable can take, it has a corresponding \\(z\\)-score that gives how many standard units away that value is from the mean \\(\\mu\\). \\(z\\)-scores are normally distributed with mean 0 and standard deviation 1. This curve is called a “\\(z\\)-distribution” or “standard normal” curve and has the common, bell-shaped pattern from Figure 7.19 discussed in Appendix A.2. FIGURE 7.19: Standard normal z curve. Bringing these back to the difference of sample mean horn lengths \\(\\overline{x}_k - \\overline{x}_l\\) of killed versus living horned lizards, how would we standardize this variable? By once again subtracting its mean and dividing by its standard deviation. Recall two facts from Subsection 5.3.3. First, if the sampling was done in a representative fashion, then the sampling distribution of \\(\\overline{x}_k - \\overline{x}_l\\) will be centered at the true population parameter \\(\\mu_k - \\mu_l\\). Second, the standard deviation of point estimates like \\(\\overline{x}_k - \\overline{x}_l\\) has a special name: the standard error. Applying these ideas, we present the two-sample \\(t\\)-statistic: \\[t = \\dfrac{ (\\bar{x}_k - \\bar{x}_l) - (\\mu_k - \\mu_l)}{ \\text{SE}_{\\bar{x}_k - \\bar{x}_l} } = \\dfrac{ (\\bar{x}_k - \\bar{x}_l) - (\\mu_k - \\mu_l)}{ \\sqrt{\\dfrac{{s_k}^2}{n_k} + \\dfrac{{s_l}^2}{n_l}} }\\] Oofda! There is a lot to try to unpack here! Let’s go slowly. In the numerator, \\(\\bar{x}_k-\\bar{x}_l\\) is the difference in sample means, while \\(\\mu_k - \\mu_l\\) is the difference in population means. In the denominator, \\(s_k\\) and \\(s_l\\) are the sample standard deviations of the killed and living horned lizards in our sample HornedLizards. Lastly, \\(n_k\\) and \\(n_l\\) are the sample sizes of the killed and living horned lizards. Putting this together under the square root gives us the standard error \\(\\text{SE}_{\\bar{x}_k - \\bar{x}_l}\\). Observe that the formula for \\(\\text{SE}_{\\bar{x}_k - \\bar{x}_l}\\) has the sample sizes \\(n_k\\) and \\(n_l\\) in them. So as the sample sizes increase, the standard error goes down. We’ve seen this concept numerous times now, in particular in our simulations using the three virtual shovels with \\(n\\) = 25, 50, and 100 slots in Figure 5.16 and in Subsection 6.5.3 where we studied the effect of using larger sample sizes on the widths of confidence intervals. So how can we use the two-sample \\(t\\)-statistic as a test statistic in our hypothesis test? First, assuming the null hypothesis \\(H_0: \\mu_k - \\mu_l = 0\\) is true, the right-hand side of the numerator (to the right of the \\(-\\) sign), \\(\\mu_k - \\mu_l\\), becomes 0. Second, similarly to how the Central Limit Theorem from Subsection 5.5 states that sample means follow a normal distribution, it can be mathematically proven that the two-sample \\(t\\)-statistic follows a \\(t\\) distribution with degrees of freedom “roughly equal” to \\(df = n_k + n_l - 2\\). To better understand this concept of degrees of freedom, we next display three examples of \\(t\\)-distributions in Figure 7.20 along with the standard normal \\(z\\) curve. FIGURE 7.20: Examples of t-distributions and the z curve. Begin by looking at the center of the plot at 0 on the horizontal axis. As you move up from the value of 0, follow along with the labels and note that the bottom curve corresponds to 1 degree of freedom, the curve above it is for 3 degrees of freedom, the curve above that is for 10 degrees of freedom, and lastly the dotted curve is the standard normal \\(z\\) curve. Observe that all four curves have a bell shape, are centered at 0, and that as the degrees of freedom increase, the \\(t\\)-distribution more and more resembles the standard normal \\(z\\) curve. The “degrees of freedom” measures how different the \\(t\\) distribution will be from a normal distribution. \\(t\\)-distributions tend to have more values in the tails of their distributions than the standard normal \\(z\\) curve. This “roughly equal” statement indicates that the equation \\(df = n_k + n_l - 2\\) is a “good enough” approximation to the true degrees of freedom (although the approximation becomes weaker when the sample sizes differ significantly, as in this study). The true formula is a bit more complicated than this simple expression, but we’ve found the formula to be beyond the reach of those new to statistical inference and it does little to build the intuition of the \\(t\\)-test. The message to retain, however, is that small sample sizes lead to small degrees of freedom and thus small sample sizes lead to \\(t\\)-distributions that are different than the \\(z\\) curve. On the other hand, large sample sizes correspond to large degrees of freedom and thus produce \\(t\\) distributions that closely align with the standard normal \\(z\\)-curve. So, assuming the null hypothesis \\(H_0\\) is true, our formula for the test statistic simplifies a bit: \\[t = \\dfrac{ (\\bar{x}_k - \\bar{x}_l) - 0}{ \\sqrt{\\dfrac{{s_k}^2}{n_k} + \\dfrac{{s_l}^2}{n_l}} } = \\dfrac{ \\bar{x}_k - \\bar{x}_l}{ \\sqrt{\\dfrac{{s_k}^2}{n_k} + \\dfrac{{s_l}^2}{n_l}} }\\] Let’s compute the values necessary for this two-sample \\(t\\)-statistic. Recall the summary statistics we computed during our exploratory data analysis in Section 7.5.1. HornedLizards %&gt;% group_by(group) %&gt;% summarize(n = n(), mean_rating = mean(horn.length), std_dev = sd(horn.length)) # A tibble: 2 × 4 group n mean_rating std_dev &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 killed 30 21.9867 2.70946 2 living 154 24.2812 2.63078 Using these values, the observed two-sample \\(t\\)-test statistic is \\[ \\dfrac{ \\bar{x}_k - \\bar{x}_l}{ \\sqrt{\\dfrac{{s_k}^2}{n_k} + \\dfrac{{s_l}^2}{n_l}} } = \\dfrac{21.987 - 24.281}{ \\sqrt{\\dfrac{{2.709}^2}{30} + \\dfrac{{2.631}^2}{154}} } = -4.263 \\] Great! How can we compute the \\(p\\)-value using this theory-based test statistic? We need to compare it to a null distribution, which we construct next. Null distribution Let’s revisit the null distribution for the test statistic \\(\\bar{x}_k - \\bar{x}_l\\) we constructed in Section 7.5. Let’s visualize this in the left-hand plot of Figure 7.21. # Construct null distribution of xbar_a - xbar_r: null_distribution_HornedLizards &lt;- HornedLizards %&gt;% specify(formula = horn.length ~ group) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;killed&quot;, &quot;living&quot;)) visualize(null_distribution_HornedLizards, bins = 10) The infer package also includes some built-in theory-based test statistics as well. So instead of calculating the test statistic of interest as the \"diff in means\" \\(\\bar{x}_k - \\bar{x}_l\\), we can calculate this defined two-sample \\(t\\)-statistic by setting stat = \"t\". Let’s visualize this in the right-hand plot of Figure 7.21. # Construct null distribution of t: null_distribution_HornedLizards_t &lt;- HornedLizards %&gt;% specify(formula = horn.length ~ group) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% # Notice we switched stat from &quot;diff in means&quot; to &quot;t&quot; calculate(stat = &quot;t&quot;, order = c(&quot;killed&quot;, &quot;living&quot;)) visualize(null_distribution_HornedLizards_t, bins = 10) FIGURE 7.21: Comparing the null distributions of two test statistics. Observe that while the shape of the null distributions of both the difference in means \\(\\bar{x}_k - \\bar{x}_l\\) and the two-sample \\(t\\)-statistics are similar, the scales on the x-axis are different. The two-sample \\(t\\)-statistic values are spread out over a larger range. However, a traditional theory-based \\(t\\)-test doesn’t look at the simulated histogram in null_distribution_HornedLizards_t, but instead it looks at the \\(t\\)-distribution curve with the appropriate degrees of freedom equal. Since the samples sizes here differ significantly, the degrees of freedom was calculated using the complicated formula referenced previously, and hence is significantly different from the “close enough approximation” that works when the sample sizes are equal. Let’s overlay this \\(t\\)-distribution curve over the top of our simulated two-sample \\(t\\)-statistics using the method = \"both\" argument in visualize(). visualize(null_distribution_HornedLizards_t, bins = 10, method = &quot;both&quot;) FIGURE 7.22: Null distribution using t-statistic and t-distribution. Observe that the curve does a good job of approximating the histogram here. To calculate the \\(p\\)-value in this case, we need to figure out how much of the total area under the \\(t\\)-distribution curve is at or “more extreme” than our observed two-sample \\(t\\)-statistic. Since \\(H_A: \\mu_k - \\mu_l \\neq 0\\) is a two-sided alternative, we need to add up the areas in both tails. We first compute the observed two-sample \\(t\\)-statistic using infer verbs. This shortcut calculation further assumes that the null hypothesis is true: that the population of killed and living horned lizards have an equal average horn.length. obs_two_sample_t &lt;- HornedLizards %&gt;% specify(formula = horn.length ~ group) %&gt;% calculate(stat = &quot;t&quot;, order = c(&quot;killed&quot;, &quot;living&quot;)) obs_two_sample_t Response: horn.length (numeric) Explanatory: group (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -4.26337 We want to find the percentage of values that are at or below obs_two_sample_t \\(= -4.263\\) or at or above -obs_two_sample_t \\(= 4.263\\). We use the shade_p_value() function with the direction argument set to \"both\" to do this: visualize(null_distribution_HornedLizards_t, method = &quot;both&quot;) + shade_p_value(obs_stat = obs_two_sample_t, direction = &quot;both&quot;) Warning: Check to make sure the conditions have been met for the theoretical method. {infer} currently does not check these for you. FIGURE 7.23: Null distribution using t-statistic and t-distribution with \\(p\\)-value shaded. (We’ll discuss this warning message shortly.) What is the \\(p\\)-value? We apply get_p_value() to our null distribution saved in null_distribution_HornedLizards_t: null_distribution_HornedLizards_t %&gt;% get_p_value(obs_stat = obs_two_sample_t, direction = &quot;both&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0 We have a very small \\(p\\)-value, and thus it is very unlikely that these results are due to sampling variation. Thus, we are inclined to reject \\(H_0\\). Let’s come back to that earlier warning message: Check to make sure the conditions have been met for the theoretical method. {infer} currently does not check these for you. To be able to use the \\(t\\)-test and other such theoretical methods, there are always a few conditions to check. The infer package does not automatically check these conditions, hence the warning message we received. These conditions are necessary so that the underlying mathematical theory holds. In order for the results of our two-sample \\(t\\)-test to be valid, three conditions must be met: Nearly normal populations or large sample sizes. A general rule of thumb that works in many (but not all) situations is that the sample size \\(n\\) should be greater than 30. Both samples are selected independently of each other. All observations are independent from each other. Let’s see if these conditions hold for our HornedLizards data: This is met since \\(n_k\\) = 30 and \\(n_l\\) = 154 are both 30 or larger, satisfying our rule of thumb. This is presumably met since we sampled the killed and living horned lizards at random and in an unbiased fashion from the populatiion of all horned lizards. Unfortunately, we can’t be sure that observations are completely independent of each other. For example, if living horned lizards were all found at one location, then those observations might be related and hence not independent. Assuming all three conditions are roughly met, we can be reasonably certain that the theory-based \\(t\\)-test results are valid. If any of the conditions were clearly not met, we couldn’t put as much trust into any conclusions reached. On the other hand, in most scenarios, the only assumption that needs to be met in the simulation-based method is that the sample is selected at random. Thus, in our experience, we prefer simulation-based methods as they have fewer assumptions, are conceptually easier to understand, and since computing power has recently become easily accessible, they can be run quickly. That being said since much of the world’s research still relies on traditional theory-based methods, we also believe it is important to understand them. You may be wondering why we chose reps = 1000 for these simulation-based methods. We’ve noticed that after around 1000 replicates for the null distribution and the bootstrap distribution for most problems you can start to get a general sense for how the statistic behaves. You can change this value to something like 10,000 though for reps if you would like even finer detail but this will take more time to compute. Feel free to iterate on this as you like to get an even better idea about the shape of the null and bootstrap distributions as you wish. 7.7 Problems with p-values On top of the many common misunderstandings about hypothesis testing and \\(p\\)-values we listed in Section 7.4, another unfortunate consequence of the expanded use of \\(p\\)-values and hypothesis testing is a phenomenon known as “p-hacking.” p-hacking is the act of “cherry-picking” only results that are “statistically significant” while dismissing those that aren’t, even if at the expense of the scientific ideas. There are lots of articles written recently about misunderstandings and the problems with \\(p\\)-values. We encourage you to check some of them out: Misunderstandings of \\(p\\)-values What a nerdy debate about \\(p\\)-values shows about science - and how to fix it Statisticians issue warning over misuse of \\(P\\) values You Can’t Trust What You Read About Nutrition A Litany of Problems with p-values Such issues were getting so problematic that the American Statistical Association (ASA) put out a statement in 2016 titled, “The ASA Statement on Statistical Significance and \\(P\\)-Values,” with six principles underlying the proper use and interpretation of \\(p\\)-values. The ASA released this guidance on \\(p\\)-values to improve the conduct and interpretation of quantitative science and to inform the growing emphasis on reproducibility of science research. We as authors much prefer the use of confidence intervals for statistical inference, since in our opinion they are much less prone to large misinterpretation. However, many fields still exclusively use \\(p\\)-values for statistical inference and this is one reason for including them in this text. We encourage you to learn more about “p-hacking” as well and its implication for science. 7.8 Conclusion Chapter Learning Summary Hypothesis testing is used to determine how likely an observed difference of a test statistic between two groups is due to chance. A statistical hypothesis states the expected values of an unknown population parameter. The null hypothesis supposes that there is no difference of interest; the alternative hypothesis supposes that there is a difference of interest. The null distribution is the sampling distribution of a test statistic in a hypothesized universe where there is no difference between the two groups. The null distribution of a test statistic can be calculated using computer simulations or theories based on mathematical formulas. Permutations (shuffling) are a form of resampling without replacement. Hypothesis testing compares the observed test statistic to its null distribution to determine the likelihood of the observed value (or more extreme values) assuming the null hypothesis is true. If the p-value is below a pre-selected threshold, called the significance level or alpha, then the null hypothesis is rejected; otherwise, it cannot be rejected (which doesn’t mean that it’s accepted). Computer-based methods of hypothesis testing with different test statistics use one consistent framework and require fewer specific conditions compared to theory-based methods. A Type I error (false positive) occurs when the null hypothesis is rejected although it is actually true; the probability of a type I error is denoted by alpha. A type II error (false negative) occurs when the null hypothesis is not rejected although it is not true; the probability of a type II error is denoted by beta. Power, 1 - beta, is the probability of rejecting the null hypothesis when it is not true (true positive). The theory-based method of hypothesis testing uses probability models, probability distributions and some assumptions to construct the null distribution. Compared to computer based methods, theory-based methods require that additional conditions be met to trust the conclusions. While p-values are commonly used as evidence for scientific conclusions, they can be easily misunderstood or even misused; in contrast, confidence intervals better indicate the uncertainty associated with a sample statistic. 7.8.1 Additional resources For more examples of the infer workflow for conducting hypothesis tests, we suggest you check out the infer package homepage, in particular, a series of example analyses available at https://infer.netlify.app/articles/. (LC7.15) In Section 7.1, you used “diff in props” to conduct a hypothesis test comparing two proportions (malaria rates in control and experimental birds). Referring to the “Full infer Pipeline Examples” article, what other test statistics can be used to compare two proportions? (Hint: Click “Hypothesis tests” in the right side bar and then select from the Contents listed.) (LC7.16) The case study in Section 5.4 examined if a sample of Drosophila fruit flies produced the expected Mendelian proportions of 3/4 dominant phenotype to 1/4 recessive phenotype. Referring to the “Full infer Pipeline Examples” article, what test statistics can be used to compare the observed phenotypic proportions to the expected? (LC7.17) The case study in Section 7.5 asked if living lizards have longer horns than dead lizards. Referring to the “Full infer Pipeline Examples” article, what other test statistic, besides “diff in means” and “t”, could be used to compare horn length in the two groups? (LC7.18) The case study in Section 7.5 compared the means of two populations. Referring to the “Full infer Pipeline Examples” article, what test statistic can be used to compare the means of more than two groups? 7.8.2 What’s to come We conclude with the infer pipeline for hypothesis testing in Figure 7.24. FIGURE 7.24: infer package workflow for hypothesis testing. We’ll now move to the “Data modeling with moderndive” portion of this book in Chapters 8 and 10, where you’ll leverage your data visualization and wrangling skills to model relationships between different variables in data frames. Then, you’ll use your understanding of confidence intervals from Chapter 6 and hypothesis tests from this chapter, to explore inference for regression in Chapter 9. Onwards and upwards into Data Modeling as shown in Figure 7.25! ModernDive for Life Scientists flowchart - on to Part II! ModernDive for Life Scientists flowchart - on to Part III! FIGURE 7.25: ModernDive for Life Scientists flowchart - on to Part II! "],["8-regression.html", "Chapter 8 Basic Regression 8.1 One numerical explanatory variable 8.2 One categorical explanatory variable 8.3 Related topics 8.4 Conclusion", " Chapter 8 Basic Regression The fundamental premise of data modeling is to make explicit the relationship between an response variable \\(y\\), also called a dependent variable or outcome variable, and * an explanatory variable \\(x\\), also called an independent variable, predictor variable, or covariate. Another way to state this is using mathematical terminology: we will model the outcome variable \\(y\\) “as a function” of the explanatory/predictor variable \\(x\\). When we say “function” here, we are referring to a mathematical function (not functions in R like ggplot()). While there exist many techniques for modeling, such as tree-based models and neural networks, in this book we’ll focus on one particular technique: linear regression. Linear regression is one of the most commonly-used and easy-to-understand approaches to modeling. Linear regression involves a numerical outcome variable \\(y\\) and explanatory variables \\(x\\) that are either numerical or categorical. Furthermore, the relationship between \\(y\\) and \\(x\\) is assumed to be linear, or in other words, a line. However, we’ll see that what constitutes a “line” will vary depending on the nature of your explanatory variables \\(x\\). In Chapter 8 on basic regression, we’ll only consider models with a single explanatory variable \\(x\\). In Section 8.1, the explanatory variable will be numerical. This scenario is known as simple linear regression. In Section 8.2, the explanatory variable will be categorical. In Chapter 9 on inference for regression, we’ll revisit our regression models and analyze the results using the tools for statistical inference developed in Chapters 5, 6, and 7 on sampling, bootstrapping and confidence intervals, and hypothesis testing and \\(p\\)-values, respectively. In Chapter 10 on multiple regression, we’ll extend the ideas behind basic regression and consider models with two explanatory variables \\(x_1\\) and \\(x_2\\). In Section 10.1, we’ll have one numerical and one categorical variable. In Section 10.2, we’ll have two categorical explanatory variables. In particular, we’ll consider two such models: interaction and parallel slopes models. Modeling for explanation or prediction Why do we have two different labels, explanatory and predictor, for the variable \\(x\\)? That’s because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes: Modeling for explanation: When you want to explicitly describe and quantify the relationship between the outcome variable \\(y\\) and a set of explanatory variables \\(x\\), determine the significance of any relationships, have measures summarizing these relationships, and possibly identify any causal relationships between the variables. Modeling for prediction: When you want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(x\\). Unlike modeling for explanation, however, you don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \\(y\\) using the information in \\(x\\). For example, say you are interested in an outcome variable \\(y\\) of whether patients develop lung cancer and information \\(x\\) on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as targeting smokers of a specific age group with advertising for smoking cessation programs. If we are modeling for prediction, however, we wouldn’t care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer. We’ll generally focus on modeling for explanation and hence refer to \\(x\\) as explanatory variables. If you are interested in learning about modeling for prediction, we suggest you check out books and courses on the field of machine learning such as An Introduction to Statistical Learning with Applications in R (ISLR) (James et al. 2017). Chapter Learning Objectives At the end of this chapter, you should be able to… • Calculate the correlation coefficient to estimate the strength of association between two variables. • Interpret a regression table to estimate the intercept and slope of a simple regression line. • Compute the fitted values and residuals for observed values. • Interpret a regression table to estimate the baseline and offset values of linear regression with a categorical explanatory variable. • Explain how correlation can occur without causation. Needed packages Let’s now load all the packages needed for this chapter (this assumes you’ve already installed them). In this chapter, we introduce the moderndive package of datasets and functions for tidyverse-friendly introductory linear regression. If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(skimr) library(abd) library(gapminder) Let’s now begin with basic regression, which refers to linear regression models with a single explanatory variable \\(x\\). We’ll also discuss important statistical concepts like the correlation coefficient, that “correlation isn’t necessarily causation,” and what it means for a line to be “best-fitting.” 8.1 One numerical explanatory variable Determining the age of lions living in the wild can be difficult to estimate. One hypothesis is that the amount of black pigment in the nose increases with age and that therefore the proportion of black in the nose can be used to estimate a lion’s age. Here we’ll look at the LionNoses data set in the abd package. More information, including a reference to the original study, can be found by typing ?LionNoses in the console to view its help file. Researchers at the University of Minnesota tried to answer the following research question: can the age of male lions be used to explain the proportion of black in the nose or is there no relation? To this end, they used age and nose coloration data from 32 lions. We’ll answer these questions by modeling the relationship between proportion black and age using simple linear regression where we have: A numerical outcome variable \\(y\\) (the relative nose coloration) and A single numerical explanatory variable \\(x\\) (the lion’s age). 8.1.1 Exploratory data analysis The data on the 32 male lions can be found in the LionNoses data frame included in the abd package. Recall that a crucial step before doing any kind of analysis or modeling is performing an exploratory data analysis, or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA: Most crucially, looking at the raw data values. Computing summary statistics, such as means, medians, and interquartile ranges. Creating data visualizations. Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road. You can do this by using RStudio’s spreadsheet viewer or by using the glimpse() function as introduced in Subsection 1.4.3 on exploring data frames: glimpse(LionNoses) Rows: 32 Columns: 2 $ age &lt;dbl&gt; 1.1, 1.5, 1.9, 2.2, 2.6, 3.2, 3.2, 2.9, 2.4, 2.1, 1.9… $ proportion.black &lt;dbl&gt; 0.21, 0.14, 0.11, 0.13, 0.12, 0.13, 0.12, 0.18, 0.23,… Let’s fully describe the variables in LionNoses: proportion.black: A numerical variable of the relative coloration of the nose. This is the outcome variable \\(y\\) of interest. age: A numerical variable of the male lion’s age. This will be the explanatory variable \\(x\\). An alternative way to look at the raw data values is by choosing a random sample of the rows in LionNoses by piping it into the sample_n() function from the dplyr package. Here we set the size argument to be 5, indicating that we want a random sample of 5 rows. We display the results in Table 8.1. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. LionNoses %&gt;% sample_n(size = 5) TABLE 8.1: A random sample of 5 out of the 32 lions age proportion.black 2.6 0.12 1.1 0.21 1.9 0.15 7.1 0.37 3.8 0.42 Now that we’ve looked at the raw values in our LionNoses data frame and got a preliminary sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics for our numerical outcome variable denoted as proportion.black and our numerical explanatory variable age. We can do this by using the dplyr::summarize() function as we saw in Section 3.5, but instead let’s use the convenient skim() function from the skimr package that you saw earlier in Section 3.5. This function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take the LionNoses data frame, with the outcome and explanatory variables proportion.black and age, and pipe them into the skim() function: LionNoses %&gt;% skim() Looking at this output, we can see how the values of both variables distribute. For example, the mean nose coloration was 0.322 black, whereas the mean age was 4.31 years. Furthermore, the middle 50% of nose coloration was between 0.165 and 0.433 (the first and third quartiles), whereas the middle 50% of age falls within 2.18 to 5.85 years. 8.1.1.1 Correlation coefficient The skim() function only returns what are known as univariate summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist bivariate summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the correlation coefficient. Generally speaking, coefficients are quantitative expressions of a specific phenomenon. A correlation coefficient is a quantitative expression of the strength of the linear relationship between two numerical variables. Its value ranges between -1 and 1 where: -1 indicates a perfect negative relationship: As one variable increases, the value of the other variable tends to go down, following a straight line. 0 indicates no relationship: The values of both variables go up/down independently of each other. +1 indicates a perfect positive relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion. Figure 8.1 gives examples of 9 different correlation coefficient values for hypothetical numerical variables \\(x\\) and \\(y\\). For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between \\(x\\) and \\(y\\), but it is not as strong as the negative linear relationship between \\(x\\) and \\(y\\) when the correlation coefficient is -0.9 or -1. FIGURE 8.1: Nine different correlation coefficients. The correlation coefficient can be computed using the get_correlation() function in the moderndive package. In this case, the inputs to the function are the two numerical variables for which we want to calculate the correlation coefficient. We put the name of the outcome variable on the left-hand side of the ~ “tilde” sign, while putting the name of the explanatory variable on the right-hand side. This is known as R’s formula notation. We will use this same “formula” syntax with regression later in this chapter. LionNoses %&gt;% get_correlation(formula = proportion.black ~ age) cor 1 0.79 An alternative way to compute correlation is to use the cor() summary function within a summarize() command: LionNoses %&gt;% summarize(correlation = cor(proportion.black, age)) In our case, the correlation coefficient of 0.79 indicates that the relationship between proportion black and age is “clearly positive.” There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to the extreme values of -1, 0, and 1. To develop your intuition about correlation coefficients, play the “Guess the Correlation” 1980’s style video game called “Guess the Correlation”, at http://guessthecorrelation.com/. FIGURE 8.2: Preview of “Guess the Correlation” game. Let’s now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the proportion.black and age variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using geom_point() and display the result in Figure 8.3. FIGURE 8.3: Lion age and nose coloration. Observe that most ages lie between 1 and 5 years, while most nose coloration proportions lie between 0.1 and 0.3 black. Furthermore, while opinions may vary, it is our opinion that the relationship between proportion black and age is “clearly positive.” This is consistent with our earlier computed correlation coefficient of 0.79. Let’s build on the scatterplot in Figure 8.3 by adding a “best-fitting” line: of all possible lines we can draw on this scatterplot, it is the line that “best” fits through the cloud of points. We do this by adding a new geom_smooth(method = \"lm\", se = FALSE) layer to the ggplot() code that created the scatterplot in Figure 8.3. The method = \"lm\" argument sets the line to be a “linear model.” The se = FALSE argument suppresses standard error uncertainty bars. (We defined the concept of standard error earlier in Subsection 5.3.2.) ggplot(LionNoses, aes(x = age, y = proportion.black)) + geom_point() + labs(x = &quot;Age (years)&quot;, y = &quot;Proportion of black nose&quot;, title = &quot;Scatterplot of relationship of relative coloration and age of male lions.&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 8.4: Regression line. The line in the resulting Figure 8.4 is called a “regression line.” The regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable proportion.black and the explanatory variable age. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of 0.79 suggesting that there is a positive relationship between these two variables: as lions have higher ages, they also have noses with a higher proportion of black. We’ll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value. Furthermore, a regression line is “best-fitting” in that it minimizes some mathematical criteria. We present these mathematical criteria in Subsection 8.3.2, but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable. Learning check (LC8.1) Conduct a new exploratory data analysis with the ProgesteroneExercise data set in the abd package. Remember, this involves three things: Looking at the raw data values. Computing summary statistics. Creating data visualizations. The outcome variable \\(y\\) is ventilation rate and the explanatory variable \\(x\\) is progesterone levels. What can you say about the relationship between progesterone levels and ventilation rate based on this exploration? 8.1.2 Simple linear regression You may recall from secondary/high school algebra that the equation of a line is \\(y = a + b\\cdot x\\). (Note that the \\(\\cdot\\) symbol is equivalent to the \\(\\times\\) “multiply by” mathematical symbol. We’ll use the \\(\\cdot\\) symbol in the rest of this book as it is more succinct.) It is defined by two coefficients \\(a\\) and \\(b\\). The intercept coefficient \\(a\\) is the value of \\(y\\) when \\(x = 0\\). The slope coefficient \\(b\\) for \\(x\\) is the increase in \\(y\\) for every increase of one in \\(x\\). This is also called the “rise over run.” However, when defining a regression line like the regression line in Figure 8.4, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) . The intercept coefficient is \\(b_0\\), so \\(b_0\\) is the value of \\(\\widehat{y}\\) when \\(x = 0\\). The slope coefficient for \\(x\\) is \\(b_1\\), i.e., the increase in \\(\\widehat{y}\\) for every increase of one in \\(x\\). Why do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression to indicate that we have a “fitted value,” or the value of \\(y\\) on the regression line for a given \\(x\\) value. We’ll discuss this more in the upcoming Subsection 8.1.3. We know that the regression line in Figure 8.4 has a positive slope \\(b_1\\) corresponding to our explanatory \\(x\\) variable age. Why? Because as lions tend to have higher ages, so also do they tend to have higher proportion.black noses. However, what is the numerical value of the slope \\(b_1\\)? What about the intercept \\(b_0\\)? Let’s not compute these two values by hand, but rather let’s use a computer! We can obtain the values of the intercept \\(b_0\\) and the slope for age \\(b_1\\) by outputting a linear regression table. This is done in two steps: We first “fit” the linear regression model using the lm() function and save it in lion_model. We get the regression table by applying the get_regression_table() function from the moderndive package to lion_model. # Fit regression model: lion_model &lt;- lm(proportion.black ~ age, data = LionNoses) # Get regression table: get_regression_table(lion_model) TABLE 8.2: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 0.070 0.042 1.66 0.107 -0.016 0.155 age 0.059 0.008 7.05 0.000 0.042 0.076 Let’s first focus on interpreting the regression table output in Table 8.2, and then we’ll later revisit the code that produced it. In the estimate column of Table 8.2 are the intercept \\(b_0\\) = 0.07 and the slope \\(b_1\\) = 0.059 for age. Thus the equation of the regression line in Figure 8.4 follows: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x\\\\ \\widehat{\\text{proportion.black}} &amp;= b_0 + b_{\\text{age}} \\cdot\\text{age}\\\\ &amp;= 0.07 + 0.059\\cdot\\text{age} \\end{aligned} \\] The intercept \\(b_0\\) = 0.07 is the average proportion of black \\(\\widehat{y}\\) = \\(\\widehat{\\text{proportion.black}}\\) for those lions that had an age of 0. Or in graphical terms, it’s where the line intersects the \\(y\\) axis when \\(x\\) = 0. Note, however, that while the intercept of the regression line has a mathematical interpretation, it has no practical interpretation here, since observing an age of 0 is unlikely. Furthermore, looking at the scatterplot with the regression line in Figure 8.4, no lions had an age near 0. Of greater interest is the slope \\(b_1\\) = \\(b_{\\text{age}}\\) for age of 0.059, as this summarizes the relationship between proportion.black and age variables. Note that the sign is positive, suggesting a positive relationship between these two variables, meaning lions with higher black proportions of the nose also tend to have higher ages. Recall from earlier that the correlation coefficient is 0.79. They both have the same positive sign, but have a different value. Recall further that the correlation’s interpretation is the “strength of linear association”. The slope’s interpretation is a little different: For every increase of 1 unit in age, there is an associated increase of, on average, 0.059 units of proportion.black. We only state that there is an associated increase and not necessarily a causal increase. For example, perhaps it’s not that higher “age” directly cause higher proportions of black per se. Instead, the following could hold true: lions with blacker noses are more likely to survive and hence have higher ages. In other words, just because two variables are strongly associated, it doesn’t necessarily mean that one causes the other. This is summed up in the often quoted phrase, “correlation is not necessarily causation.” We discuss this idea further in Subsection 8.3.1. Furthermore, we say that this associated increase is on average 0.059 years of age, because you might have two lions whose age scores differ by 1 unit, but their difference in proportion black won’t necessarily be exactly 0.059. What the slope of 0.059 is saying is that across all possible lions, the average difference in proportion black between two lions whose age differ by one is 0.059. Now that we’ve learned how to compute the equation for the regression line in Figure 8.4 using the values in the estimate column of Table 8.2, and how to interpret the resulting intercept and slope, let’s revisit the code that generated this table: # Fit regression model: lion_model &lt;- lm(proportion.black ~ age, data = LionNoses) # Get regression table: get_regression_table(lion_model) First, we “fit” the linear regression model to the data using the lm() function and save this as lion_model. When we say “fit”, we mean “find the best fitting line to this data.” lm() stands for “linear model” and is used as follows: lm(y ~ x, data = data_frame_name) where: y is the outcome variable, followed by a tilde ~. In our case, y is set to proportion.black. x is the explanatory variable. In our case, x is set to age. The combination of y ~ x is called a model formula. (Note the order of y and x.) In our case, the model formula is proportion.black ~ age. We saw such model formulas earlier when we computed the correlation coefficient using the get_correlation() function in Subsection 8.1.1. data_frame_name is the name of the data frame that contains the variables y and x. In our case, data_frame_name is the LionNoses data frame. Second, we take the saved model in lion_model and apply the get_regression_table() function from the moderndive package to it to obtain the regression table in Table 8.2. This function is an example of what’s known in computer programming as a wrapper function. They take other pre-existing functions and “wrap” them into a single function that hides its inner workings. This concept is illustrated in Figure 8.5. FIGURE 8.5: The concept of a wrapper function. So all you need to worry about is what the inputs look like and what the outputs look like; you leave all the other details “under the hood of the car.” In our regression modeling example, the get_regression_table() function takes a saved lm() linear regression model as input and returns a data frame of the regression table as output. If you’re interested in learning more about the get_regression_table() function’s inner workings, check out Subsection 8.3.3. Lastly, the remaining five columns in Table 8.2 are: std_error, statistic, p_value, lower_ci and upper_ci are the standard error, test statistic, p-value, lower 95% confidence interval bound, and upper 95% confidence interval bound. As you know, they tell us about both the statistical significance and practical significance of our results, that is, the “meaningfulness” of our results from a statistical perspective. Let’s put aside these ideas for now and revisit them in Chapter 9 on (statistical) inference for regression. Learning check (LC8.2) Fit a new simple linear regression using lm(ventilation ~ progesterone, data = ProgesteroneExercise). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your earlier exploratory data analysis of this dataset? 8.1.3 Observed/fitted values and residuals We just saw how to get the value of the intercept and the slope of a regression line from the estimate column of a regression table generated by the get_regression_table() function. Now instead say we want information on individual observations. For example, let’s focus on the 21st of the 32 courses in the LionNoses data frame in Table 8.3: TABLE 8.3: Data for the 21st lion out of 32 age proportion.black 5.8 0.6 What is the value \\(\\widehat{y}\\) on the regression line corresponding to this lion’s age of 5.8 years? In Figure 8.6 we mark three values corresponding to the 21st lion and give their statistical names: Circle: The observed value \\(y\\) = 0.6 is this lion’s actual proportion of the nose that was black. Square: The fitted value \\(\\widehat{y}\\) is the value on the regression line for \\(x\\) = age = 5.8. This value is computed using the intercept and slope in the previous regression table: \\[\\widehat{y} = b_0 + b_1 \\cdot x = 0.07 + 0.059 \\cdot 5.8 = 0.41\\] Arrow: The length of this arrow is the residual and is computed by subtracting the fitted value \\(\\widehat{y}\\) from the observed value \\(y\\). The residual can be thought of as a model’s error or “lack of fit” for a particular observation. In the case of this lion, it is \\(y - \\widehat{y}\\) = 0.6 - 0.41 = 0.19. FIGURE 8.6: Example of observed value, fitted value, and residual. Now say we want to compute both the fitted value \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) and the residual \\(y - \\widehat{y}\\) for all 32 lions in the study. Recall that each lion corresponds to one of the 32 rows in the LionNoses data frame and also one of the 32 points in the regression plot in Figure 8.6. We could repeat the previous calculations we performed by hand 32 times, but that would be tedious and time consuming. Instead, let’s do this using a computer with the get_regression_points() function. Just like the get_regression_table() function, the get_regression_points() function is a “wrapper” function. However, this function returns a different output. Let’s apply the get_regression_points() function to lion_model, which is where we saved our lm() model in the previous section. In Table 8.4 we present the results of only the 21st through 24th courses for brevity’s sake. regression_points &lt;- get_regression_points(lion_model) regression_points TABLE 8.4: Regression points (for only the 21st through 24th lions) ID proportion.black age proportion.black_hat residual 21 0.60 5.8 0.410 0.190 22 0.72 6.0 0.421 0.299 23 0.29 3.4 0.269 0.021 24 0.10 4.0 0.304 -0.204 Let’s inspect the individual columns and match them with the elements of Figure 8.6: The proportion.black column represents the observed outcome variable \\(y\\). This is the y-position of the 32 black points. The age column represents the values of the explanatory variable \\(x\\). This is the x-position of the 32 black points. The proportion.black_hat column represents the fitted values \\(\\widehat{y}\\). This is the corresponding value on the regression line for the 32 \\(x\\) values. The residual column represents the residuals \\(y - \\widehat{y}\\). This is the 32 vertical distances between the 32 black points and the regression line. Just as we did for the 21st lion in the LionNoses dataset (in the first row of the table), let’s repeat the calculations for the lion of the 24th lion (in the fourth row of Table 8.4): proportion.black = 0.1 is the observed proportion.black \\(y\\) for this lion. age = 4.00 is the value of the explanatory variable age \\(x\\) for this lion. proportion.black_hat = 0.304 = 0.07 + 0.059 \\(\\cdot\\) 4.00 is the fitted value \\(\\widehat{y}\\) on the regression line for this lion. residual = -0.204 = 0.1 - 0.304 is the value of the residual for this lion. In other words, the model’s fitted value was off by -0.204 for this lion. At this point, you can skip ahead if you like to Subsection 8.3.2 to learn about the processes behind what makes “best-fitting” regression lines. As a primer, a “best-fitting” line refers to the line that minimizes the sum of squared residuals out of all possible lines we can draw through the points. In Section 8.2, we’ll discuss another common scenario of having a categorical explanatory variable and a numerical outcome variable. Learning check (LC8.3) Generate a data frame of the residuals of the model lm(ventilation ~ progesterone, data = ProgesteroneExercise). 8.2 One categorical explanatory variable It’s an unfortunate truth that life expectancy is not the same across all countries in the world. International development agencies are interested in studying these differences in life expectancy in the hopes of identifying where governments should allocate resources to address this problem. In this section, we’ll explore differences in life expectancy in two ways: Differences between continents: Are there significant differences in average life expectancy between the five populated continents of the world: Africa, the Americas, Asia, Europe, and Oceania? Differences within continents: How does life expectancy vary within the world’s five continents? For example, is the spread of life expectancy among the countries of Africa larger than the spread of life expectancy among the countries of Asia? To answer such questions, we’ll use the gapminder data frame included in the gapminder package. This dataset has international development statistics such as life expectancy, GDP per capita, and population for 142 countries for 5-year intervals between 1952 and 2007. Recall we visualized some of this data in Figure 2.1 in Subsection 2.1.2 on the grammar of graphics. We’ll use this data for basic regression again, but now using an explanatory variable \\(x\\) that is categorical, as opposed to the numerical explanatory variable model we used in the previous Section 8.1. A numerical outcome variable \\(y\\) (a country’s life expectancy) and A single categorical explanatory variable \\(x\\) (the continent that the country is a part of). When the explanatory variable \\(x\\) is categorical, the concept of a “best-fitting” regression line is a little different than the one we saw previously in Section 8.1 where the explanatory variable \\(x\\) was numerical. We’ll study these differences shortly in Subsection 8.2.2, but first we conduct an exploratory data analysis. 8.2.1 Exploratory data analysis The data on the 142 countries can be found in the gapminder data frame included in the gapminder package. However, to keep things simple, let’s filter() for only those observations/rows corresponding to the year 2007. Additionally, let’s select() only the subset of the variables we’ll consider in this chapter. We’ll save this data in a new data frame called gapminder2007: library(gapminder) gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% select(country, lifeExp, continent, gdpPercap) Let’s perform the first common step in an exploratory data analysis: looking at the raw data values. You can do this by using RStudio’s spreadsheet viewer or by using the glimpse() command as introduced in Subsection 1.4.3 on exploring data frames: glimpse(gapminder2007) Rows: 142 Columns: 4 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, … $ lifeExp &lt;dbl&gt; 43.8, 76.4, 72.3, 42.7, 75.3, 81.2, 79.8, 75.6, 64.1, 79.4, … $ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, Asi… $ gdpPercap &lt;dbl&gt; 975, 5937, 6223, 4797, 12779, 34435, 36126, 29796, 1391, 336… Observe that Observations: 142 indicates that there are 142 rows/observations in gapminder2007, where each row corresponds to one country. In other words, the observational unit is an individual country. Furthermore, observe that the variable continent is of type &lt;fct&gt;, which stands for factor, which is R’s way of encoding categorical variables. A full description of all the variables included in gapminder can be found by reading the associated help file (run ?gapminder in the console). However, let’s fully describe only the 4 variables we selected in gapminder2007: country: An identification variable of type character/text used to distinguish the 142 countries in the dataset. lifeExp: A numerical variable of that country’s life expectancy at birth. This is the outcome variable \\(y\\) of interest. continent: A categorical variable with five levels. Here “levels” correspond to the possible categories: Africa, Asia, Americas, Europe, and Oceania. This is the explanatory variable \\(x\\) of interest. gdpPercap: A numerical variable of that country’s GDP per capita in US inflation-adjusted dollars that we’ll use as another outcome variable \\(y\\) in the Learning check at the end of this subsection. Let’s look at a random sample of five out of the 142 countries in Table 8.5. gapminder2007 %&gt;% sample_n(size = 5) TABLE 8.5: Random sample of 5 out of 142 countries country lifeExp continent gdpPercap Togo 58.4 Africa 883 Sao Tome and Principe 65.5 Africa 1598 Congo, Dem. Rep.  46.5 Africa 278 Lesotho 42.6 Africa 1569 Bulgaria 73.0 Europe 10681 Note that random sampling will likely produce a different subset of 5 rows for you than what’s shown. Now that we’ve looked at the raw values in our gapminder2007 data frame and got a sense of the data, let’s move on to computing summary statistics. Let’s once again apply the skim() function from the skimr package. Recall from our previous EDA that this function takes in a data frame, “skims” it, and returns commonly used summary statistics. Let’s take our gapminder2007 data frame, select() only the outcome and explanatory variables lifeExp and continent, and pipe them into the skim() function: gapminder2007 %&gt;% select(lifeExp, continent) %&gt;% skim() Skim summary statistics n obs: 142 n variables: 2 ── Variable type:factor variable missing complete n n_unique top_counts ordered continent 0 142 142 5 Afr: 52, Asi: 33, Eur: 30, Ame: 25 FALSE ── Variable type:numeric variable missing complete n mean sd p0 p25 p50 p75 p100 lifeExp 0 142 142 67.01 12.07 39.61 57.16 71.94 76.41 82.6 The skim() output now reports summaries for categorical variables (Variable type:factor) separately from the numerical variables (Variable type:numeric). For the categorical variable continent, it reports: missing, complete, and n, which are the number of missing, complete, and total number of values as before, respectively. n_unique: The number of unique levels to this variable, corresponding to Africa, Asia, Americas, Europe, and Oceania. This refers to how many countries are in the data for each continent. top_counts: In this case, the top four counts: Africa has 52 countries, Asia has 33, Europe has 30, and Americas has 25. Not displayed is Oceania with 2 countries. ordered: This tells us whether the categorical variable is “ordinal”: whether there is an encoded hierarchy (like low, medium, high). In this case, continent is not ordered. Turning our attention to the summary statistics of the numerical variable lifeExp, we observe that the global median life expectancy in 2007 was 71.94. Thus, half of the world’s countries (71 countries) had a life expectancy less than 71.94. The mean life expectancy of 67.01 is lower, however. Why is the mean life expectancy lower than the median? We can answer this question by performing the last of the three common steps in an exploratory data analysis: creating data visualizations. Let’s visualize the distribution of our outcome variable \\(y\\) = lifeExp in Figure 8.7. ggplot(gapminder2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Life expectancy&quot;, y = &quot;Number of countries&quot;, title = &quot;Histogram of distribution of worldwide life expectancies&quot;) FIGURE 8.7: Histogram of life expectancy in 2007. We see that this data is left-skewed, also known as negatively skewed: there are a few countries with low life expectancy that are bringing down the mean life expectancy. However, the median is less sensitive to the effects of such outliers; hence, the median is greater than the mean in this case. Remember, however, that we want to compare life expectancies both between continents and within continents. In other words, our visualizations need to incorporate some notion of the variable continent. We can do this easily with a faceted histogram. Recall from Section 2.5 that facets allow us to split a visualization by the different values of another variable. We display the resulting visualization in Figure 8.8 by adding a facet_wrap(~ continent, nrow = 2) layer. ggplot(gapminder2007, aes(x = lifeExp)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Life expectancy&quot;, y = &quot;Number of countries&quot;, title = &quot;Histogram of distribution of worldwide life expectancies&quot;) + facet_wrap(~ continent, nrow = 2) FIGURE 8.8: Life expectancy in 2007. Observe that unfortunately the distribution of African life expectancies is much lower than the other continents, while in Europe life expectancies tend to be higher and furthermore do not vary as much. On the other hand, both Asia and Africa have the most variation in life expectancies. There is the least variation in Oceania, but keep in mind that there are only two countries in Oceania: Australia and New Zealand. Recall that an alternative method to visualize the distribution of a numerical variable split by a categorical variable is by using a side-by-side boxplot. We map the categorical variable continent to the \\(x\\)-axis and the different life expectancies within each continent on the \\(y\\)-axis in Figure 8.9. ggplot(gapminder2007, aes(x = continent, y = lifeExp)) + geom_boxplot() + labs(x = &quot;Continent&quot;, y = &quot;Life expectancy&quot;, title = &quot;Life expectancy by continent&quot;) FIGURE 8.9: Life expectancy in 2007. Some people prefer comparing the distributions of a numerical variable between different levels of a categorical variable using a boxplot instead of a faceted histogram. This is because we can make quick comparisons between the categorical variable’s levels with imaginary horizontal lines. For example, observe in Figure 8.9 that we can quickly convince ourselves that Oceania has the highest median life expectancies by drawing an imaginary horizontal line at \\(y\\) = 80. Furthermore, as we observed in the faceted histogram in Figure 8.8, Africa and Asia have the largest variation in life expectancy as evidenced by their large interquartile ranges (the heights of the boxes). It’s important to remember, however, that the solid lines in the middle of the boxes correspond to the medians (the middle value) rather than the mean (the average). So, for example, if you look at Asia, the solid line denotes the median life expectancy of around 72 years. This tells us that half of all countries in Asia have a life expectancy below 72 years, whereas half have a life expectancy above 72 years. Let’s compute the median and mean life expectancy for each continent with a little more data wrangling and display the results in Table 8.6. lifeExp_by_continent &lt;- gapminder2007 %&gt;% group_by(continent) %&gt;% summarize(median = median(lifeExp), mean = mean(lifeExp)) TABLE 8.6: Life expectancy by continent continent median mean Africa 52.9 54.8 Americas 72.9 73.6 Asia 72.4 70.7 Europe 78.6 77.6 Oceania 80.7 80.7 Observe the order of the second column median life expectancy: Africa is lowest, the Americas and Asia are next with similar medians, then Europe, then Oceania. This ordering corresponds to the ordering of the solid black lines inside the boxes in our side-by-side boxplot in Figure 8.9. Let’s now turn our attention to the values in the third column mean. Using Africa’s mean life expectancy of 54.8 as a baseline for comparison, let’s start making comparisons to the mean life expectancies of the other four continents and put these values in Table 8.7, which we’ll revisit later on in this section. For the Americas, it is 73.6 - 54.8 = 18.8 years higher. For Asia, it is 70.7 - 54.8 = 15.9 years higher. For Europe, it is 77.6 - 54.8 = 22.8 years higher. For Oceania, it is 80.7 - 54.8 = 25.9 years higher. TABLE 8.7: Mean life expectancy by continent and relative differences from mean for Africa continent mean Difference versus Africa Africa 54.8 0.0 Americas 73.6 18.8 Asia 70.7 15.9 Europe 77.6 22.8 Oceania 80.7 25.9 Learning check (LC8.4) Conduct a new exploratory data analysis with the same explanatory variable \\(x\\) being continent but with gdpPercap as the new outcome variable \\(y\\). What can you say about the differences in GDP per capita between continents based on this exploration? 8.2.2 Linear regression In Subsection 8.1.2 we introduced simple linear regression, which involves modeling the relationship between a numerical outcome variable \\(y\\) and a numerical explanatory variable \\(x\\). In our life expectancy example, we now instead have a categorical explanatory variable continent. Our model will not yield a “best-fitting” regression line like in Figure 8.4, but rather offsets relative to a baseline for comparison. As we did in Subsection 8.1.2 when studying the relationship between black proportion of life noses and age, let’s output the regression table for this model. Recall that this is done in two steps: We first “fit” the linear regression model using the lm(y ~ x, data) function and save it in lifeExp_model. We get the regression table by applying the get_regression_table() function from the moderndive package to lifeExp_model. lifeExp_model &lt;- lm(lifeExp ~ continent, data = gapminder2007) get_regression_table(lifeExp_model) TABLE 8.8: Linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 54.8 1.02 53.45 0 52.8 56.8 continent: Americas 18.8 1.80 10.45 0 15.2 22.4 continent: Asia 15.9 1.65 9.68 0 12.7 19.2 continent: Europe 22.8 1.70 13.47 0 19.5 26.2 continent: Oceania 25.9 5.33 4.86 0 15.4 36.5 Let’s once again focus on the values in the term and estimate columns of Table 8.8. Why are there now 5 rows? Let’s break them down one-by-one: intercept corresponds to the mean life expectancy of countries in Africa of 54.8 years. continentAmericas corresponds to countries in the Americas and the value +18.8 is the same difference in mean life expectancy relative to Africa we displayed in Table 8.7. In other words, the mean life expectancy of countries in the Americas is \\(54.8 + 18.8 = 73.6\\). continentAsia corresponds to countries in Asia and the value +15.9 is the same difference in mean life expectancy relative to Africa we displayed in Table 8.7. In other words, the mean life expectancy of countries in Asia is \\(54.8 + 15.9 = 70.7\\). continentEurope corresponds to countries in Europe and the value +22.8 is the same difference in mean life expectancy relative to Africa we displayed in Table 8.7. In other words, the mean life expectancy of countries in Europe is \\(54.8 + 22.8 = 77.6\\). continentOceania corresponds to countries in Oceania and the value +25.9 is the same difference in mean life expectancy relative to Africa we displayed in Table 8.7. In other words, the mean life expectancy of countries in Oceania is \\(54.8 + 25.9 = 80.7\\). To summarize, the 5 values in the estimate column in Table 8.8 correspond to the “baseline for comparison” continent Africa (the intercept) as well as four “offsets” from this baseline for the remaining 4 continents: the Americas, Asia, Europe, and Oceania. You might be asking at this point why was Africa chosen as the “baseline for comparison” group. This is the case for no other reason than it comes first alphabetically of the five continents; by default R arranges factors/categorical variables in alphanumeric order. You can change this baseline group to be another continent if you manipulate the variable continent’s factor “levels” using the forcats package. See Chapter 15 of R for Data Science (Grolemund and Wickham 2017) for examples. Let’s now write the equation for our fitted values \\(\\widehat{y} = \\widehat{\\text{life exp}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\text{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\text{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x) \\end{aligned} \\] Whoa! That looks daunting! Don’t fret, however, as once you understand what all the elements mean, things simplify greatly. First, \\(\\mathbb{1}_{A}(x)\\) is what’s known in mathematics as an “indicator function.” It returns only one of two possible values, 0 and 1, where \\[ \\mathbb{1}_{A}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } x \\text{ is in } A \\\\ 0 &amp; \\text{if } \\text{otherwise} \\end{array} \\right. \\] In a statistical modeling context, this is also known as a dummy variable. In our case, let’s consider the first such indicator variable \\(\\mathbb{1}_{\\text{Amer}}(x)\\). This indicator function returns 1 if a country is in the Americas, 0 otherwise: \\[ \\mathbb{1}_{\\text{Amer}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{country } x \\text{ is in the Americas} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Second, \\(b_0\\) corresponds to the intercept as before; in this case, it’s the mean life expectancy of all countries in Africa. Third, the \\(b_{\\text{Amer}}\\), \\(b_{\\text{Asia}}\\), \\(b_{\\text{Euro}}\\), and \\(b_{\\text{Ocean}}\\) represent the 4 “offsets relative to the baseline for comparison” in the regression table output in Table 8.8: continentAmericas, continentAsia, continentEurope, and continentOceania. Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{life exp}}\\) for a country in Africa. Since the country is in Africa, all four indicator functions \\(\\mathbb{1}_{\\text{Amer}}(x)\\), \\(\\mathbb{1}_{\\text{Asia}}(x)\\), \\(\\mathbb{1}_{\\text{Euro}}(x)\\), and \\(\\mathbb{1}_{\\text{Ocean}}(x)\\) will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= b_0 + b_{\\text{Amer}}\\cdot\\mathbb{1}_{\\text{Amer}}(x) + b_{\\text{Asia}}\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad b_{\\text{Euro}}\\cdot\\mathbb{1}_{\\text{Euro}}(x) + b_{\\text{Ocean}}\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + \\\\ &amp; \\qquad 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 \\end{aligned} \\] In other words, all that’s left is the intercept \\(b_0\\), corresponding to the average life expectancy of African countries of 54.8 years. Next, say we are considering a country in the Americas. In this case, only the indicator function \\(\\mathbb{1}_{\\text{Amer}}(x)\\) for the Americas will equal 1, while all the others will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + \\\\ &amp; \\qquad 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 1 + 15.9\\cdot 0 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 18.8 \\\\ &amp; = 73.6 \\end{aligned} \\] which is the mean life expectancy for countries in the Americas of 73.6 years in Table 8.7. Note the “offset from the baseline for comparison” is +18.8 years. Let’s do one more. Say we are considering a country in Asia. In this case, only the indicator function \\(\\mathbb{1}_{\\text{Asia}}(x)\\) for Asia will equal 1, while all the others will equal 0, and thus: \\[ \\begin{aligned} \\widehat{\\text{life exp}} &amp;= 54.8 + 18.8\\cdot\\mathbb{1}_{\\text{Amer}}(x) + 15.9\\cdot\\mathbb{1}_{\\text{Asia}}(x) + 22.8\\cdot\\mathbb{1}_{\\text{Euro}}(x) + \\\\ &amp; \\qquad 25.9\\cdot\\mathbb{1}_{\\text{Ocean}}(x)\\\\ &amp;= 54.8 + 18.8\\cdot 0 + 15.9\\cdot 1 + 22.8\\cdot 0 + 25.9\\cdot 0\\\\ &amp;= 54.8 + 15.9 \\\\ &amp; = 70.7 \\end{aligned} \\] which is the mean life expectancy for Asian countries of 70.7 years in Table 8.7. The “offset from the baseline for comparison” here is +15.9 years. Let’s generalize this idea a bit. If we fit a linear regression model using a categorical explanatory variable \\(x\\) that has \\(k\\) possible categories, the regression table will return an intercept and \\(k - 1\\) “offsets.” In our case, since there are \\(k = 5\\) continents, the regression model returns an intercept corresponding to the baseline for comparison group of Africa and \\(k - 1 = 4\\) offsets corresponding to the Americas, Asia, Europe, and Oceania. Understanding a regression table output when you’re using a categorical explanatory variable is a topic those new to regression often struggle with. The only real remedy for these struggles is practice, practice, practice. However, once you equip yourselves with an understanding of how to create regression models using categorical explanatory variables, you’ll be able to incorporate many new variables into your models, given the large amount of the world’s data that is categorical. If you feel like you’re still struggling at this point, however, we suggest you closely compare Tables 8.7 and 8.8 and note how you can compute all the values from one table using the values in the other. Learning check (LC8.5) Fit a new linear regression using lm(gdpPercap ~ continent, data = gapminder2007) where gdpPercap is the new outcome variable \\(y\\). Get information about the “best-fitting” line from the regression table by applying the get_regression_table() function. How do the regression results match up with the results from your previous exploratory data analysis? 8.2.3 Observed/fitted values and residuals Recall in Subsection 8.1.3, we defined the following three concepts: Observed values \\(y\\), or the observed value of the outcome variable Fitted values \\(\\widehat{y}\\), or the value on the regression line for a given \\(x\\) value Residuals \\(y - \\widehat{y}\\), or the error between the observed value and the fitted value We obtained these values and other values using the get_regression_points() function from the moderndive package. This time, however, let’s add an argument setting ID = \"country\": this is telling the function to use the variable country in gapminder2007 as an identification variable in the output. This will help contextualize our analysis by matching values to countries. regression_points &lt;- get_regression_points(lifeExp_model, ID = &quot;country&quot;) regression_points TABLE 8.9: Regression points (First 10 out of 142 countries) country lifeExp continent lifeExp_hat residual Afghanistan 43.8 Asia 70.7 -26.900 Albania 76.4 Europe 77.6 -1.226 Algeria 72.3 Africa 54.8 17.495 Angola 42.7 Africa 54.8 -12.075 Argentina 75.3 Americas 73.6 1.712 Australia 81.2 Oceania 80.7 0.516 Austria 79.8 Europe 77.6 2.180 Bahrain 75.6 Asia 70.7 4.907 Bangladesh 64.1 Asia 70.7 -6.666 Belgium 79.4 Europe 77.6 1.792 Observe in Table 8.9 that lifeExp_hat contains the fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{lifeExp}}\\). If you look closely, there are only 5 possible values for lifeExp_hat. These correspond to the five mean life expectancies for the 5 continents that we displayed in Table 8.7 and computed using the values in the estimate column of the regression table in Table 8.8. The residual column is simply \\(y - \\widehat{y}\\) = lifeExp - lifeExp_hat. These values can be interpreted as the deviation of a country’s life expectancy from its continent’s average life expectancy. For example, look at the first row of Table 8.9 corresponding to Afghanistan. The residual of \\(y - \\widehat{y} = 43.8 - 70.7 = -26.9\\) is telling us that Afghanistan’s life expectancy is a whopping 26.9 years lower than the mean life expectancy of all Asian countries. This can in part be explained by the many years of war that country has suffered. Learning check (LC8.6) Using either the sorting functionality of RStudio’s spreadsheet viewer or using the data wrangling tools you learned in Chapter 3, identify the five countries with the five smallest (most negative) residuals? What do these negative residuals say about their life expectancy relative to their continents’ life expectancy? (LC8.7) Repeat this process, but identify the five countries with the five largest (most positive) residuals. What do these positive residuals say about their life expectancy relative to their continents’ life expectancy? 8.3 Related topics 8.3.1 Correlation is not necessarily causation Throughout this chapter we’ve been cautious when interpreting regression slope coefficients. We always discussed the “associated” effect of an explanatory variable \\(x\\) on an outcome variable \\(y\\). For example, our statement from Subsection 8.1.2 that “for every increase of 1 unit in age, there is an associated increase of on average 0.059 units of proportion.black.” We include the term “associated” to be extra careful not to suggest we are making a causal statement. So while lionage is positively correlated with proportion.black, we can’t necessarily make any statements about age’s direct causal effect on black proportion without more information on how this study was conducted. Here is another example: a not-so-great medical doctor goes through medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares, “Sleeping with shoes on causes headaches!” FIGURE 8.10: Does sleeping with shoes on cause headaches? However, there is a good chance that if someone is sleeping with their shoes on, it’s potentially because they are intoxicated from alcohol. Furthermore, higher levels of drinking leads to more hangovers, and hence more headaches. The amount of alcohol consumption here is what’s known as a confounding/lurking variable. It “lurks” behind the scenes, confounding the causal relationship (if any) of “sleeping with shoes on” with “waking up with a headache.” We can summarize this in Figure 8.11 with a causal graph where: Y is a response variable; here it is “waking up with a headache.” X is a treatment variable whose causal effect we are interested in; here it is “sleeping with shoes on.” FIGURE 8.11: Causal graph. To study the relationship between Y and X, we could use a regression model where the outcome variable is set to Y and the explanatory variable is set to be X, as you’ve been doing throughout this chapter. However, Figure 8.11 also includes a third variable with arrows pointing at both X and Y: Z is a confounding variable that affects both X and Y, thereby “confounding” their relationship. Here the confounding variable is alcohol. Alcohol will cause people to be both more likely to sleep with their shoes on as well as be more likely to wake up with a headache. Thus any regression model of the relationship between X and Y should also use Z as an explanatory variable. In other words, our doctor needs to take into account who had been drinking the night before. In the next chapter, we’ll start covering multiple regression models that allow us to incorporate more than one variable in our regression models. Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of confounding variables. Both these approaches attempt, as best they can, either to take all possible confounding variables into account or negate their impact. This allows researchers to focus only on the relationship of interest: the relationship between the outcome variable Y and the treatment variable X. As you read news stories, be careful not to fall into the trap of thinking that correlation necessarily implies causation. Check out the Spurious Correlations website for some rather comical examples of variables that are correlated, but are definitely not causally related. 8.3.2 Best-fitting line Regression lines are also known as “best-fitting” lines. But what do we mean by “best”? Let’s unpack the criteria that is used in regression to determine “best.” Recall Figure 8.6, where for an lion with an age of \\(x = 5.8\\) years we mark the observed value \\(y\\) with a circle, the fitted value \\(\\widehat{y}\\) with a square, and the residual \\(y - \\widehat{y}\\) with an arrow. We re-display Figure 8.6 in the top-left plot of Figure 8.12 in addition to three more arbitrarily chosen course lions: FIGURE 8.12: Example of observed value, fitted value, and residual. The three other plots refer to: A lion with an age of \\(x\\) = 1.9 years and black proportion of \\(y\\) = 0.27. The residual in this case is \\(0.27 - 0.181 = 0.089\\), which we mark with a new blue arrow in the top-right plot. A lion with an age of \\(x = 2.9\\) years and black proportion of \\(y = 0.18\\). The residual in this case is \\(0.18 - 0.24 = -0.06\\), which we mark with a new blue arrow in the bottom-left plot. A lion with an age of \\(x = 1.5\\) years and black proportion of \\(y = 0.0.14\\). The residual in this case is \\(0.14 - 0.158 = -0.018\\), which we mark with a new blue arrow in the bottom-right plot. Now say we repeated this process of computing residuals for all 32 courses’ lions, then we squared all the residuals, and then we summed them. We call this quantity the sum of squared residuals; it is a measure of the lack of fit of a model. Larger values of the sum of squared residuals indicate a bigger lack of fit. This corresponds to a worse fitting model. If the regression line fits all the points perfectly, then the sum of squared residuals is 0. This is because if the regression line fits all the points perfectly, then the fitted value \\(\\widehat{y}\\) equals the observed value \\(y\\) in all cases, and hence the residual \\(y-\\widehat{y}\\) = 0 in all cases, and the sum of even a large number of 0’s is still 0. Furthermore, of all possible lines we can draw through the cloud of 32 points, the regression line minimizes this value. In other words, the regression and its corresponding fitted values \\(\\widehat{y}\\) minimizes the sum of the squared residuals: \\[ \\sum_{i=1}^{n}(y_i - \\widehat{y}_i)^2 \\] Let’s use our data wrangling tools from Chapter 3 to compute the sum of squared residuals exactly: # Fit regression model: lion_model &lt;- lm(proportion.black ~ age, data = LionNoses) # Get regression points: regression_points &lt;- get_regression_points(lion_model) regression_points # A tibble: 32 × 5 ID proportion.black age proportion.black_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.21 1.1 0.134 0.076 2 2 0.14 1.5 0.158 -0.018 3 3 0.11 1.9 0.181 -0.071 4 4 0.13 2.2 0.199 -0.069 5 5 0.12 2.6 0.222 -0.102 6 6 0.13 3.2 0.257 -0.127 7 7 0.12 3.2 0.257 -0.137 8 8 0.18 2.9 0.24 -0.06 9 9 0.23 2.4 0.21 0.02 10 10 0.22 2.1 0.193 0.027 # … with 22 more rows # Compute sum of squared residuals regression_points %&gt;% mutate(squared_residuals = residual^2) %&gt;% summarize(sum_of_squared_residuals = sum(squared_residuals)) # A tibble: 1 × 1 sum_of_squared_residuals &lt;dbl&gt; 1 0.460141 Any other straight line drawn in the figure would yield a sum of squared residuals greater than 132. This is a mathematically guaranteed fact that you can prove using calculus and linear algebra. That’s why alternative names for the linear regression line are the best-fitting line and the least-squares line. Why do we square the residuals (i.e., the arrow lengths)? So that both positive and negative deviations of the same amount are treated equally. Learning check (LC8.8) Note in Figure 8.13 there are 3 points marked with dots and: The “best” fitting solid regression line in blue An arbitrarily chosen dotted red line Another arbitrarily chosen dashed green line FIGURE 8.13: Regression line and two others. Compute the sum of squared residuals by hand for each line and show that of these three lines, the regression line in blue has the smallest value. 8.3.3 get_regression_x() functions Recall in this chapter we introduced two functions from the moderndive package: get_regression_table() that returns a regression table in Subsection 8.1.2 and get_regression_points() that returns point-by-point information from a regression model in Subsection 8.1.3. What is going on behind the scenes with the get_regression_table() and get_regression_points() functions? We mentioned in Subsection 8.1.2 that these were examples of wrapper functions. Such functions take other pre-existing functions and “wrap” them into single functions that hide the user from their inner workings. This way all the user needs to worry about is what the inputs look like and what the outputs look like. In this subsection, we’ll “get under the hood” of these functions and see how the “engine” of these wrapper functions works. Recall our two-step process to generate a regression table from Subsection 8.1.2: # Fit regression model: lion_model &lt;- lm(formula = proportion.black ~ age, data = LionNoses) # Get regression table: get_regression_table(lion_model) TABLE 8.10: Regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 0.070 0.042 1.66 0.107 -0.016 0.155 age 0.059 0.008 7.05 0.000 0.042 0.076 The get_regression_table() wrapper function takes two pre-existing functions in other R packages: tidy() from the broom package (Robinson, Hayes, and Couch 2022) and clean_names() from the janitor package (Firke 2021) and “wraps” them into a single function that takes in a saved lm() linear model, here lion_model, and returns a regression table saved as a “tidy” data frame. Here is how we used the tidy() and clean_names() functions to produce Table 8.11: library(broom) library(janitor) lion_model %&gt;% tidy(conf.int = TRUE) %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% rename(lower_ci = conf_low, upper_ci = conf_high) TABLE 8.11: Regression table using tidy() from broom package term estimate std_error statistic p_value lower_ci upper_ci (Intercept) 0.070 0.042 1.66 0.107 -0.016 0.155 age 0.059 0.008 7.05 0.000 0.042 0.076 Yikes! That’s a lot of code! So, in order to simplify your lives, we made the editorial decision to “wrap” all the code into get_regression_table(), freeing you from the need to understand the inner workings of the function. Note that the mutate_if() function is from the dplyr package and applies the round() function to three significant digits precision only to those variables that are numerical. Similarly, the get_regression_points() function is another wrapper function, but this time returning information about the individual points involved in a regression model like the fitted values, observed values, and the residuals. get_regression_points() uses the augment() function in the broom package instead of the tidy() function as with get_regression_table() to produce the data shown in Table 8.12: library(broom) library(janitor) lion_model %&gt;% augment() %&gt;% mutate_if(is.numeric, round, digits = 3) %&gt;% clean_names() %&gt;% select(-c(&quot;std_resid&quot;, &quot;hat&quot;, &quot;sigma&quot;, &quot;cooksd&quot;, &quot;std_resid&quot;)) TABLE 8.12: Regression points using augment() from broom package proportion_black age fitted resid 0.21 1.1 0.134 0.076 0.14 1.5 0.158 -0.018 0.11 1.9 0.181 -0.071 0.13 2.2 0.199 -0.069 0.12 2.6 0.222 -0.102 0.13 3.2 0.257 -0.127 0.12 3.2 0.257 -0.137 0.18 2.9 0.240 -0.060 0.23 2.4 0.210 0.020 0.22 2.1 0.193 0.027 In this case, it outputs only the variables of interest to students learning regression: the outcome variable \\(y\\) (proportion.black), all explanatory/predictor variables (age), all resulting fitted values \\(\\hat{y}\\) used by applying the equation of the regression line to age, and the residual \\(y - \\hat{y}\\). If you’re even more curious about how these and other wrapper functions work, take a look at the source code for these functions on GitHub. 8.4 Conclusion Chapter Learning Summary Data modeling examines the relationship between an outcome/response variable and one or more explanatory/predictor variables. Linear regression assumes a linear relationship between a numerical outcome variable and explanatory variables. Simple linear regression assumes a linear relationship between a numerical outcome variable and a single numerical explanatory variable. The correlation coefficient quantifies the strength of the linear relationship between two numerical variables. The regression, or best-fitting, line minimizes the distance between the line and the observed values and can be modeled with the mathematical equation \\(\\hat{y} = b_0 + b_1 \\cdot x\\). The fitted value, \\(\\hat{y}\\), is the value of \\(y\\) on the regression line for a given value of \\(x\\). The residual is the difference between the fitted value \\(\\hat{y}\\) and the observed value \\(y\\). Basic linear regression can be extended to model the relationship between a numerical outcome variable and a categorical explanatory variable. Linear regression with a categorical explanatory variable uses indicator functions (or dummy variables) that return a value of 1, if the explanatory variable is true, or 0, if not. With a categorical explanatory variable, the intercept is the fitted value of the baseline comparison group and the slope is the offset of another group from the intercept. When two variables are correlated, it doesn’t necessarily mean that one causes the other. The regression, or best-fitting, line is the line that minimizes the sum of squared residuals. A wrapper function chains together other functions to seamlessly perform a desired function. 8.4.1 What’s to come? In this chapter, you’ve studied the term basic regression, where you fit models that only have one explanatory variable. So far, we’ve only studied the estimate column of all our regression tables. Chapter 9 focuses on the remaining columns: the standard error (std_error), the test statistic, the p_value, and the lower and upper bounds of confidence intervals (lower_ci and upper_ci). Furthermore, we’ll revisit the concept of residuals \\(y - \\widehat{y}\\) and discuss their importance when interpreting the results of a regression model. We’ll perform what is known as a residual analysis of the residual variable of all get_regression_points() outputs. Residual analyses allow you to verify what are known as the conditions for inference for regression. References "],["9-inference-for-regression.html", "Chapter 9 Inference for Regression 9.1 Regression refresher 9.2 Interpreting regression tables 9.3 Conditions for inference for regression 9.4 Simulation-based inference for regression 9.5 Theory-based inference for regression 9.6 Conclusion", " Chapter 9 Inference for Regression In this chapter, we’ll further explore the regression models we first studied in Chapter 8. Armed with our knowledge of confidence intervals and hypothesis tests from Chapters 6 and 7, we’ll be able to apply statistical inference to further our understanding of relationships between outcome and explanatory variables. Chapter Learning Objectives At the end of this chapter, you should be able to… • Fully interpret a regression table to determine the p-value and 95% confidence interval of the point estimates for the intercept and slope of the regression line. • Check if data meets the necessary conditions to apply linear regression. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 4.3 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(infer) library(abd) 9.1 Regression refresher Before jumping into inference for regression, let’s remind ourselves of the study to estimate lion ages based on nose coloration in Section 8.1. 9.1.1 Lion age and nose coloration Recall using simple linear regression we modeled the relationship between A numerical outcome variable \\(y\\) (proportion of the nose that’s black) and A single numerical explanatory variable \\(x\\) (lion’s age). We started with the LionNoses data frame in the abd package, which contained the two variables of interest: age and proportion.black: glimpse(LionNoses) Rows: 32 Columns: 2 $ age &lt;dbl&gt; 1.1, 1.5, 1.9, 2.2, 2.6, 3.2, 3.2, 2.9, 2.4, 2.1, 1.9… $ proportion.black &lt;dbl&gt; 0.21, 0.14, 0.11, 0.13, 0.12, 0.13, 0.12, 0.18, 0.23,… In Subsection 8.1.1, we performed an exploratory data analysis of the relationship between these two variables of age and proportion.black. We saw there that a clearly positive correlation of 0.79 existed between the two variables. This was evidenced in Figure 9.1 of the scatterplot along with the “best-fitting” regression line that summarizes the linear relationship between the two variables of proportion.black and age. Recall in Subsection 8.3.2 that we defined a “best-fitting” line as the line that minimizes the sum of squared residuals. ggplot(LionNoses, aes(x = age, y = proportion.black)) + geom_point() + labs(x = &quot;Age (years)&quot;, y = &quot;Proportion of black nose&quot;, title = &quot;Scatterplot of relationship of relative coloration and age of male lions.&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 9.1: Relationship with regression line. Looking at this plot again, you can see that the line has a positive slope, increasing from left to right as the age variable increases, but by how much? To get to this information, recall that we followed a two-step procedure: We first “fit” the linear regression model using the lm() function with the formula proportion.black ~ age. We saved this model in lion_model. We get the regression table by applying the get_regression_table() function from the moderndive package to lion_model. # Fit regression model: lion_model &lt;- lm(proportion.black ~ age, data = LionNoses) # Get regression table: get_regression_table(lion_model) TABLE 9.1: Previously seen linear regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 0.070 0.042 1.66 0.107 -0.016 0.155 age 0.059 0.008 7.05 0.000 0.042 0.076 Using the values in the estimate column of the resulting regression table in Table 9.1, we could then obtain the equation of the “best-fitting” regression line in Figure 9.1: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x\\\\ \\widehat{\\text{proportion.black}} &amp;= b_0 + b_{\\text{age}} \\cdot\\text{age}\\\\ &amp;= 0.070 + 0.059\\cdot\\text{age} \\end{aligned} \\] where \\(b_0\\) is the fitted intercept and \\(b_1\\) is the fitted slope for age. Recall the interpretation of the \\(b_1\\) = 0.059 value of the fitted slope: For every increase of one unit in lion age, there is an associated increase, on average, of 0.059 units of the black proportion of the nose. Thus, the slope value quantifies the relationship between the \\(y\\) variable proportion.black and the \\(x\\) variable age. We also discussed the intercept value of \\(b_0\\) = 0.07 and its lack of practical interpretation, since the range of ages doesn’t include 0. 9.1.2 Sampling scenario Let’s now revisit this study in terms of the terminology and notation related to sampling we studied in Subsection 5.3.1. First, let’s view these 32 lions as a representative sample from a greater study population. In our case, let’s assume that the study population is all lions living in the wild and that this sample of 32 lions is a representative sample. Unfortunately, we can only assume these two facts without more knowledge of the sampling methodology used by the researchers. Since we are viewing these \\(n\\) = 32 lions as a sample, we can view our fitted slope \\(b_1\\) = 0.059 as a point estimate of the population slope \\(\\beta_1\\). In other words, \\(\\beta_1\\) quantifies the relationship between proportion.black and age for all lions in the wild. Similarly, we can view our fitted intercept \\(b_0\\) = 0.07 as a point estimate of the population intercept \\(\\beta_0\\) for all lions in the wild. Putting these two ideas together, we can view the equation of the fitted line \\(\\widehat{y}\\) = \\(b_0 + b_1 \\cdot x\\) = \\(0.070 + 0.059 \\cdot \\text{age}\\) as an estimate of some true and unknown population line \\(y = \\beta_0 + \\beta_1 \\cdot x\\). Thus we can draw parallels between our lion age analysis and all the sampling scenarios we’ve seen previously. In this chapter, we’ll focus on the final scenario of regression slopes as shown in Table 9.2. TABLE 9.2: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) Since we are now viewing our fitted slope \\(b_1\\) and fitted intercept \\(b_0\\) as point estimates based on a sample, these estimates will again be subject to sampling variability. In other words, if we collected a new sample of data on a different set of \\(n\\) = 32 lions, the new fitted slope \\(b_1\\) will likely differ from 0.059. The same goes for the new fitted intercept \\(b_0\\). But by how much will these estimates vary? This information is in the remaining columns of the regression table in Table 9.1. Our knowledge of sampling from Chapter 5, confidence intervals from Chapter 6, and hypothesis tests from Chapter 7 will help us interpret these remaining columns. 9.2 Interpreting regression tables We’ve so far focused only on the two leftmost columns of the regression table in Table 9.1: term and estimate. Let’s now shift our attention to the remaining columns: std_error, statistic, p_value, lower_ci and upper_ci in Table 9.3. TABLE 9.3: Previously seen regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 0.070 0.042 1.66 0.107 -0.016 0.155 age 0.059 0.008 7.05 0.000 0.042 0.076 Given the lack of practical interpretation for the fitted intercept \\(b_0\\), in this section we’ll focus only on the second row of the table corresponding to the fitted slope \\(b_1\\). We’ll first interpret the std_error, statistic, p_value, lower_ci and upper_ci columns. Afterwards in the upcoming Subsection 9.2.5, we’ll discuss how R computes these values. 9.2.1 Standard error The third column of the regression table in Table 9.1 std_error corresponds to the standard error of our estimates. Recall the definition of standard error we saw in Subsection 5.3.2: The standard error is the standard deviation of any point estimate computed from a sample. So what does this mean in terms of the fitted slope \\(b_1\\) = 0.059? This value is just one possible value of the fitted slope resulting from this particular sample of \\(n\\) = 32 pairs of black proportions and ages. However, if we collected a different sample of \\(n\\) = 32 pairs of black proportions and ages, we will almost certainly obtain a different fitted slope \\(b_1\\). This is due to sampling variability. Say we hypothetically collected 1000 such samples of pairs of black proportions and ages, computed the 1000 resulting values of the fitted slope \\(b_1\\), and visualized them in a histogram. This would be a visualization of the sampling distribution of \\(b_1\\), which we defined in Subsection 5.3.2. Further recall that the standard deviation of the sampling distribution of \\(b_1\\) has a special name: the standard error. Recall that we constructed three sampling distributions for the sample proportion \\(\\widehat{p}\\) using shovels of size 25, 50, and 100 in Figure 5.13. We observed that as the sample size increased, the standard error decreased as evidenced by the narrowing sampling distribution. The standard error of \\(b_1\\) similarly quantifies how much variation in the fitted slope \\(b_1\\) one would expect between different samples. So in our case, we can expect about 0.008 units of variation in the age slope variable. Recall that the estimate and std_error values play a key role in inferring the value of the unknown population slope \\(\\beta_1\\) relating to all lions. In Section 9.4, we’ll perform a simulation using the infer package to construct the bootstrap distribution for \\(b_1\\) in this case. Recall from Subsection 6.7 that the bootstrap distribution is an approximation to the sampling distribution in that they have a similar shape. Since they have a similar shape, they have similar standard errors. However, unlike the sampling distribution, the bootstrap distribution is constructed from a single sample, which is a practice more aligned with what’s done in real life. 9.2.2 Test statistic The fourth column of the regression table in Table 9.1 statistic corresponds to a test statistic relating to the following hypothesis test: \\[ \\begin{aligned} H_0 &amp;: \\beta_1 = 0\\\\ \\text{vs } H_A&amp;: \\beta_1 \\neq 0. \\end{aligned} \\] Recall our terminology, notation, and definitions related to hypothesis tests we introduced in Section 7.2. A hypothesis test consists of a test between two competing hypotheses: (1) a null hypothesis \\(H_0\\) versus (2) an alternative hypothesis \\(H_A\\). A test statistic is a point estimate/sample statistic formula used for hypothesis testing. Here, our null hypothesis \\(H_0\\) assumes that the population slope \\(\\beta_1\\) is 0. If the population slope \\(\\beta_1\\) is truly 0, then this is saying that there is no true relationship between the black nose proportion and age for all lions in our population. In other words, \\(x\\) = age would have no associated effect on \\(y\\) = proportion.black. The alternative hypothesis \\(H_A\\), on the other hand, assumes that the population slope \\(\\beta_1\\) is not 0, meaning it could be either positive or negative. This suggests either a positive or negative relationship between black nose proportion and lion age. Recall we called such alternative hypotheses two-sided. By convention, all hypothesis testing for regression assumes two-sided alternatives. Recall our “hypothesized universe” of no effect of egg removal we assumed in our BirdMalaria activity in Section 7.1. Similarly here when conducting this hypothesis test, we’ll assume a “hypothesized universe” where there is no relationship between black nose proportion and lion age. In other words, we’ll assume the null hypothesis \\(H_0: \\beta_1 = 0\\) is true. The statistic column in the regression table is a tricky one, however. It corresponds to a standardized t-test statistic, much like the two-sample \\(t\\) statistic we saw in Subsection 7.6 where we used a theory-based method for conducting hypothesis tests. In both these cases, the null distribution can be mathematically proven to be a \\(t\\)-distribution. Since such test statistics are tricky for individuals new to statistical inference to study, we’ll skip this and jump into interpreting the \\(p\\)-value. If you’re curious, we have included a discussion of this standardized t-test statistic in Subsection 9.5. 9.2.3 p-value The fifth column of the regression table in Table 9.1 p_value corresponds to the p-value of the hypothesis test \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\). Again recalling our terminology, notation, and definitions related to hypothesis tests we introduced in Section 7.2, let’s focus on the definition of the \\(p\\)-value: A p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true. Recall that you can intuitively think of the \\(p\\)-value as quantifying how “extreme” the observed fitted slope of \\(b_1\\) = 0.059 is in a “hypothesized universe” where there is no relationship between black nose proportions and lion ages. Following the hypothesis testing procedure we outlined in Section 7.4, since the \\(p\\)-value in this case is 0, for any choice of significance level \\(\\alpha\\) we would reject \\(H_0\\) in favor of \\(H_A\\). Using non-statistical language, this is saying: we reject the hypothesis that there is no relationship between black nose proportions and lion ages in favor of the hypothesis that there is. That is to say, the evidence suggests there is a significant relationship, one that is positive. More precisely, however, the \\(p\\)-value corresponds to how extreme the observed test statistic of 7.053 is when compared to the appropriate null distribution. In Section 9.4, we’ll perform a simulation using the infer package to construct the null distribution in this case. An extra caveat here is that the results of this hypothesis test are only valid if certain “conditions for inference for regression” are met, which we’ll introduce shortly in Section 9.3. 9.2.4 Confidence interval The two rightmost columns of the regression table in Table 9.1 (lower_ci and upper_ci) correspond to the endpoints of the 95% confidence interval for the population slope \\(\\beta_1\\). Recall our analogy of “nets are to fish” what “confidence intervals are to population parameters” from Section 6.3. The resulting 95% confidence interval for \\(\\beta_1\\) of (0.042, 0.076) can be thought of as a range of plausible values for the population slope \\(\\beta_1\\) of the linear relationship between black nose proportions and lion ages. As we introduced in Subsection 6.5.2 on the precise and shorthand interpretation of confidence intervals, the statistically precise interpretation of this confidence interval is: “if we repeated this sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population slope \\(\\beta_1\\).” However, we’ll summarize this using our shorthand interpretation that “we’re 95% ‘confident’ that the true population slope \\(\\beta_1\\) lies between 0.042 and 0.076.” Notice in this case that the resulting 95% confidence interval for \\(\\beta_1\\) of \\((0.042, \\, 0.076)\\) does not contain a very particular value: \\(\\beta_1\\) equals 0. Recall we mentioned that if the population regression slope \\(\\beta_1\\) is 0, this is equivalent to saying there is no relationship between black nose proportions and lion ages. Since \\(\\beta_1\\) = 0 is not in our plausible range of values for \\(\\beta_1\\), we are inclined to believe that there, in fact, is a relationship between black nose proportions and lion ages and a positive one at that. So in this case, the conclusion about the population slope \\(\\beta_1\\) from the 95% confidence interval matches the conclusion from the hypothesis test: evidence suggests that there is a meaningful relationship between black nose proportions and lion ages. Recall from Subsection 6.5.3, however, that the confidence level is one of many factors that determine confidence interval widths. So for example, say we used a higher confidence level of 99% instead of 95%. The resulting confidence interval for \\(\\beta_1\\) would be wider and thus might now include 0. The lesson to remember here is that any confidence-interval-based conclusion depends highly on the confidence level used. What are the calculations that went into computing the two endpoints of the 95% confidence interval for \\(\\beta_1\\)? Recall our sampling bowl example from Subsection 6.8 discussing lower_ci and upper_ci. Since the sampling and bootstrap distributions of the sample proportion \\(\\widehat{p}\\) were roughly normal, we could use the rule of thumb for bell-shaped distributions from Appendix A.2 to create a 95% confidence interval for \\(p\\) with the following equation, where MoE is the margin of error: \\[\\widehat{p} \\pm \\text{MoE}_{\\widehat{p}} = \\widehat{p} \\pm 1.96 \\cdot \\text{SE}_{\\widehat{p}} = \\widehat{p} \\pm 1.96 \\cdot \\sqrt{\\frac{\\widehat{p}(1-\\widehat{p})}{n}}\\] We can generalize this to other point estimates that have roughly normally shaped sampling and/or bootstrap distributions: \\[\\text{point estimate} \\pm \\text{MoE} = \\text{point estimate} \\pm 1.96 \\cdot \\text{SE}.\\] We’ll show in Section 9.4 that the sampling/bootstrap distribution for the fitted slope \\(b_1\\) is in fact bell-shaped as well. Thus we can construct a 95% confidence interval for \\(\\beta_1\\) with the following equation: \\[b_1 \\pm \\text{MoE}_{b_1} = b_1 \\pm 1.96 \\cdot \\text{SE}_{b_1}.\\] What is the value of the standard error \\(\\text{SE}_{b_1}\\)? It is in fact in the third column of the regression table in Table 9.1: 0.008. Thus \\[ \\begin{aligned} b_1 \\pm 1.96 \\cdot \\text{SE}_{b_1} &amp;= 0.059 \\pm 1.96 \\cdot 0.008 = 0.059 \\pm 0.016\\\\ &amp;= (0.043, 0.075) \\end{aligned} \\] This closely matches the \\((0.042, 0.076)\\) confidence interval in the last two columns of Table 9.1. Much like hypothesis tests, however, the results of this confidence interval also are only valid if the “conditions for inference for regression” to be discussed in Section 9.3 are met. 9.2.5 How does R compute the table? Since we didn’t perform the simulation to get the values of the standard error, test statistic, \\(p\\)-value, and endpoints of the 95% confidence interval in Table 9.1, you might be wondering how were these values computed. What did R do behind the scenes? Does R run simulations like we did using the infer package in Chapters 6 and 7 on confidence intervals and hypothesis testing? The answer is no! Much like the theory-based method for constructing confidence intervals you saw in Subsection 6.8 and the theory-based hypothesis test you saw in Subsection 7.6, there exist mathematical formulas that allow you to construct confidence intervals and conduct hypothesis tests for inference for regression. These formulas were derived in a time when computers didn’t exist, so it would’ve been impossible to run the extensive computer simulations we have in this book. We present these formulas in Subsection 9.5 on “theory-based inference for regression.” In Section 9.4, we’ll go over a simulation-based approach to constructing confidence intervals and conducting hypothesis tests using the infer package. In particular, we’ll convince you that the bootstrap distribution of the fitted slope \\(b_1\\) is indeed bell-shaped. 9.3 Conditions for inference for regression Recall in Subsection 6.3.2 we stated that we could only use the standard-error-based method for constructing confidence intervals if the bootstrap distribution was bell shaped. Similarly, there are certain conditions that need to be met in order for the results of our hypothesis tests and confidence intervals we described in Section 9.2 to have valid meaning. These conditions must be met for the assumed underlying mathematical and probability theory to hold true. For inference for regression, there are four conditions that need to be met. Note the first four letters of these conditions are highlighted in bold in what follows: LINE. This can serve as a nice reminder of what to check for whenever you perform linear regression. Linearity of relationship between variables Independence of the residuals Normality of the residuals Equality of variance of the residuals Conditions L, N, and E can be verified through what is known as a residual analysis. Condition I can only be verified through an understanding of how the data was collected. In this section, we’ll go over a refresher on residuals, verify whether each of the four LINE conditions hold true, and then discuss the implications. 9.3.1 Residuals refresher Recall our definition of a residual from Subsection 8.1.3: it is the observed value minus the fitted value denoted by \\(y - \\widehat{y}\\). Recall that residuals can be thought of as the error or the “lack-of-fit” between the observed value \\(y\\) and the fitted value \\(\\widehat{y}\\) on the regression line in Figure 9.1. In Figure 9.2, we illustrate one particular residual out of 32 using an arrow, as well as its corresponding observed and fitted values using a circle and a square, respectively. FIGURE 9.2: Example of observed value, fitted value, and residual. Furthermore, we can automate the calculation of all \\(n\\) = 32 residuals by applying the get_regression_points() function to our saved regression model in lion_model. Observe how the resulting values of residual are roughly equal to proportion.black - proportion.black_hat (there is potentially a slight difference due to rounding error). # Fit regression model: lion_model &lt;- lm(proportion.black ~ age, data = LionNoses) # Get regression points: regression_points &lt;- get_regression_points(lion_model) regression_points # A tibble: 32 × 5 ID proportion.black age proportion.black_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0.21 1.1 0.134 0.076 2 2 0.14 1.5 0.158 -0.018 3 3 0.11 1.9 0.181 -0.071 4 4 0.13 2.2 0.199 -0.069 5 5 0.12 2.6 0.222 -0.102 6 6 0.13 3.2 0.257 -0.127 7 7 0.12 3.2 0.257 -0.137 8 8 0.18 2.9 0.24 -0.06 9 9 0.23 2.4 0.21 0.02 10 10 0.22 2.1 0.193 0.027 # … with 22 more rows A residual analysis is used to verify conditions L, N, and E and can be performed using appropriate data visualizations. While there are more sophisticated statistical approaches that can also be done, we’ll focus on the much simpler approach of looking at plots. 9.3.2 Linearity of relationship The first condition is that the relationship between the outcome variable \\(y\\) and the explanatory variable \\(x\\) must be Linear. Recall the scatterplot in Figure 9.1 where we had the explanatory variable \\(x\\) as lion age and the outcome variable \\(y\\) as proportion.black. Would you say that the relationship between \\(x\\) and \\(y\\) is linear? It’s hard to say because of the scatter of the points about the line, but in the authors’ opinions, this relationship is “linear enough.” Let’s present an example where the relationship between \\(x\\) and \\(y\\) is clearly not linear in Figure 9.3. In this case, the points clearly do not form a line, but rather a U-shaped polynomial curve. In this case, any results from an inference for regression would not be valid. FIGURE 9.3: Example of a clearly non-linear relationship. 9.3.3 Independence of residuals The second condition is that the residuals must be Independent. In other words, the different observations in our data must be independent of one another. Presumably, the observations are independent, but without further details about how the study was conducted, we can’t be certain. For example, if the black nose proportion of the same lion was measured multiple times at different ages, then there would be dependence between those observations. If this was the case, a proper analysis of the data would need to take into account that we have repeated measures for the same lions. We’ll presume, however, that the observations are independent of each other. 9.3.4 Normality of residuals The third condition is that the residuals should follow a Normal distribution. Furthermore, the center of this distribution should be 0. In other words, sometimes the regression model will make positive errors: \\(y - \\widehat{y} &gt; 0\\). Other times, the regression model will make equally negative errors: \\(y - \\widehat{y} &lt; 0\\). However, on average the errors should equal 0 and their shape should be similar to that of a bell. The simplest way to check the normality of the residuals is to look at a histogram, which we visualize in Figure 9.4. ggplot(regression_points, aes(x = residual)) + geom_histogram(binwidth = 0.05, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) FIGURE 9.4: Histogram of residuals. This histogram shows that we have more negative residuals than positive. Since the residual \\(y-\\widehat{y}\\) is positive when \\(y &gt; \\widehat{y}\\), it seems our regression model’s fitted black nose proportions \\(\\widehat{y}\\) tend to overestimate the true proportion.black \\(y\\). Furthermore, this histogram has a slight right-skew in that there is a tail on the right. This is another way to say the residuals exhibit a positive skew. Is this a problem? Again, there is a certain amount of subjectivity in the response. In the authors’ opinion, while there is a slight skew to the residuals, we feel it isn’t drastic. On the other hand, others might disagree with our assessment. Let’s present examples where the residuals clearly do and don’t follow a normal distribution in Figure 9.5. In this case of the model yielding the clearly non-normal residuals on the right, any results from an inference for regression would not be valid. FIGURE 9.5: Example of clearly normal and clearly not normal residuals. 9.3.5 Equality of variance The fourth and final condition is that the residuals should exhibit Equal variance across all values of the explanatory variable \\(x\\). In other words, the value and spread of the residuals should not depend on the value of the explanatory variable \\(x\\). Recall the scatterplot in Figure 9.1: we had the explanatory variable \\(x\\) of age on the x-axis and the outcome variable \\(y\\) of proportion.black on the y-axis. Instead, let’s create a scatterplot that has the same values on the x-axis, but now with the residual \\(y-\\widehat{y}\\) on the y-axis as seen in Figure 9.6. ggplot(regression_points, aes(x = age, y = residual)) + geom_point() + labs(x = &quot;Age (years)&quot;, y = &quot;Residual&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) FIGURE 9.6: Plot of residuals over age. You can think of Figure 9.6 as a modified version of the plot with the regression line in Figure 9.1, but with the regression line flattened out to \\(y=0\\). Looking at this plot, would you say that the spread of the residuals around the line at \\(y=0\\) is constant across all values of the explanatory variable \\(x\\) of age? This question is rather qualitative and subjective in nature, thus different people may respond with different answers. For example, some people might say that there is slightly less variation in the residuals for smaller values of \\(x\\) than for middle ones. However, it can be argued that there isn’t a drastic non-constancy. In Figure 9.7 let’s present an example where the residuals clearly do not have equal variance across all values of the explanatory variable \\(x\\). FIGURE 9.7: Example of clearly non-equal variance. Observe how the spread of the residuals increases as the value of \\(x\\) increases. This is a situation known as heteroskedasticity. Any inference for regression based on a model yielding such a pattern in the residuals would not be valid. 9.3.6 What’s the conclusion? Let’s list our four conditions for inference for regression again and indicate whether or not they were satisfied in our analysis: Linearity of relationship between variables: Yes Independence of residuals: Presumably Normality of residuals: Somewhat Equality of variance: Yes So what does this mean for the results of our confidence intervals and hypothesis tests in Section 9.2? First, the Independence condition. Since the observations are presumably independent, we can proceed. If there had been dependencies between different rows in LionNoses, then we would need to address this. In more advanced statistics courses, you’ll learn how to incorporate such dependencies into your regression models. One such technique is called hierarchical/multilevel modeling. Second, when conditions L, N, E are not met, it often means there is a shortcoming in our model. For example, it may be the case that using only a single explanatory variable is insufficient, as we did with proportion.black. We may need to incorporate more explanatory variables in a multiple regression model as we did in Chapter 10. The conditions for inference in regression problems are a key part of regression analysis that are of vital importance to the processes of constructing confidence intervals and conducting hypothesis tests. However, it is often the case with regression analysis in the real world that not all the conditions are completely met. Furthermore, as you saw, there is a level of subjectivity in the residual analyses to verify the L, N, and E conditions. So what can you do? We as authors advocate for transparency in communicating all results. This lets the stakeholders of any analysis know about a model’s shortcomings or whether the model is “good enough.” So while the checking of assumptions may lead to some fuzzy “it depends” results, you should be prepared for difficult statistical decisions you may need to make down the road. Learning check (LC9.1) Continuing with our regression using age as the explanatory variable and proportion.black as the outcome variable. Use the get_regression_points() function to get the observed values, fitted values, and residuals for all 32 lions Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here. 9.4 Simulation-based inference for regression Recall in Subsection 9.2.5 when we interpreted the third through seventh columns of a regression table, we stated that R doesn’t do simulations to compute these values. Rather R uses theory-based methods that involve mathematical formulas. In this section, we’ll use the simulation-based methods you previously learned in Chapters 6 and 7 to recreate the values in the regression table in Table 9.1. In particular, we’ll use the infer package workflow to Construct a 95% confidence interval for the population slope \\(\\beta_1\\) using bootstrap resampling with replacement. We did this previously in Sections 6.4 with the pennies data and 6.6 with the mythbusters_yawn data. Conduct a hypothesis test of \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\) using a permutation test. We did this previously in Sections 7.3 with the BirdMalaria data and 7.5 with the HornedLizards data. 9.4.1 Confidence interval for slope We’ll construct a 95% confidence interval for \\(\\beta_1\\) using the infer workflow outlined in Subsection 6.4.2. Specifically, we’ll first construct the bootstrap distribution for the fitted slope \\(b_1\\) using our single sample of 32 lions: specify() the variables of interest in LionNoses with the formula: proportion.black ~ age. generate() replicates by using bootstrap resampling with replacement from the original sample of 32 lions We generate reps = 1000 replicates using type = \"bootstrap\". calculate() the summary statistic of interest: the fitted slope \\(b_1\\). Using this bootstrap distribution, we’ll construct the 95% confidence interval using the percentile method and (if appropriate) the standard error method as well. It is important to note in this case that the bootstrapping with replacement is done row-by-row. Thus, the original pairs of proportion.black and age values are always kept together, but different pairs of proportion.black and age values may be resampled multiple times. The resulting confidence interval will denote a range of plausible values for the unknown population slope \\(\\beta_1\\) quantifying the relationship between black nose proportions and lion ages for all lions in this wild population. Let’s first construct the bootstrap distribution for the fitted slope \\(b_1\\): bootstrap_distn_slope &lt;- LionNoses %&gt;% specify(formula = proportion.black ~ age) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;slope&quot;) bootstrap_distn_slope # A tibble: 1,000 × 2 replicate stat &lt;int&gt; &lt;dbl&gt; 1 1 0.0651055 2 2 0.0382313 3 3 0.108056 4 4 0.0666601 5 5 0.0715932 6 6 0.0854565 7 7 0.0624868 8 8 0.0412859 9 9 0.0796269 10 10 0.0761299 # … with 990 more rows Observe how we have 1000 values of the bootstrapped slope \\(b_1\\) in the stat column. Let’s visualize the 1000 bootstrapped values in Figure 9.8. visualize(bootstrap_distn_slope) FIGURE 9.8: Bootstrap distribution of slope. Observe how the bootstrap distribution is roughly bell-shaped. Recall from Subsection 6.7 that the shape of the bootstrap distribution of \\(b_1\\) closely approximates the shape of the sampling distribution of \\(b_1\\). Percentile-method First, let’s compute the 95% confidence interval for \\(\\beta_1\\) using the percentile method. We’ll do so by identifying the 2.5th and 97.5th percentiles which include the middle 95% of values. Recall that this method does not require the bootstrap distribution to be normally shaped. percentile_ci &lt;- bootstrap_distn_slope %&gt;% get_confidence_interval(type = &quot;percentile&quot;, level = 0.95) percentile_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0323411 0.0990027 The resulting percentile-based 95% confidence interval for \\(\\beta_1\\) of (0.032, 0.099) is similar to the confidence interval in the regression Table 9.1 of (0.042, 0.076). Standard error method Since the bootstrap distribution in Figure 9.8 appears to be roughly bell-shaped, we can also construct a 95% confidence interval for \\(\\beta_1\\) using the standard error method. In order to do this, we need to first compute the fitted slope \\(b_1\\), which will act as the center of our standard error-based confidence interval. While we saw in the regression table in Table 9.1 that this was \\(b_1\\) = 0.059, we can also use the infer pipeline with the generate() step removed to calculate it: observed_slope &lt;- LionNoses %&gt;% specify(proportion.black ~ age) %&gt;% calculate(stat = &quot;slope&quot;) observed_slope Response: proportion.black (numeric) Explanatory: age (numeric) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 0.0585912 We then use the get_ci() function with level = 0.95 to compute the 95% confidence interval for \\(\\beta_1\\). Note that setting the point_estimate argument to the observed_slope of 0.059 sets the center of the confidence interval. se_ci &lt;- bootstrap_distn_slope %&gt;% get_ci(level = 0.95, type = &quot;se&quot;, point_estimate = observed_slope) se_ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.0253308 0.0918515 The resulting standard error-based 95% confidence interval for \\(\\beta_1\\) of \\((0.025, 0.092)\\) is slightly different than the confidence interval in the regression Table 9.1 of \\((0.042, 0.076)\\). Comparing all three Let’s compare all three confidence intervals in Figure 9.9, where the percentile-based confidence interval is marked with solid lines, the standard error based confidence interval is marked with dashed lines, and the theory-based confidence interval (0.042, 0.076) from the regression table in Table 9.1 is marked with dotted lines. visualize(bootstrap_distn_slope) + shade_confidence_interval(endpoints = percentile_ci, fill = NULL, linetype = &quot;solid&quot;, color = &quot;grey90&quot;) + shade_confidence_interval(endpoints = se_ci, fill = NULL, linetype = &quot;dashed&quot;, color = &quot;grey60&quot;) + shade_confidence_interval(endpoints = c(0.035, 0.099), fill = NULL, linetype = &quot;dotted&quot;, color = &quot;black&quot;) FIGURE 9.9: Comparing three confidence intervals for the slope. Observe that all three are quite similar! Furthermore, none of the three confidence intervals for \\(\\beta_1\\) contain 0 and are entirely located above 0. This is suggesting that there is in fact a meaningful positive relationship between black nose proportions and lion ages. 9.4.2 Hypothesis test for slope Let’s now conduct a hypothesis test of \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\neq 0\\). We will use the infer package, which follows the hypothesis testing paradigm in the “There is only one test” diagram in Figure 7.14. Let’s first think about what it means for \\(\\beta_1\\) to be zero as assumed in the null hypothesis \\(H_0\\). Recall we said if \\(\\beta_1 = 0\\), then this is saying there is no relationship between the black nose proportions and lion ages. Thus assuming this particular null hypothesis \\(H_0\\) means that in our “hypothesized universe” there is no relationship between proportion.black and age. We can therefore shuffle/permute the age variable to no consequence. We construct the null distribution of the fitted slope \\(b_1\\) by performing the steps that follow. Recall from Section 7.2 on terminology, notation, and definitions related to hypothesis testing where we defined the null distribution: the sampling distribution of our test statistic \\(b_1\\) assuming the null hypothesis \\(H_0\\) is true. specify() the variables of interest in LionNoses with the formula: proportion.black ~ age. hypothesize() the null hypothesis of independence. Recall from Section 7.3 that this is an additional step that needs to be added for hypothesis testing. generate() replicates by permuting/shuffling values from the original sample of 32 courses. We generate reps = 1000 replicates using type = \"permute\" here. calculate() the test statistic of interest: the fitted slope \\(b_1\\). In this case, we permute the values of age across the values of proportion.black 1000 times. We can do this shuffling/permuting since we assumed a “hypothesized universe” of no relationship between these two variables. Then we calculate the \"slope\" coefficient for each of these 1000 generated samples. null_distn_slope &lt;- LionNoses %&gt;% specify(proportion.black ~ age) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;slope&quot;) Observe the resulting null distribution for the fitted slope \\(b_1\\) in Figure 9.10. FIGURE 9.10: Null distribution of slopes. Notice how it is centered at \\(b_1\\) = 0. This is because in our hypothesized universe, there is no relationship between proportion.black and age and so \\(\\beta_1 = 0\\). Thus, the most typical fitted slope \\(b_1\\) we observe across our simulations is 0. Observe, furthermore, how there is variation around this central value of 0. Let’s visualize the \\(p\\)-value in the null distribution by comparing it to the observed test statistic of \\(b_1\\) = 0.059 in Figure 9.11. We’ll do this by adding a shade_p_value() layer to the previous visualize() code. FIGURE 9.11: Null distribution and \\(p\\)-value. Since the observed fitted slope 0.059 falls far to the right of this null distribution and thus the shaded region doesn’t overlap it, we’ll have a \\(p\\)-value of 0. For completeness, however, let’s compute the numerical value of the \\(p\\)-value anyways using the get_p_value() function. Recall that it takes the same inputs as the shade_p_value() function: null_distn_slope %&gt;% get_p_value(obs_stat = observed_slope, direction = &quot;both&quot;) # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0 This matches the \\(p\\)-value of 0 in the regression table in Table 9.1. We therefore reject the null hypothesis \\(H_0: \\beta_1 = 0\\) in favor of the alternative hypothesis \\(H_A: \\beta_1 \\neq 0\\). We thus have evidence that suggests there is a significant relationship between black nose proportions and lion ages for all lions in this population. When the conditions for inference for regression are met and the null distribution has a bell shape, we are likely to see similar results between the simulation-based results we just demonstrated and the theory-based results shown in the regression table in Table 9.1. Learning check (LC9.2) Repeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of stat = \"correlation\" in the calculate() function of the infer package. 9.5 Theory-based inference for regression Recall in Subsection 9.2.5 when we interpreted the regression table in Table 9.1, we mentioned that R does not compute its values using simulation-based methods for constructing confidence intervals and conducting hypothesis tests as we did in Chapters 6 and 7 using the infer package. Rather, R uses a theory-based approach using mathematical formulas, much like the theory-based confidence intervals you saw in Subsection 6.8 and the theory-based hypothesis tests you saw in Subsection 7.6. These formulas were derived in a time when computers didn’t exist, so it would’ve been incredibly labor intensive to run extensive simulations. In particular, there is a formula for the standard error of the fitted slope \\(b_1\\): \\[\\text{SE}_{b_1} = \\dfrac{\\dfrac{s_y}{s_x} \\cdot \\sqrt{1-r^2}}{\\sqrt{n-2}}\\] As with many formulas in statistics, there’s a lot going on here, so let’s first break down what each symbol represents. First \\(s_x\\) and \\(s_y\\) are the sample standard deviations of the explanatory variable age and the response variable proportion.black, respectively. Second, \\(r\\) is the sample correlation coefficient between proportion.black and age. This was computed as 0.79 in Chapter 8. Lastly, \\(n\\) is the number of pairs of points in the LionNoses data frame, here 32. To put this formula into words, the standard error of \\(b_1\\) depends on the relationship between the variability of the response variable and the variability of the explanatory variable as measured in the \\(s_y / s_x\\) term. Next, it looks into how the two variables relate to each other in the \\(\\sqrt{1-r^2}\\) term. However, the most important observation to make in the previous formula is that there is an \\(n - 2\\) in the denominator. In other words, as the sample size \\(n\\) increases, the standard error \\(\\text{SE}_{b_1}\\) decreases. Just as we demonstrated in Subsection 5.3.3 when we used shovels with \\(n\\) = 25, 50, and 100 slots, the amount of sampling variation of the fitted slope \\(b_1\\) will depend on the sample size \\(n\\). In particular, as the sample size increases, both the sampling and bootstrap distributions narrow and the standard error \\(\\text{SE}_{b_1}\\) decreases. Hence, our estimates of \\(b_1\\) for the true population slope \\(\\beta_1\\) get more and more precise. R then uses this formula for the standard error of \\(b_1\\) in the third column of the regression table and subsequently to construct 95% confidence intervals. But what about the hypothesis test? Much like with our theory-based hypothesis test in Subsection 7.6, R uses the following \\(t\\)-statistic as the test statistic for hypothesis testing: \\[ t = \\dfrac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} \\] And since the null hypothesis \\(H_0: \\beta_1 = 0\\) is assumed during the hypothesis test, the \\(t\\)-statistic becomes \\[ t = \\dfrac{ b_1 - 0}{ \\text{SE}_{b_1}} = \\dfrac{ b_1 }{ \\text{SE}_{b_1}} \\] What are the values of \\(b_1\\) and \\(\\text{SE}_{b_1}\\)? They are in the estimate and std_error column of the regression table in Table 9.1. Thus the value of 7.053 in the table is computed as 0.059/0.008 = 7.375. Note there is a difference due to some rounding error here. Lastly, to compute the \\(p\\)-value, we need to compare the observed test statistic of 7.053 to the appropriate null distribution. Recall from Section 7.2, that a null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true. Much like in our theory-based hypothesis test in Subsection 7.6, it can be mathematically proven that this distribution is a \\(t\\)-distribution with degrees of freedom equal to \\(df = n - 2 = 32 - 2 = 30\\). Don’t worry if you’re feeling a little overwhelmed at this point. There is a lot of background theory to understand before you can fully make sense of the equations for theory-based methods. That being said, theory-based methods and simulation-based methods for constructing confidence intervals and conducting hypothesis tests often yield consistent results. As mentioned before, in our opinion, two large benefits of simulation-based methods over theory-based are that (1) they are easier for people new to statistical inference to understand, and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist. 9.6 Conclusion Chapter Learning Summary The fitted intercept and slope for a regression line based on a sample are point estimates of these values for the regression line of the population. The standard errors of the fitted intercept \\(b_0\\) and slope \\(b_1\\) quantify the sampling variability. The null hypothesis for linear regression is that the population slope \\(B_1\\) equals 0, indicating no relationship between the response and explanatory variables. Linear regression compares the observed standardized t-test statistic to its null distribution to determine the p-value. Linear regression uses the standard errors of \\(b_0\\) and \\(b_1\\) to calculate the confidence intervals of each coefficient. For linear regression results to be valid, the residuals must have a linear relationship, be independent, and have a normal distribution and equal variance across all \\(x\\) values (LINE). In addition to the theory-based method, computer-based methods can also be used to compute the p-value and 95% confidence interval for \\(B_0\\) and \\(B_1\\). 9.6.1 Summary of statistical inference We’ve finished the last scenario from the “Scenarios of sampling for inference” table in Subsection 5.6, which we re-display in Table 9.4. TABLE 9.4: Scenarios of sampling for inference Scenario Population parameter Notation Point estimate Symbol(s) 1 Population proportion \\(p\\) Sample proportion \\(\\widehat{p}\\) 2 Population mean \\(\\mu\\) Sample mean \\(\\overline{x}\\) or \\(\\widehat{\\mu}\\) 3 Difference in population proportions \\(p_1 - p_2\\) Difference in sample proportions \\(\\widehat{p}_1 - \\widehat{p}_2\\) 4 Difference in population means \\(\\mu_1 - \\mu_2\\) Difference in sample means \\(\\overline{x}_1 - \\overline{x}_2\\) 5 Population regression slope \\(\\beta_1\\) Fitted regression slope \\(b_1\\) or \\(\\widehat{\\beta}_1\\) Armed with the regression modeling techniques you learned in Chapter 8, your understanding of sampling for inference in Chapter 5, and the tools for statistical inference like confidence intervals and hypothesis tests in Chapters 6 and 7, you’re now equipped to study the significance of relationships between variables in a wide array of data! Many of the ideas presented here can be extended into multiple regression, as we’ll see in Chapter 10 and other more advanced modeling techniques. 9.6.2 What’s to come In Chapter 10, we’ll study multiple regression, where our regression models can now have more than one explanatory variable! In particular, we’ll consider two scenarios: regression models with one numerical and one categorical explanatory variable and regression models with two categorical explanatory variables. This will allow you to construct more sophisticated and more powerful models, all in the hopes of better explaining your outcome variable \\(y\\). "],["10-multiple-regression.html", "Chapter 10 Multiple Regression 10.1 One numerical and one categorical explanatory variable 10.2 Two categorical explanatory variables 10.3 Related topics 10.4 Conclusion", " Chapter 10 Multiple Regression In Chapter 8 we introduced ideas related to modeling for explanation, in particular that the goal of modeling is to make explicit the relationship between some outcome variable \\(y\\) and some explanatory variable \\(x\\). While there are many approaches to modeling, we focused on one particular technique: linear regression, one of the most commonly used and easy-to-understand approaches to modeling. Furthermore to keep things simple, we only considered models with one explanatory \\(x\\) variable that was either numerical in Section 8.1 or categorical in Section 8.2. In this chapter on multiple regression, we’ll start considering models that include more than one explanatory variable \\(x\\). You can imagine when trying to model a particular outcome variable, like proportion of the lion nose that is black as in Section 8.1 or life expectancy as in Section 8.2, that it would be useful to include more than just one explanatory variable’s worth of information. Since our regression models will now consider more than one explanatory variable, the interpretation of the associated effect of any one explanatory variable must be made in conjunction with the other explanatory variables included in your model. Let’s begin! Chapter Learning Objectives At the end of this chapter, you should be able to… * Fully interpret a multiple linear regression table for multiple regression. * Use available evidence to determine if an interaction or parallel slopes model is more appropriate. * Use a mathematical formula for multiple linear regression to compute the fitted value for given explanatory values. Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 4.3 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(moderndive) library(skimr) library(abd) 10.1 One numerical and one categorical explanatory variable Previously we looked at the relationship between age and nose coloration (proportion black) of male lions in Section 8.1. The variable proportion.black was the numerical outcome variable \\(y\\), and the variable age was the numerical explanatory \\(x\\) variable. In this section, we are going to consider a different model. Rather than a single explanatory variable, we’ll now include two different explanatory variables. In this example, we’ll look at the relationship between tooth growth and vitamin C intake in guinea pigs. Could it be that increasing levels of vitamin C stimulate tooth growth? Or could it instead be that increasing levels reduce tooth growth? Are there differences in tooth growth depending on the method used to deliver vitamin C? We’ll answer these questions by modeling the relationship between these variables using multiple regression, where we have: A numerical outcome variable \\(y\\), len, the length of odontoblasts, the cells responsible for tooth growth, and Two explanatory variables: A numerical explanatory variable \\(x_1\\), dose, the dose of vitamin C, in milligrams/day. A categorical explanatory variable \\(x_2\\), supp, the delivery method, either as orange juice (OJ) or ascorbic acid (VC). 10.1.1 Exploratory data analysis Our dataset is called ToothGrowth and included in R’s datasets package. Recall the three common steps in an exploratory data analysis we saw in Subsection 8.1.1: Looking at the raw data values. Computing summary statistics. Creating data visualizations. Let’s first look at the raw data values by either looking at ToothGrowth using RStudio’s spreadsheet viewer or by using the glimpse() function from the dplyr package: glimpse(ToothGrowth) Rows: 60 Columns: 3 $ len &lt;dbl&gt; 4.2, 11.5, 7.3, 5.8, 6.4, 10.0, 11.2, 11.2, 5.2, 7.0, 16.5, 16.5,… $ supp &lt;fct&gt; VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, V… $ dose &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, … Let’s also display a random sample of 5 rows of the 60 rows corresponding to different courses in Table 10.1. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. ToothGrowth %&gt;% sample_n(size = 5) TABLE 10.1: A random sample of 5 of the 60 data rows len supp dose 8.2 OJ 0.5 4.2 VC 0.5 20.0 OJ 1.0 21.5 VC 2.0 27.3 OJ 1.0 Now that we’ve looked at the raw values in our ToothGrowth data frame and got a sense of the data, let’s compute summary statistics. As we did in our exploratory data analyses in Sections 8.1.1 and 8.2.1 from the previous chapter, let’s use the skim() function from the skimr package: ToothGrowth %&gt;% skim() Observe that we have no missing data, that there are 30 guinea pigs given orange juice and 30 guinea pigs given ascorbic acid, and that the average vitamin C dose is 1.17. Furthermore, let’s compute the correlation coefficient between our two numerical variables: len and dose. Recall from Subsection 8.1.1 that correlation coefficients only exist between numerical variables. We observe that they are “clearly positively” correlated. ToothGrowth %&gt;% get_correlation(formula = len ~ dose) cor 1 0.803 Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations. Given that the outcome variable len and explanatory variable dose are both numerical, we’ll use a scatterplot to display their relationship. How can we incorporate the categorical variable supp, however? By mapping the variable supp to the color aesthetic, thereby creating a colored scatterplot. The following code is similar to the code that created the scatterplot of proportion black nose over lion age in Figure 8.3, but with color = supp added to the aes()thetic mapping. ggplot(ToothGrowth, aes(x = dose, y = len, color = supp)) + geom_point() + labs(x = &quot;Dose&quot;, y = &quot;Tooth length&quot;, color = &quot;Supplement&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 10.1: Colored scatterplot of relationship of teaching and beauty scores. In the resulting Figure 10.1, observe that ggplot() assigns a default in red/blue color scheme to the points and to the lines associated with the two levels of supp: OJ and VC. Furthermore, the geom_smooth(method = \"lm\", se = FALSE) layer automatically fits a different regression line for each group. We notice an interesting trend. While both regression lines are positively sloped with dose levels (i.e., guinea pigs receiving higher doses of vitamin C tend to have longer teeth), the slope for dose for the VC guinea pigs is more positive. 10.1.2 Interaction model Let’s now quantify the relationship of our outcome variable \\(y\\) and the two explanatory variables using one type of multiple regression model known as an interaction model. We’ll explain where the term “interaction” comes from at the end of this section. In particular, we’ll write out the equation of the two regression lines in Figure 10.1 using the values from a regression table. Before we do this, however, let’s go over a brief refresher of regression when you have a categorical explanatory variable \\(x\\). Recall in Subsection 8.2.2 we fit a regression model for countries’ life expectancies as a function of which continent the country was in. In other words, we had a numerical outcome variable \\(y\\) = lifeExp and a categorical explanatory variable \\(x\\) = continent which had 5 levels: Africa, Americas, Asia, Europe, and Oceania. Let’s re-display the regression table you saw in Table 8.8: TABLE 10.2: Regression table for life expectancy as a function of continent term estimate std_error statistic p_value lower_ci upper_ci intercept 54.8 1.02 53.45 0 52.8 56.8 continent: Americas 18.8 1.80 10.45 0 15.2 22.4 continent: Asia 15.9 1.65 9.68 0 12.7 19.2 continent: Europe 22.8 1.70 13.47 0 19.5 26.2 continent: Oceania 25.9 5.33 4.86 0 15.4 36.5 Recall our interpretation of the estimate column. Since Africa was the “baseline for comparison” group, the intercept term corresponds to the mean life expectancy for all countries in Africa of 54.8 years. The other four values of estimate correspond to “offsets” relative to the baseline group. So, for example, the “offset” corresponding to the Americas is +18.8 as compared to the baseline for comparison group Africa. In other words, the average life expectancy for countries in the Americas is 18.8 years higher. Thus the mean life expectancy for all countries in the Americas is 54.8 + 18.8 = 73.6. The same interpretation holds for Asia, Europe, and Oceania. Going back to our multiple regression model for tooth length using dose and supplement in Figure 10.1, we generate the regression table using the same two-step approach from Chapter 8: we first “fit” the model using the lm() “linear model” function and then we apply the get_regression_table() function. This time, however, our model formula won’t be of the form y ~ x, but rather of the form y ~ x1 * x2. In other words, our two explanatory variables x1 and x2 are separated by a * sign: # Fit regression model: len_model_interaction &lt;- lm(len ~ dose * supp, data = ToothGrowth) # Get regression table: get_regression_table(len_model_interaction) TABLE 10.3: Regression table for interaction model term estimate std_error statistic p_value lower_ci upper_ci intercept 11.55 1.58 7.30 0.000 8.382 14.72 dose 7.81 1.20 6.53 0.000 5.417 10.21 supp: VC -8.26 2.24 -3.69 0.001 -12.735 -3.77 dose:suppVC 3.90 1.69 2.31 0.025 0.518 7.29 Looking at the regression table output in Table 10.3, there are four rows of values in the estimate column. While it is not immediately apparent, using these four values we can write out the equations of both lines in Figure 10.1. First, since the word OJ comes alphabetically before VC, the orange juice supplement is the “baseline for comparison” group. Thus, intercept is the intercept for only the orange juice recipients. This holds similarly for dose. It is the slope for dose for only the OJ recipients. Thus, the red regression line in Figure 10.1 has an intercept of 11.55 and slope for dose of 7.811. What about the intercept and slope for dose of the ascorbic acid recipients in the blue line in Figure 10.1? This is where our notion of “offsets” comes into play once again. The value for suppVC of -8.255 is not the intercept for the ascorbic acid recipients, but rather the offset in intercept for these guinea pigs relative to orange juice recipients. The intercept for the ascorbic acid recipients is intercept + suppVC = 11.55 + (-8.255) = 11.55 - 8.255 = 3.295. Similarly, dose:suppVC = 3.904 is not the slope for dose for the ascorbic acid recipients, but rather the offset in slope for those guinea pigs. Therefore, the slope for dose for the ascorbic acid recipients is dose + dose:suppVC \\(= 7.811 + 3.904 = 11.715\\). Thus, the blue regression line in Figure 10.1 has intercept 3.295 and slope for dose of 11.715. Let’s summarize these values in Table 10.4 and focus on the two slopes for dose: TABLE 10.4: Comparison of intercepts and slopes for interaction model Supplement Intercept Slope for dose Orange juice 11.550 7.811 Ascorbic acid 3.295 11.715 Since the slope for dose for the orange recipients was 7.811, it means that on average, a guinea pig who receives one unit more would have a tooth length that is 7.811 units higher. For the ascorbic acid recipients, however, the corresponding associated increase was on average 11.715 units. While both slopes for dose were positive, the slope for dose for the VC recipients is more positive. This is consistent with our observation from Figure 10.1, that this model is suggesting that dose impacts tooth length for ascorbic acid (VC) recipients more than for orange juice recipients. Let’s now write the equation for our regression lines, which we can use to compute our fitted values \\(\\widehat{y} = \\widehat{\\text{len}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{len}} &amp;= b_0 + b_{\\text{dose}} \\cdot \\text{dose} + b_{\\text{VC}} \\cdot \\mathbb{1}_{\\text{is VC}}(x) + b_{\\text{dose,VC}} \\cdot \\text{dose} \\cdot \\mathbb{1}_{\\text{is VC}}(x)\\\\ &amp;= 11.55 + 7.811 \\cdot \\text{dose} - 8.255 \\cdot \\mathbb{1}_{\\text{is VC}}(x) + 3.904 \\cdot \\text{dose} \\cdot \\mathbb{1}_{\\text{is VC}}(x) \\end{aligned} \\] Whoa! That’s even more daunting than the equation you saw for the life expectancy as a function of continent in Subsection 8.2.2! However, if you recall what an “indicator function” does, the equation simplifies greatly. In the previous equation, we have one indicator function of interest: \\[ \\mathbb{1}_{\\text{is VC}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{supp } x \\text{ is VC} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Second, let’s match coefficients in the previous equation with values in the estimate column in our regression table in Table 10.3: \\(b_0\\) is the intercept = 11.55 for the orange juice recipients \\(b_{\\text{dose}}\\) is the slope for dose = 7.811 for the orange juice recipients \\(b_{\\text{VC}}\\) is the offset in intercept = -8.255 for the ascorbic acid (VC) recipients \\(b_{\\text{dose,VC}}\\) is the offset in slope for dose = 3.904 for the ascorbic acid (VC) recipients Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{len}}\\) for orange juice recipients. Since for orange juice recipients \\(\\mathbb{1}_{\\text{is VC}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{len}} &amp;= 11.55 + 7.811 \\cdot \\text{dose} - 8.255 \\cdot 0 + 3.904 \\cdot \\text{dose} \\cdot 0\\\\ &amp;= 11.55 + 7.811 \\cdot \\text{dose} - 0 + 0\\\\ &amp;= 11.55 - -7.811 \\cdot \\text{dose}\\\\ \\end{aligned} \\] which is the equation of the red regression line in Figure 10.1 corresponding to the orange juice recipients in Table 10.4. Correspondingly, since for ascorbic acid (VC) recipients \\(\\mathbb{1}_{\\text{is VC}}(x)\\) = 1, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{len}} &amp;= 11.55 + 7.811 \\cdot \\text{dose} - 8.255 + 3.904 \\cdot \\text{dose}\\\\ &amp;= (11.55 - 8.255) + (`slope_OJ` + 3.904) * \\text{dose}\\\\ &amp;= 3.295 + 11.715 \\cdot \\text{dose}\\\\ \\end{aligned} \\] which is the equation of the blue regression line in Figure 10.1 corresponding to the ascorbic acid (VC) recipients in Table 10.4. Phew! That was a lot of arithmetic! Don’t fret, however, this is as hard as modeling will get in this book. If you’re still a little unsure about using indicator functions and using categorical explanatory variables in a regression model, we highly suggest you re-read Subsection 8.2.2. This involves only a single categorical explanatory variable and thus is much simpler. Before we end this section, we explain why we refer to this type of model as an “interaction model.” The \\(b_{\\text{dose,VC}}\\) term in the equation for the fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{len}}\\) is what’s known in statistical modeling as an “interaction effect.” The interaction term corresponds to the dose:suppVC = 3.904 in the final row of the regression table in Table 10.3. We say there is an interaction effect if the associated effect of one variable depends on the value of another variable. That is to say, the two variables are “interacting” with each other. Here, the associated effect of the variable dose depends on the value of the other variable supp. The difference in slopes for dose of +3.904 of ascorbic acid (VC) recipients relative to orange juice recipients shows this. Another way of thinking about interaction effects on tooth length is as follows. For a given pig, there might be an associated effect of their dose by itself, there might be an associated effect of their supplement by itself, but when dose and supplement are considered together there might be an additional effect above and beyond the two individual effects. 10.1.3 Parallel slopes model When creating regression models with one numerical and one categorical explanatory variable, we are not just limited to interaction models as we just saw. Another type of model we can use is known as a parallel slopes model. Unlike interaction models where the regression lines can have different intercepts and different slopes, parallel slopes models still allow for different intercepts but force all lines to have the same slope. The resulting regression lines are thus parallel. Let’s visualize the best-fitting parallel slopes model to ToothGrowth. Unfortunately, the geom_smooth() function in the ggplot2 package does not have a convenient way to plot parallel slopes models. Evgeni Chasnovski thus created a special purpose function called geom_parallel_slopes() that is included in the moderndive package. You won’t find geom_parallel_slopes() in the ggplot2 package, so to use it, you will need to load both the ggplot2 and moderndive packages. Using this function, let’s now plot the parallel slopes model for tooth lengt. Notice how the code is identical to the code that produced the visualization of the interaction model in Figure 10.1, but now the geom_smooth(method = \"lm\", se = FALSE) layer is replaced with geom_parallel_slopes(se = FALSE). ggplot(ToothGrowth, aes(x = dose, y = len, color = supp)) + geom_point() + labs(x = &quot;Dose&quot;, y = &quot;Tooth length&quot;, color = &quot;Supplement type&quot;) + geom_parallel_slopes(se = FALSE) FIGURE 10.2: Parallel slopes model of len with dose and supp. Observe in Figure 10.2 that we now have parallel lines corresponding to the orange juice and ascorbic acid (VC) recipients, respectively: here they have the same positive slope. This is telling us that guinea pigs who receive higher doses of vitamin C will tend to have longer teeth than guinea pigs who receive less. Furthermore, since the lines are parallel, the associated gain for higher dose is assumed to be the same for both orange juice and ascorbic acid (VC) recipients. Also observe in Figure 10.2 that these two lines have different intercepts as evidenced by the fact that the blue line corresponding to the ascorbic acid (VC) recipients is lower than the red line corresponding to the orange juice recipients. This is telling us that irrespective of dose, orange juice recipients tended to have longer teeth than ascorbic acid (VC) recipients. In order to obtain the precise numerical values of the two intercepts and the single common slope, we once again “fit” the model using the lm() “linear model” function and then apply the get_regression_table() function. However, unlike the interaction model which had a model formula of the form y ~ x1 * x2, our model formula is now of the form y ~ x1 + x2. In other words, our two explanatory variables x1 and x2 are separated by a + sign: # Fit regression model: len_model_parallel_slopes &lt;- lm(len ~ dose + supp, data = ToothGrowth) # Get regression table: get_regression_table(len_model_parallel_slopes) TABLE 10.5: Regression table for parallel slopes model term estimate std_error statistic p_value lower_ci upper_ci intercept 9.27 1.282 7.23 0.000 6.71 11.84 dose 9.76 0.877 11.13 0.000 8.01 11.52 supp: VC -3.70 1.094 -3.38 0.001 -5.89 -1.51 Similar to the regression table for the interaction model from Table 10.3, we have an intercept term corresponding to the intercept for the “baseline for comparison” orange juice (OJ) group and a suppVC term corresponding to the offset in intercept for the ascorbic acid (VC) recipients relative to orange juice recipients. In other words, in Figure 10.2 the red regression line corresponding to the orange juice recipients has an intercept of 9.27 while the blue regression line corresponding to the ascorbic acid (VC) recipients has an intercept of 9.27 + -3.7 = 5.57. Unlike in Table 10.3, however, we now only have a single slope for dose of 9.764. This is because the model dictates that both the orange juice and ascorbic acid (VC) recipients have a common slope for dose. This is telling us that a guinea pig who received a vitamin C dose one unit higher has tooth length that is on average 9.764 units longer. This benefit for receiving a higher vitamin C dose applies equally to both orange juice and ascorbic acid (VC) recipients. Let’s summarize these values in Table 10.6, noting the different intercepts but common slopes: TABLE 10.6: Comparison of intercepts and slope for parallel slopes model Supplement Intercept Slope for dose Orange juice (OJ) 9.27 9.764 Ascorbic acid (VC) 5.57 9.764 Let’s now write the equation for our regression lines, which we can use to compute our fitted values \\(\\widehat{y} = \\widehat{\\text{len}}\\). \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{len}} &amp;= b_0 + b_{\\text{dose}} \\cdot \\text{dose} + b_{\\text{VC}} \\cdot \\mathbb{1}_{\\text{is VC}}(x)\\\\ &amp;= 9.27 + 9.764 \\cdot \\text{dose} + -3.7 \\cdot \\mathbb{1}_{\\text{is VC}}(x) \\end{aligned} \\] Let’s put this all together and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{len}}\\) for orange juice recipients. Since for orange juice recipients the indicator function \\(\\mathbb{1}_{\\text{is VC}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{len}} &amp;= 9.27 + 9.764 \\cdot \\text{dose} + -3.7 \\cdot 0\\\\ &amp;= 9.27 + 9.764 \\cdot \\text{dose} \\end{aligned} \\] which is the equation of the red regression line in Figure 10.2 corresponding to the orange juice recipients. Correspondingly, since for ascorbic acid (VC) recipients the indicator function \\(\\mathbb{1}_{\\text{is VC}}(x)\\) = 1, the previous equation becomes \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{len}} &amp;= 9.27 9.764 \\cdot \\text{dose} + -3.7 \\cdot 1\\\\ &amp;= (9.27 + -3.7) - -9.764 \\cdot \\text{dose}\\\\ &amp;= 5.57+ 9.764 \\cdot \\text{dose} \\end{aligned} \\] which is the equation of the blue regression line in Figure 10.2 corresponding to the ascorbic acid (VC) recipients. Great! We’ve considered both an interaction model and a parallel slopes model for our data. Let’s compare the visualizations for both models side-by-side in Figure 10.3. FIGURE 10.3: Comparison of interaction and parallel slopes models. At this point, you might be asking yourself: “Why would we ever use a parallel slopes model?”. Looking at the left-hand plot in Figure 10.3, the two lines definitely do not appear to be parallel, so why would we force them to be parallel? For this data, we agree! It can easily be argued that the interaction model on the left is more appropriate. However, in the upcoming Subsection 10.3.1 on model selection, we’ll present an example where it can be argued that the case for a parallel slopes model might be stronger. 10.1.4 Observed/fitted values and residuals For brevity’s sake, in this section we’ll only compute the observed values, fitted values, and residuals for the interaction model which we saved in len_model_interaction. You’ll have an opportunity to study the corresponding values for the parallel slopes model in the upcoming Learning check. Say, you have a guinea pig who receives orange juice of 2 mg/day. What fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{len}}\\) would our model yield? Say, you have another guinea pig who receives ascorbic acid of 1 mg/day. What would their fitted value \\(\\widehat{y}\\) be? We answer this question visually first for the orange juice recipient by finding the intersection of the red regression line and the vertical line at \\(x\\) = dose = 2. We mark this value with a large red dot in Figure 10.4. Similarly, we can identify the fitted value \\(\\widehat{y}\\) = \\(\\widehat{\\text{len}}\\) for the ascorbic acid recipient by finding the intersection of the blue regression line and the vertical line at \\(x\\) = dose = 1. We mark this value with a large blue dot in Figure 10.4. FIGURE 10.4: Fitted values for two supplement doses. What are these two values of \\(\\widehat{y}\\) = \\(\\widehat{\\text{len}}\\) precisely? We can use the equations of the two regression lines we computed in Subsection 10.1.2, which in turn were based on values from the regression table in Table 10.3: For all orange juice recipients: \\(\\widehat{y} = \\widehat{\\text{len}} = 11.55 - -7.811 \\cdot \\text{dose}\\) For all ascorbic acid (VC) recipients: \\(\\widehat{y} = \\widehat{\\text{len}} = 3.295 - -11.715 \\cdot \\text{dose}\\) So our fitted values would be: \\(11.55 - -7.811 \\cdot 2 = 27.17\\) and \\(3.295 - -11.715 \\cdot 1 = 15.01\\), respectively. Now what if we want the fitted values not just for these two guinea pigs, but for all 60 guinea pigs included in the ToothGrowth data frame? Doing this by hand would be long and tedious! This is where the get_regression_points() function from the moderndive package can help: it will quickly automate the above calculations for all 60 guinea pigs. We present a preview of the first 5 rows for each supplement type out of 60 in Table 10.7. regression_points &lt;- get_regression_points(len_model_interaction) regression_points %&gt;% group_by(supp) %&gt;% slice_head(n=5) TABLE 10.7: Regression points (First 10 out of 463 courses) ID len dose supp len_hat residual 31 15.2 0.5 OJ 15.46 -0.256 32 21.5 0.5 OJ 15.46 6.044 33 17.6 0.5 OJ 15.46 2.144 34 9.7 0.5 OJ 15.46 -5.756 35 14.5 0.5 OJ 15.46 -0.956 1 4.2 0.5 VC 9.15 -4.953 2 11.5 0.5 VC 9.15 2.347 3 7.3 0.5 VC 9.15 -1.853 4 5.8 0.5 VC 9.15 -3.353 5 6.4 0.5 VC 9.15 -2.753 It turns out that the first five orange juice recipients received 0.5 mg/day of vitamin C. The resulting \\(\\widehat{y}\\) = \\(\\widehat{\\text{len}}\\) fitted values are in the len_hat column. Furthermore, the get_regression_points() function also returns the residuals \\(y-\\widehat{y}\\). Notice, for example, the second and third guinea pigs receiving orange juice had positive residuals, indicating that the actual tooth growth values were greater than their fitted length of 15.46. On the other hand, the first and fourth guinea pigs had negative residuals, indicating that the actual tooth growth values were less than 15.46. Learning check (LC9.1) Compute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in len_model_parallel_slopes. 10.2 Two categorical explanatory variables Let’s now switch gears and consider multiple regression models where instead of one numerical and one categorical explanatory variable, we now have two categorical explanatory variables. The dataset we’ll use is from [Analysis of Biological Data] by Whitlock and Schluter. Its accompanying abd R package contains the IntertidalAlgae dataset that we’ll use next. This dataset is from a study looking at the effect of herbivores and height above low tide, and the interaction between these factors, on abundance of a red intertidal alga. In this section, we’ll fit a regression model where we have A numerical outcome variable \\(y\\), sqrt.area, the square root of the area (in cm2) covered by red algae at the experiment’s end, and Two explanatory variables: One categorical explanatory variable \\(x_1\\), herbivores, absence or presence of herbivores Another categorical explanatory variable \\(x_2\\), height, the height of the experimental plot relative to the tide levels. 10.2.1 Exploratory data analysis Let’s examine the Intertidal dataset using RStudio’s spreadsheet viewer or glimpse(). library(abd) glimpse(IntertidalAlgae) Rows: 64 Columns: 3 $ height &lt;fct&gt; low, low, low, low, low, low, low, low, low, low, low, low,… $ herbivores &lt;fct&gt; minus, minus, minus, minus, minus, minus, minus, minus, min… $ sqrt.area &lt;dbl&gt; 9.406, 34.468, 46.673, 16.642, 24.377, 38.351, 33.777, 61.0… Furthermore, let’s look at a random sample of five out of the 64 plots in Table 10.8. Once again, note that due to the random nature of the sampling, you will likely end up with a different subset of five rows. IntertidalAlgae %&gt;% sample_n(size = 5) TABLE 10.8: Random sample of 5 experimental plots height herbivores sqrt.area low minus 46.17 low minus 41.79 mid minus 45.54 low plus 27.64 mid minus 3.15 Now that we’ve looked at the raw values in our IntertidalAlgae data frame and got a sense of the data, let’s move on to the next common step in an exploratory data analysis: computing summary statistics. Let’s use the skim() function from the skimr package: IntertidalAlgae %&gt;% skim() Observe the summary statistics for the outcome variable sqrt.area: the mean and median sqrt.area are 22.84 and 22.25, respectively, and that 25% of plots had values of 5.92 or less. Now let’s look at the explanatory variables herbivores and height. First, let’s visualize the relationship of the outcome variable with each of the two explanatory variables in separate box plots with an overlaid scatterplot to see the individual data points in Figure 10.5. ggplot(IntertidalAlgae, aes(x = herbivores, y = sqrt.area)) + geom_boxplot() + geom_jitter(width = 0.1) + labs(x = &quot;Herbivore treatment&quot;, y = &quot;Square root of area covered&quot;, title = &quot;Area coverage and herbivore treatment&quot;) ggplot(IntertidalAlgae, aes(x = height, y = sqrt.area)) + geom_boxplot() + geom_jitter(width = 0.1) + labs(x = &quot;Height relative to low tide&quot;, y = &quot;Square root of area covered&quot;, title = &quot;Area coverage and tidal location&quot;) FIGURE 10.5: Relationship between red algae coverage and herbivores/height. It seems that the herbivore treatment, but not tidal location, has some effect on its own on red algae coverage. With respect to herbivore treatment, the median of \\(y\\) is almost double in the presence of herbivores (plus) as compared to the absence (minus). With respect to height above low tide, the median of \\(y\\) is nearly the same in the low tide group as in the mid tide group. However, the two plots in Figure 10.5 only focus on the relationship of the outcome variable with each of the two explanatory variables separately. To visualize the joint relationship of all three variables simultaneously, let’s make just a scatterplot and indicate the other explanatory variable by color. Each of the 64 observations in the IntertidalAlgae data frame are marked with a point where The numerical outcome variable \\(y\\) sqrt.area is on the vertical axis. The first categorical explanatory variable \\(x_1\\) herbivores is on the horizontal axis. The second categorical explanatory variable \\(x_2\\) height is indicated by color. ggplot(IntertidalAlgae, aes(x = herbivores, y = sqrt.area, color = height, group = height)) + geom_jitter(width = 0.1) + labs(x = &quot;Herbivore treatment&quot;, y = &quot;Square root of area covered&quot;, title = &quot;Area coverage and herbivore treatment&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) FIGURE 10.6: Relationship between red algae coverage and herbivores plus height. Furthermore, we also include the regression lines for each height level (low or mid). Recall from Subsection 8.3.2 that regression lines are “best-fitting” in that of all possible lines we can draw through a cloud of points, the regression line minimizes the sum of squared residuals. This concept also extends to models with two categorical explanatory variables. Learning check (LC9.2) Prepare a new plot with the same outcome variable \\(y\\) sqrt.area but with the second explanatory variable \\(x_2\\) height on the horizontal axis and the first explanatory variable \\(x_1\\) on the color layers. How does this plot compare with the previous one? 10.2.2 Regression lines Let’s now fit a regression model and get the regression table corresponding to the regression line in Figure 10.6. As we have done throughout Chapter 8 and this chapter, we use our two-step process to obtain the regression table for this model in Table 10.9. # Fit regression model: area_model1 &lt;- lm(sqrt.area ~ herbivores * height, data = IntertidalAlgae) # Get regression table: get_regression_table(area_model1) TABLE 10.9: Multiple regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 32.9 3.86 8.54 0.000 25.2 40.627 herbivores: plus -22.5 5.45 -4.13 0.000 -33.4 -11.604 height: mid -10.4 5.45 -1.91 0.061 -21.3 0.476 herbivores: plus:heightmid 25.6 7.71 3.32 0.002 10.2 41.003 We first “fit” the linear regression model using the lm(y ~ x1 * x2, data) function and save it in area_model1. We get the regression table by applying the get_regression_table() function from the moderndive package to area_model1. Let’s interpret the three values in the estimate column. First, the intercept value is 32.9, and this will be our base value in comparison to other groups. This intercept represents the mean sqrt.area for a plot who has herbivores absent and height of low tide, and this corresponds to where the red line (low) intersects \\(x\\) = minus. Second, the herbivoresplus value is -22.5. Taking into account all the other explanatory variables in our model, the addition of herbivoreshas an associated decrease on average of 22.5 in sqrt.area. We preface our interpretation with the statement, “taking into account all the other explanatory variables in our model.” Here, by all other explanatory variables we mean height and the interaction between height and herbivores. We do this to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model at the same time. Third, heightmid is -10.4 Taking into account all other explanatory variables in our model, plots with a height of mid tide, have an associated sqrt.area decrease of, on average, 10.4 cm. And finally, herbivoresplus:heightmid is 25.6. Taking into account all other explanatory variables in our model, the addition of herbivores and a height of mid tide, have an associated sqrt.area increase of, on average, 25.6 cm. Putting these results together, the equation of the regression lines for the interaction model to give us fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{sqrt.area}}\\) is: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_{\\text{1,2}} \\cdot x_1 \\cdot x_2\\\\ \\widehat{\\text{sqrt.area}} &amp;= b_0 + b_{\\text{herbivores}} \\cdot \\mathbb{1}_{\\text{is plus}}(x) + b_{\\text{height}} \\cdot \\mathbb{1}_{\\text{is mid}}(x) \\\\ &amp;\\text{ } + b_{\\text{herbivores,height}} \\cdot \\mathbb{1}_{\\text{is plus}}(x) \\cdot \\mathbb{1}_{\\text{is mid}}(x)\\\\ &amp;= 32.9 - 22.5 \\cdot\\mathbb{1}_{\\text{is plus}}(x) - 10.4 \\cdot\\mathbb{1}_{\\text{is mid}}(x) \\\\ &amp;\\text{ } + 25.6 \\cdot \\mathbb{1}_{\\text{is plus}}(x) \\cdot \\mathbb{1}_{\\text{is mid}}(x) \\end{aligned} \\] Yes, the equation is again quite daunting, but as we saw above, once you recall what an “indicator function” does, the equation simplifies greatly. In this equation, we now have two indicator functions of interest: \\[ \\mathbb{1}_{\\text{is plus}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{herbivores } x \\text{ is plus} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\mathbb{1}_{\\text{is mid}}(x) = \\left\\{ \\begin{array}{ll} 1 &amp; \\text{if } \\text{height } x \\text{ is mid} \\\\ 0 &amp; \\text{otherwise}\\end{array} \\right. \\] Okay, let’s put this all together again and compute the fitted value \\(\\widehat{y} = \\widehat{\\text{sqrt.area}}\\) for red algae minus herbivores and at low level. In this case \\(\\mathbb{1}_{\\text{is plus}}(x)\\) = 0 and \\(\\mathbb{1}_{\\text{is mid}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{\\text{sqrt.area}} &amp;= 32.9 - 22.5 \\cdot 0 - 10.4 \\cdot0 + 25.6 \\cdot 0 \\cdot 0 \\\\ &amp;= 32.9 + 0 + 0 + 0\\\\ &amp;= 32.9 \\end{aligned} \\] which is the left end of the red regression line in Figure 10.1 corresponding to the intercept in Table 10.4. Correspondingly, for red algae plus herbivores and at low level, since \\(\\mathbb{1}_{\\text{is plus}}(x)\\) = 1 and \\(\\mathbb{1}_{\\text{is mid}}(x)\\) = 0, the previous equation becomes \\[ \\begin{aligned} \\widehat{\\text{sqrt.area}} &amp;= 32.9 - 22.5 \\cdot 1 - 10.4 \\cdot0 + 25.6 \\cdot 1 \\cdot 0 \\\\ &amp;= 32.9 - 22.5 + 0 +0 \\\\ &amp;= 10.4 \\end{aligned} \\] which is the right end of the red regression line in Figure 10.1. Moving on, for red algae minus herbivores and at mid level, since \\(\\mathbb{1}_{\\text{is plus}}(x)\\) = 0 and \\(\\mathbb{1}_{\\text{is mid}}(x)\\) = 1, the equation becomes \\[ \\begin{aligned} \\widehat{\\text{sqrt.area}} &amp;= 32.9 - 22.5 \\cdot 0 - 10.4 \\cdot 1 + 25.6 \\cdot 0 \\cdot 1 \\\\ &amp;= 32.9 + 0 - 10.4 + 0\\\\ &amp;= 22.5 \\end{aligned} \\] which is the left end of the blue regression line in Figure 10.1. And finally, for red algae plus herbivores and at mid level, since \\(\\mathbb{1}_{\\text{is plus}}(x)\\) = 1 and \\(\\mathbb{1}_{\\text{is mid}}(x)\\) = 1, the equation becomes \\[ \\begin{aligned} \\widehat{\\text{sqrt.area}} &amp;= 32.9 - 22.5 \\cdot 1 - 10.4 \\cdot 1 + 25.6 \\cdot 1 \\cdot 1 \\\\ &amp;= 32.9 - 22.5 - 10.4 + 25.6\\\\ &amp;= 25.6 \\end{aligned} \\] which is the right end of the blue regression line in Figure 10.1. That was a lot of arithmetic, but you made it! If you’re still a little unsure about using indicator functions and categorical explanatory variables in a regression model, you should re-read Subsection 8.2.2. Recall however in the right-hand plot of Figure 10.5 that when plotting the relationship between sqrt.area and height in isolation, there appeared to be a positive relationship. In the last discussed multiple regression, however, when jointly modeling the relationship between sqrt.area, herbivores, and height, there appears to be a negative relationship of sqrt.area and height as evidenced by the negative offset for height of -10.4. What explains these contradictory results? A phenomenon known as Simpson’s Paradox, whereby overall trends that exist in aggregate either disappear or reverse when the data are broken down into groups. Learning check (LC9.3) Fit a new simple linear regression for a non-interaction model using lm(sqrt.area ~ herbivores + height, data = IntertidalAlgae). Get information about the “best-fitting” regression lines from the regression table by applying the get_regression_table() function. How do the regression results compare to those for the interaction model above? 10.2.3 Observed/fitted values and residuals Let’s compute all fitted values and residuals for our interaction model using the get_regression_points() function and present only the first two rows of output for each combination of explanatory variables in Table 10.10. Remember that the coordinates of each of the blue points in Figure 10.6 can be found in the height, herbivores, and sqrt.area columns. The fitted values on the regression lines are found in the sqrt.area_hat column and are computed using our equation in the previous section: \\[ \\begin{aligned} \\widehat{y} = \\widehat{\\text{sqrt.area}} &amp;= 32.9 - 22.5 \\cdot\\mathbb{1}_{\\text{is plus}}(x) - 10.4 \\cdot\\mathbb{1}_{\\text{is mid}}(x) \\\\ &amp;\\text{ } + 25.6 \\cdot \\mathbb{1}_{\\text{is plus}}(x) \\cdot \\mathbb{1}_{\\text{is mid}}(x) \\end{aligned} \\] get_regression_points(area_model1) %&gt;% group_by(herbivores, height) %&gt;% slice_head(n=2) TABLE 10.10: Regression points (First 2 samples of each treatment combination) ID sqrt.area herbivores height sqrt.area_hat residual 1 9.406 minus low 32.9 -23.51 2 34.468 minus low 32.9 1.55 33 0.707 minus mid 22.5 -21.78 34 24.951 minus mid 22.5 2.47 17 11.977 plus low 10.4 1.57 18 14.471 plus low 10.4 4.07 49 0.707 plus mid 25.6 -24.84 50 32.206 plus mid 25.6 6.66 ###Which model, without or with interaction, fits the data better? Here we compare the mean of the observed \\(y\\) values (Obs) of each of the four groups with their mean fitted \\(\\widehat{y}\\) values for the interaction (Int) and non-interaction (Non) models: area_model2 &lt;- lm(sqrt.area ~ herbivores + height, data = IntertidalAlgae) means_Obs &lt;- IntertidalAlgae %&gt;% group_by(herbivores, height) %&gt;% summarise(mean_Obs=mean(sqrt.area)) means_Int &lt;- get_regression_points(area_model1) %&gt;% group_by(herbivores, height) %&gt;% summarise(mean_hat_Int = mean(sqrt.area_hat)) means_Non &lt;- get_regression_points(area_model2) %&gt;% group_by(herbivores, height) %&gt;% summarise(mean_hat_Non = mean(sqrt.area_hat)) means_table &lt;- means_Obs %&gt;% inner_join(means_Int) %&gt;% inner_join(means_Non) means_table Joining, by = c(&quot;herbivores&quot;, &quot;height&quot;) Joining, by = c(&quot;herbivores&quot;, &quot;height&quot;) TABLE 10.11: Observed means compared to fitted means for both models herbivores height mean_Obs mean_hat_Int mean_hat_Non minus low 32.9 32.9 26.5 minus mid 22.5 22.5 28.9 plus low 10.4 10.4 16.8 plus mid 25.6 25.6 19.2 It’s clear from this table that the predicted (fitted) means for the interaction model (mean_hat_Int) are much closer to the observed mean (mean_Obs) than for the non-interaction model (mean_hat_Non). This is even more apparent when we overlay our scatterplot with the observed and predicted (fitted) means for each model, along with colored crossbars to mark the observed mean values (mean_Obs) for each of the four groups: FIGURE 10.7: Comparison of interaction and parallel slopes model for red algae coverage. 10.3 Related topics 10.3.1 Model selection using visualizations When should we use an interaction model versus a parallel slopes (non-interaction) model? Recall in Sections 10.1.2 and 10.1.3 we fit both interaction and parallel slopes models for the outcome variable \\(y\\) (tooth length) using a numerical explanatory variable \\(x_1\\) (dose) and a categorical explanatory variable \\(x_2\\) (supplement type). We compared these models in Figure 10.3, which we display again now. FIGURE 10.8: Previously seen comparison of interaction and parallel slopes models. A lot of you might have asked yourselves: “Why would I force the lines to have parallel slopes (as seen in the right-hand plot) when they clearly have different slopes (as seen in the left-hand plot)?”. The answer lies in a philosophical principle known as “Occam’s Razor.” It states that, “all other things being equal, simpler solutions are more likely to be correct than complex ones.” When viewed in a modeling framework, Occam’s Razor can be restated as, “all other things being equal, simpler models are to be preferred over complex ones.” In other words, we should only favor the more complex model if the additional complexity is warranted. Let’s revisit the equations for the regression line for both the interaction and parallel slopes model: \\[ \\begin{aligned} \\text{Interaction} &amp;: \\widehat{y} = \\widehat{\\text{len}} = b_0 + b_{\\text{dose}} \\cdot \\text{dose} + b_{\\text{VC}} \\cdot \\mathbb{1}_{\\text{is VC}}(x) + \\\\ &amp; \\qquad b_{\\text{dose,VC}} \\cdot \\text{dose} \\cdot \\mathbb{1}_{\\text{is VC}}\\\\ \\text{Parallel slopes} &amp;: \\widehat{y} = \\widehat{\\text{len}} = b_0 + b_{\\text{dose}} \\cdot \\text{dose} + b_{\\text{VC}} \\cdot \\mathbb{1}_{\\text{is VC}}(x) \\end{aligned} \\] The interaction model is “more complex” in that there is an additional \\(b_{\\text{dose,VC}} \\cdot \\text{dose} \\cdot \\mathbb{1}_{\\text{is VC}}\\) interaction term in the equation not present for the parallel slopes model. Or viewed alternatively, the regression table for the interaction model in Table 10.3 has four rows, whereas the regression table for the parallel slopes model in Table 10.5 has three rows. The question becomes: “Is this additional complexity warranted?”. In this case, it can be argued that this additional complexity is warranted, as evidenced by the two converging regression lines in the left-hand plot of Figure 10.8. However, let’s consider an example where the additional complexity might not be warranted. Let’s consider the MoleRats data included in the moderndive package which contains data on energy expenditure in two castes of mole rats. For more details, read the help file for this data by running ?MoleRats in the console. Let’s model the numerical outcome variable \\(y\\), ln.energy, log-transformed energy expenditure for a given mole rat, as a function of two explanatory variables: A numerical explanatory variable \\(x_1\\), ln.mass, the log-transformed mass of the mole rat and A categorical explanatory variable \\(x_2\\), the caste of the rat, either lazy or worker. Let’s create visualizations of both the interaction and parallel slopes model once again and display the output in Figure 10.9. Recall from Subsection 10.1.3 that the geom_parallel_slopes() function is a special purpose function included in the moderndive package, since the geom_smooth() method in the ggplot2 package does not have a convenient way to plot parallel slopes models. # Interaction model ggplot(MoleRats, aes(x = ln.mass, y = ln.energy, color = caste)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Log body mass&quot;, y = &quot;Log energy expenditure&quot;, color = &quot;Caste&quot;, title = &quot;Interaction model&quot;) # Parallel slopes model ggplot(MoleRats, aes(x = ln.mass, y = ln.energy, color = caste)) + geom_point(alpha = 0.5) + geom_parallel_slopes(se = FALSE) + labs(x = &quot;Log body mass&quot;, y = &quot;Log energy expenditure&quot;, color = &quot;Caste&quot;, title = &quot;Parallel slopes model&quot;) FIGURE 10.9: Comparison of interaction and parallel slopes models for mole rats datasets. Look closely at the left-hand plot of Figure 10.9 corresponding to an interaction model. While the slopes are indeed different, they do not differ by much and are nearly identical. Now compare the left-hand plot with the right-hand plot corresponding to a parallel slopes model. The two models don’t appear all that different. So in this case, it can be argued that the additional complexity of the interaction model is not warranted. Thus following Occam’s Razor, we should prefer the “simpler” parallel slopes model. Let’s explicitly define what “simpler” means in this case. Let’s compare the regression tables for the interaction and parallel slopes models in Tables 10.12 and 10.13. model_2_interaction &lt;- lm(ln.energy ~ ln.mass * caste, data = MoleRats) get_regression_table(model_2_interaction) TABLE 10.12: Interaction model regression table term estimate std_error statistic p_value lower_ci upper_ci intercept 1.294 1.669 0.775 0.444 -2.110 4.70 ln.mass 0.607 0.343 1.771 0.086 -0.092 1.31 caste: worker -1.571 1.952 -0.805 0.427 -5.552 2.41 ln.mass:casteworker 0.419 0.415 1.009 0.321 -0.427 1.26 model_2_parallel_slopes &lt;- lm(ln.energy ~ ln.mass + caste, data = MoleRats) get_regression_table(model_2_parallel_slopes) TABLE 10.13: Parallel slopes regression table term estimate std_error statistic p_value lower_ci upper_ci intercept -0.097 0.942 -0.103 0.919 -2.016 1.823 ln.mass 0.893 0.193 4.625 0.000 0.500 1.286 caste: worker 0.393 0.146 2.692 0.011 0.096 0.691 Observe how the regression table for the interaction model has 1 more rows (4 versus 3). This reflects the additional “complexity” of the interaction model over the parallel slopes model. Furthermore, note in Table 10.12 how the offsets for the slope ln.mass::casteworker being 0.419 is small relative to the slope for the baseline group of lazy caste of \\(1.294\\). In other words, both slopes are similarly positive: \\(1.294\\) for the lazy caste and \\(1.713\\) \\((=1.294 + 0.419)\\) for the worker caste. These results are suggesting that irrespective of caste, the relationship between mass and energy expenditure is similar and, alas, quite positive. What you have just performed is a rudimentary model selection: choosing which model fits data best among a set of candidate models. The model selection we performed used the “eyeball test”: qualitatively looking at visualizations to choose a model. In the next subsection, you’ll once again perform the same model selection, but this time using a numerical approach via the \\(R^2\\) (pronounced “R-squared”) value. 10.3.2 Model selection using R-squared At the end of the previous section in Figure 10.9 you compared an interaction model with a parallel slopes model, where both models attempted to explain \\(y\\) = the average energy expenditure of mole rats. In Tables 10.12 and 10.13, we observed that the interaction model was “more complex” in that the regression table had 4 rows versus the 3 rows of the parallel slopes model. Most importantly however, when comparing the left and right-hand plots of Figure 10.9, we observed that the two lines corresponding to lazy and worker mole rats were not that different. Given this similarity, we stated it could be argued that the “simpler” parallel slopes model should be favored. In this section, we’ll mimic the model selection we just performed using the qualitative “eyeball test”, but this time using a numerical and quantitative approach. Specifically, we’ll use the \\(R^2\\) summary statistic (pronounced “R-squared”), also called the “coefficient of determination”. But first, we must introduce one new concept: the variance of a numerical variable. We’ve previously studied two summary statistics of the spread (or variation) of a numerical variable: the standard deviation when studying the normal distribution in A.2 and the interquartile range (IQR) when studying boxplots in Section 2.7.1. We now introduce a third summary statistic of spread: the variance. The variance is merely the standard deviation squared and it can be computed in R using the var() summary function within summarize(). If you would like to see the formula, see A.1.3. Recall that to get: 1) the observed values \\(y\\), 2) the fitted values \\(\\widehat{y}\\) from a regression model, and 3) the resulting residuals \\(y - \\widehat{y}\\), we can apply the get_regression_points() function our saved model, in this case model_2_interaction: get_regression_points(model_2_interaction) # A tibble: 35 × 6 ID ln.energy ln.mass caste ln.energy_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 3.689 3.85 worker 3.671 0.018 2 2 3.689 3.989 worker 3.813 -0.125 3 3 3.689 4.111 worker 3.938 -0.25 4 4 3.664 4.174 worker 4.004 -0.34 5 5 3.871 4.248 worker 4.08 -0.208 6 6 3.85 4.263 worker 4.094 -0.244 7 7 3.932 4.344 worker 4.177 -0.245 8 8 3.689 4.489 worker 4.326 -0.637 9 9 3.951 4.511 worker 4.349 -0.397 10 10 4.111 3.951 worker 3.775 0.336 # … with 25 more rows Let’s now use the var() summary function within a summarize() to compute the variance of these three terms: get_regression_points(model_2_interaction) %&gt;% summarize(var_y = var(ln.energy), var_y_hat = var(ln.energy_hat), var_residual = var(residual)) # A tibble: 1 × 3 var_y var_y_hat var_residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.140050 0.0599682 0.0801086 Observe that the variance of \\(y\\) is equal to the variance of \\(\\widehat{y}\\) plus the variance of the residuals. But what do these three terms tell us individually? First, the variance of \\(y\\) (denoted as \\(var(y)\\)) tells us how much do mole rats differ in energy expenditure. The goal of regression modeling is to fit a model that hopefully explains this variation. In other words, we want to understand what factors explain why certain mole rats have high energy expenditures, while others have low expenditures. The variance of \\(y\\) is independent of the model; this is just data. In other words, whether we fit an interaction or parallel slopes model, \\(var(y)\\) remains the same. Second, the variance of \\(\\widehat{y}\\) (denoted as \\(var(\\widehat{y})\\)) tells us how much the fitted values from our interaction model vary. That is to say, after accounting for (1) the log mass and (2) caste of a mole rat in an interaction model, how much do our model’s explanations of log energy expenditure vary? Third, the variance of the residuals tells us how much do “the left-overs” from the model vary. Observe how the points in the left-hand plot of Figure 10.9 scatter around the two lines. Say instead all the points fell exactly on their corresponding line. Then all residuals would be zero and hence the variance of the residuals would be zero. We’re now ready to introduce \\(R^2\\): \\[ R^2 = \\frac{var(\\widehat{y})}{var(y)} \\] It is the proportion of the spread/variation of the outcome variable \\(y\\) that is explained by our model, where our model’s explanatory power is embedded in the fitted values \\(\\widehat{y}\\). Furthermore, since it can be mathematically proven that \\(0 \\leq var(\\widehat{y}) \\leq var(y)\\) (a fact we leave for an advanced class on regression), we are guaranteed that: \\[ 0 \\leq R^2 \\leq 1 \\] \\(R^2\\) can be interpreted as follows: \\(R^2\\) values of 0 tell us that our model explains 0% of the variation in \\(y\\). Say we fit a model to the mole rats data and obtained \\(R^2 = 0\\). This would be telling us that the combination of explanatory variables \\(x\\) we used and model form we chose (interaction or parallel slopes) tell us nothing about energy expenditure of a mole rat. The model is a poor fit. \\(R^2\\) values of 1 tell us that our model explains 100% of the variation in \\(y\\). Say we fit a model to the mole rats data and obtained \\(R^2 = 1\\). This would be telling us that the combination of explanatory variables \\(x\\) we used and model form we chose (interaction or parallel slopes) tell us everything we need to know about energy expenditure of a mole rat. In practice however, \\(R^2\\) values of 1 almost never occur. Think about it in the context of mole rats. There are an infinite number of factors that influence why certain mole rats expend much energy while others don’t. The idea that a human-designed statistical model can capture all the heterogeneity of all mole rats is bordering on hubris. However, even if such models are not perfect, they may still prove useful in predicting energy expenditure. A general principle of modeling we should keep in mind is a famous quote by eminent statistician George Box: “All models are wrong, but some are useful.” Let’s repeat the above calculations for the parallel slopes model and compare them in Table 10.14. TABLE 10.14: Comparing variances from interaction and parallel slopes models for mole rats data model var_y var_y_hat var_residual r_squared Interaction 0.14 0.060 0.080 0.428 Parallel slopes 0.14 0.057 0.083 0.409 Observe how the \\(R^2\\) values are near identical at around 0.409 = 40.9%. In other words, the additional complexity of the interaction model only improves our \\(R^2\\) value by a near zero amount. Thus, we are inclined to favor the “simpler” parallel slopes model. Now let’s repeat this \\(R^2\\) comparison between interaction and parallel slopes model for our models of \\(y\\) = coverage area of red algae which you visually compared in Figure 10.7. We compare these values in Table 10.15 TABLE 10.15: Comparing variances from interaction and parallel slopes models for tooth growth data model var_y var_y_hat var_residual r_squared Interaction 293 67.0 227 0.228 Parallel slopes 293 25.4 268 0.087 Observe how the \\(R^2\\) values are now very different! In other words, since the additional complexity of the interaction model over the parallel slopes model improves our \\(R^2\\) value by a relatively large amount (0.228 versus 0.087, which is an increase of about 163.4%), it could be argued that the additional complexity is warranted. As a final note, we can also use the third of our get_regression() wrapper functions, get_regression_summaries(), to quickly automate calculating \\(R^2\\) for both the interaction and parallels slopes models, first for the MoleRats data. # R-squared for interaction model: get_regression_summaries(model_2_interaction) # A tibble: 1 × 9 r_squared adj_r_squared mse rmse sigma statistic p_value df nobs &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.428 0.372 0.0778555 0.279026 0.296 7.727 0.001 3 35 # R-squared for parallel slopes model: get_regression_summaries(model_2_parallel_slopes) # A tibble: 1 × 9 r_squared adj_r_squared mse rmse sigma statistic p_value df nobs &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.409 0.372 0.0804143 0.283574 0.297 11.075 0 2 35 In addition to the \\(R^2\\) value for a model, this wrapper function also returns an adjusted-\\(R^2\\) value, which accounts for the additional feature contained in the interaction model. While \\(R^2\\) will always increase as we add additional variables to our model, the adjusted-\\(R^2\\) will only increase if the more complex model is better. You’ll notice that the adjusted-\\(R^2\\) of the interaction and the parallel slopes models for the MoleRats data is identical at 0.372. This indicates that the additional complexity of the model that accounts for an interaction between the two explanatory variables is not any better than the simpler parallel-slopes model. In contrast when we compare the interaction and parallel slopes models for the IntertidalAlgae data, we see marked increase in the adjusted-\\(R^2\\) values with the interaction model: # A tibble: 1 × 9 r_squared adj_r_squared mse rmse sigma statistic p_value df nobs &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.228 0.19 222.977 14.9324 15.422 5.912 0.001 3 64 # A tibble: 1 × 9 r_squared adj_r_squared mse rmse sigma statistic p_value df nobs &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.087 0.057 263.867 16.2440 16.639 2.892 0.063 2 64 10.4 Conclusion Chapter Learning Summary Multiple linear regression examines the relationship between a numerical response variable and multiple explanatory variables, which may be numerical and/or categorical. There is an interaction effect when the effect of an explanatory variable on the response variable depends on a second explanatory variable. Because simpler models are to be preferred (Occam’s razor), a parallel slopes (or non-#interaction) model is more appropriate if there is no evidence for an interaction effect. Data visualization, R-squared values and p-values can provide evidence for an interaction effect. 10.4.1 What’s to come? You’ve now concluded the last major part of the book on “Data Modeling with moderndive.” The closing Chapter 11 concludes this book with a review and short case studies involving real data. You’ll see how the principles in this book can help you become a great storyteller with data! "],["11-thinking-with-data.html", "Chapter 11 Tell Your Story with Data 11.1 Review 11.2 Case study: Effective data storytelling Concluding remarks", " Chapter 11 Tell Your Story with Data Recall in the Preface and at the end of chapters throughout this book, we displayed the “ModernDive flowchart” mapping your journey through this book. FIGURE 11.1: ModernDive for Life Scientists flowchart. 11.1 Review Let’s go over a refresher of what you’ve covered so far. You first got started with data in Chapter 1 where you learned about the difference between R and RStudio, started coding in R, installed and loaded your first R packages, and explored your first dataset. Then you covered the following three parts of this book: Data science with tidyverse. You assembled your data science toolbox using tidyverse packages. In particular, you: Ch.2: Visualized data using the ggplot2 package. Ch.3: Wrangled data using the dplyr package. Ch.4: Learned about the concept of “tidy” data as a standardized data input and output format for all packages in the tidyverse. Furthermore, you learned how to import spreadsheet files into R using the readr package. Statistical inference with infer. Using your newly acquired data science tools and helper functions from the moderndive package, you unpacked statistical inference using the infer package. In particular, you: Ch.5: Learned about the role that sampling variability plays in statistical inference and the role that sample size plays in this sampling variability. Ch.6: Constructed confidence intervals using bootstrapping. Ch.7: Conducted hypothesis tests using permutation. Data modeling with moderndive. Once again using these data science tools and helper functions from the infer and moderndive packages, you fit your first data models. In particular, you: Ch.8: Discovered basic regression models with only one explanatory variable. Ch.9: Interpreted confidence intervals and hypothesis tests in a regression setting. Ch.10: Examined multiple regression models with more than one explanatory variable. Throughout this book, we’ve guided you through your first experiences of “thinking with data,” an expression originally coined by Dr. Diane Lambert. The philosophy underlying this expression guided your path in the flowchart in Figure 11.1. This philosophy is also well-summarized in “Practical Data Science for Stats”: a collection of pre-prints focusing on the practical side of data science workflows and statistical analysis curated by Dr. Jennifer Bryan and Dr. Hadley Wickham. They quote: There are many aspects of day-to-day analytical work that are almost absent from the conventional statistics literature and curriculum. And yet these activities account for a considerable share of the time and effort of data analysts and applied statisticians. The goal of this collection is to increase the visibility and adoption of modern data analytical workflows. We aim to facilitate the transfer of tools and frameworks between industry and academia, between software engineering and statistics and computer science, and across different domains. In other words, to be equipped to “think with data” in the 21st century, analysts need practice going through the “data/science pipeline” we saw in the Preface (re-displayed in Figure 11.2). It is our opinion that, for too long, statistics education has only focused on parts of this pipeline, instead of going through it in its entirety. FIGURE 11.2: Data/science pipeline. 11.2 Case study: Effective data storytelling As we’ve progressed throughout this book, you’ve seen how to work with data in a variety of ways. You’ve learned effective strategies for plotting data by understanding which types of plots work best for which combinations of variable types. You’ve summarized data in spreadsheet form and calculated summary statistics for a variety of different variables. Furthermore, you’ve seen the value of statistical inference as a process to come to conclusions about a population by using sampling. Lastly, you’ve explored how to fit linear regression models and the importance of checking the conditions required so that all confidence intervals and hypothesis tests have valid interpretation. All throughout, you’ve learned many computational techniques and focused on writing R code that’s reproducible. We now present another case study, but this time on the “effective data storytelling” done by data journalists around the world. Great data stories don’t mislead the reader, but rather engulf them in understanding the importance that data plays in our lives through storytelling. 11.2.1 US Births in 1999 The US_births_1994_2003 data frame included in the fivethirtyeight package provides information about the number of daily births in the United States between 1994 and 2003. For more information on this data frame including a link to the original article on FiveThirtyEight.com, check out the help file by running ?US_births_1994_2003 in the console. It’s always a good idea to preview your data, either by using RStudio’s spreadsheet View() function or using glimpse() from the dplyr package: glimpse(US_births_1994_2003) Rows: 3,652 Columns: 6 $ year &lt;int&gt; 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 19… $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… $ date_of_month &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1… $ date &lt;date&gt; 1994-01-01, 1994-01-02, 1994-01-03, 1994-01-04, 1994-01… $ day_of_week &lt;ord&gt; Sat, Sun, Mon, Tues, Wed, Thurs, Fri, Sat, Sun, Mon, Tue… $ births &lt;int&gt; 8096, 7772, 10142, 11248, 11053, 11406, 11251, 8653, 791… We’ll focus on the number of births for each date, but only for births that occurred in 1999. Recall from Section 3.2 we can do this using the filter() function from the dplyr package: US_births_1999 &lt;- US_births_1994_2003 %&gt;% filter(year == 1999) As discussed in Section 2.4, since date is a notion of time and thus has sequential ordering to it, a linegraph would be a more appropriate visualization to use than a scatterplot. In other words, we should use a geom_line() instead of geom_point(). Recall that such plots are called time series plots. ggplot(US_births_1999, aes(x = date, y = births)) + geom_line() + labs(x = &quot;Date&quot;, y = &quot;Number of births&quot;, title = &quot;US Births in 1999&quot;) FIGURE 11.3: Number of births in the US in 1999. We see a big dip occurring just before January 1st, 2000, most likely due to the holiday season. However, what about the large spike of over 14,000 births occurring just before October 1st, 1999? What could be the reason for this anomalously high spike? Let’s sort the rows of US_births_1999 in descending order of the number of births. Recall from Section 3.8 that we can use the arrange() function from the dplyr function to do this, making sure to sort births in descending order: US_births_1999 %&gt;% arrange(desc(births)) # A tibble: 365 × 6 year month date_of_month date day_of_week births &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;ord&gt; &lt;int&gt; 1 1999 9 9 1999-09-09 Thurs 14540 2 1999 12 21 1999-12-21 Tues 13508 3 1999 9 8 1999-09-08 Wed 13437 4 1999 9 21 1999-09-21 Tues 13384 5 1999 9 28 1999-09-28 Tues 13358 6 1999 7 7 1999-07-07 Wed 13343 7 1999 7 8 1999-07-08 Thurs 13245 8 1999 8 17 1999-08-17 Tues 13201 9 1999 9 10 1999-09-10 Fri 13181 10 1999 12 28 1999-12-28 Tues 13158 # … with 355 more rows The date with the highest number of births (14,540) is in fact 1999-09-09. If we write down this date in month/day/year format (a standard format in the US), the date with the highest number of births is 9/9/99! All nines! Could it be that parents deliberately induced labor at a higher rate on this date? Maybe? Whatever the cause may be, this fact makes a fun story! Learning check (LC11.2) What date between 1994 and 2003 has the fewest number of births in the US? What story could you tell about why this is the case? Time to think with data and further tell your story with data! How could statistical modeling help you here? What types of statistical inference would be helpful? What else can you find and where can you take this analysis? What assumptions did you make in this analysis? We leave these questions to you as the reader to explore and examine. Remember to get in touch with us via our contact info in the Preface. We’d love to see what you come up with! Concluding remarks Now that you’ve made it to this point in the book, we suspect that you know a thing or two about how to work with data in R! You’ve also gained a lot of knowledge about how to use simulation-based techniques for statistical inference and how these techniques help build intuition about traditional theory-based inferential methods like the \\(t\\)-test. The hope is that you’ve come to appreciate the power of data in all respects, such as data wrangling, tidying datasets, data visualization, data modeling, and statistical inference. In our opinion, while each of these is important, data visualization may be the most important tool for a citizen or professional data scientist to have in their toolbox. If you can create truly beautiful graphics that display information in ways that the reader can clearly understand, you have great power to tell your tale with data. Let’s hope that these skills help you tell great stories with data into the future. Thanks for coming along this journey as we dove into modern data analysis using R and the tidyverse! "],["A-appendixA.html", "A Statistical Background A.1 Basic statistical terms A.2 Normal distribution A.3 log10 transformations", " A Statistical Background A.1 Basic statistical terms Note that all the following statistical terms apply only to numerical variables, except the distribution which can exist for both numerical and categorical variables. A.1.1 Mean The mean is the most commonly reported measure of center. It is commonly called the average though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \\(n\\) data points, the mean is given by: \\[Mean = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] A.1.2 Median The median is calculated by first sorting a variable’s data from smallest to largest. After sorting the data, the middle element in the list is the median. If the middle falls between two values, then the median is the mean of those two middle values. A.1.3 Standard deviation and variance We will next discuss the standard deviation (\\(sd\\)) of a variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far we expect a given data value will be from its mean: \\[sd = \\sqrt{\\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \\cdots + (x_n - Mean)^2}{n - 1}}\\] The variance of a variable is merely the standard deviation squared. \\[variance = sd^2 = \\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \\cdots + (x_n - Mean)^2}{n - 1}\\] A.1.4 Five-number summary The five-number summary consists of five summary statistics: the minimum, the first quantile AKA 25th percentile, the second quantile AKA median or 50th percentile, the third quantile AKA 75th, and the maximum. The five-number summary of a variable is used when constructing boxplots, as seen in Section 2.7. The quantiles are calculated as first quantile (\\(Q_1\\)): the median of the first half of the sorted data third quantile (\\(Q_3\\)): the median of the second half of the sorted data The interquartile range (IQR) is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values are. The IQR corresponds to the length of the box in a boxplot. The median and the IQR are not influenced by the presence of outliers in the ways that the mean and standard deviation are. They are, thus, recommended for skewed datasets. We say in this case that the median and IQR are more robust to outliers. A.1.5 Distribution The distribution of a variable shows how frequently different values of a variable occur. Looking at the visualization of a distribution can show where the values are centered, show how the values vary, and give some information about where a typical value might fall. It can also alert you to the presence of outliers. Recall from Chapter 2 that we can visualize the distribution of a numerical variable using binning in a histogram and that we can visualize the distribution of a categorical variable using a barplot. A.1.6 Outliers Outliers correspond to values in the dataset that fall far outside the range of “ordinary” values. In the context of a boxplot, by default they correspond to values below \\(Q_1 - (1.5 \\cdot IQR)\\) or above \\(Q_3 + (1.5 \\cdot IQR)\\). A.2 Normal distribution Let’s next discuss one particular kind of distribution: normal distributions. Such bell-shaped distributions are defined by two values: (1) the mean \\(\\mu\\) (“mu”) which locates the center of the distribution and (2) the standard deviation \\(\\sigma\\) (“sigma”) which determines the variation of the distribution. In Figure A.1, we plot three normal distributions where: The solid normal curve has mean \\(\\mu = 5\\) &amp; standard deviation \\(\\sigma = 2\\). The dotted normal curve has mean \\(\\mu = 5\\) &amp; standard deviation \\(\\sigma = 5\\). The dashed normal curve has mean \\(\\mu = 15\\) &amp; standard deviation \\(\\sigma = 2\\). FIGURE A.1: Three normal distributions. Notice how the solid and dotted line normal curves have the same center due to their common mean \\(\\mu\\) = 5. However, the dotted line normal curve is wider due to its larger standard deviation of \\(\\sigma\\) = 5. On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation \\(\\sigma\\) = 2. However, they are centered at different locations. When the mean \\(\\mu\\) = 0 and the standard deviation \\(\\sigma\\) = 1, the normal distribution has a special name. It’s called the standard normal distribution or the \\(z\\)-curve. Furthermore, if a variable follows a normal curve, there are three rules of thumb we can use: 68% of values will lie within \\(\\pm\\) 1 standard deviation of the mean. 95% of values will lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations of the mean. 99.7% of values will lie within \\(\\pm\\) 3 standard deviations of the mean. Let’s illustrate this on a standard normal curve in Figure A.2. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100%. For example: The middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34% + 34% = 68% of the area under the curve. In other words, 68% of values. The middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5% + 34% + 34% + 13.5% = 95% of the area under the curve. In other words, 95% of values. The middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% of the area under the curve. In other words, 99.7% of values. FIGURE A.2: Rules of thumb about areas under normal curves. Learning check Say you have a normal distribution with mean \\(\\mu = 6\\) and standard deviation \\(\\sigma = 3\\). (LCA.1) What proportion of the area under the normal curve is less than 3? Greater than 12? Between 0 and 12? (LCA.2) What is the 2.5th percentile of the area under the normal curve? The 97.5th percentile? The 100th percentile? A.3 log10 transformations At its simplest, log10 transformations return base 10 logarithms. For example, since \\(1000 = 10^3\\), running log10(1000) returns 3 in R. To undo a log10 transformation, we raise 10 to this value. For example, to undo the previous log10 transformation and return the original value of 1000, we raise 10 to the power of 3 by running 10^(3) = 1000 in R. Log transformations allow us to focus on changes in orders of magnitude. In other words, they allow us to focus on multiplicative changes instead of additive ones. Let’s illustrate this idea in Table A.1 with examples of prices of consumer goods in 2019 US dollars. TABLE A.1: log10 transformed prices, orders of magnitude, and examples Price log10(Price) Order of magnitude Examples $1 0 Singles Cups of coffee $10 1 Tens Books $100 2 Hundreds Mobile phones $1,000 3 Thousands High definition TVs $10,000 4 Tens of thousands Cars $100,000 5 Hundreds of thousands Luxury cars and houses $1,000,000 6 Millions Luxury houses Let’s make some remarks about log10 transformations based on Table A.1: When purchasing a cup of coffee, we tend to think of prices ranging in single dollars, such as $2 or $3. However, when purchasing a mobile phone, we don’t tend to think of their prices in units of single dollars such as $313 or $727. Instead, we tend to think of their prices in units of hundreds of dollars like $300 or $700. Thus, cups of coffee and mobile phones are of different orders of magnitude in price. Let’s say we want to know the log10 transformed value of $76. This would be hard to compute exactly without a calculator. However, since $76 is between $10 and $100 and since log10(10) = 1 and log10(100) = 2, we know log10(76) will be between 1 and 2. In fact, log10(76) is 1.880814. log10 transformations are monotonic, meaning they preserve orders. So if Price A is lower than Price B, then log10(Price A) will also be lower than log10(Price B). Most importantly, increments of one in log10-scale correspond to relative multiplicative changes in the original scale and not absolute additive changes. For example, increasing a log10(Price) from 3 to 4 corresponds to a multiplicative increase by a factor of 10: $100 to $1000. "],["B-appendixB.html", "B Inference Examples B.1 Inference mind map B.2 One mean B.3 One proportion B.4 Two proportions B.5 Two means (independent samples) B.6 Two means (paired samples)", " B Inference Examples This appendix is designed to provide you with examples of the five basic hypothesis tests and their corresponding confidence intervals. Traditional theory-based methods as well as computational-based methods are presented. Note: This appendix is still under construction. If you would like to contribute, please check us out on GitHub at https://github.com/moderndive/moderndive_book. Needed packages library(tidyverse) library(infer) library(janitor) B.1 Inference mind map To help you better navigate and choose the appropriate analysis, we’ve created a mind map on http://coggle.it available here and below. FIGURE B.1: Mind map for Inference. B.2 One mean B.2.1 Problem statement The National Survey of Family Growth conducted by the Centers for Disease Control gathers information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health. One of the variables collected on this survey is the age at first marriage. 5,534 randomly sampled US women between 2006 and 2010 completed the survey. The women sampled here had been married at least once. Do we have evidence that the mean age of first marriage for all US women from 2006 to 2010 is greater than 23 years? (Tweaked a bit from Diez, Barr, and Çetinkaya-Rundel 2014 [Chapter 4]) B.2.2 Competing hypotheses In words Null hypothesis: The mean age of first marriage for all US women from 2006 to 2010 is equal to 23 years. Alternative hypothesis: The mean age of first marriage for all US women from 2006 to 2010 is greater than 23 years. In symbols (with annotations) \\(H_0: \\mu = \\mu_{0}\\), where \\(\\mu\\) represents the mean age of first marriage for all US women from 2006 to 2010 and \\(\\mu_0\\) is 23. \\(H_A: \\mu &gt; 23\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.2.3 Exploring the sample data age_at_marriage &lt;- read_csv(&quot;https://moderndive.com/data/ageAtMar.csv&quot;) age_summ &lt;- age_at_marriage %&gt;% summarize( sample_size = n(), mean = mean(age), sd = sd(age), minimum = min(age), lower_quartile = quantile(age, 0.25), median = median(age), upper_quartile = quantile(age, 0.75), max = max(age) ) kable(age_summ) %&gt;% kable_styling( font_size = ifelse(is_latex_output(), 10, 16), latex_options = c(&quot;hold_position&quot;) ) sample_size mean sd minimum lower_quartile median upper_quartile max 5534 23.4 4.72 10 20 23 26 43 The histogram below also shows the distribution of age. ggplot(data = age_at_marriage, mapping = aes(x = age)) + geom_histogram(binwidth = 3, color = &quot;white&quot;) The observed statistic of interest here is the sample mean: x_bar &lt;- age_at_marriage %&gt;% specify(response = age) %&gt;% calculate(stat = &quot;mean&quot;) x_bar Response: age (numeric) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 23.4402 Guess about statistical significance We are looking to see if the observed sample mean of 23.44 is statistically greater than \\(\\mu_0 = 23\\). They seem to be quite close, but we have a large sample size here. Let’s guess that the large sample size will lead us to reject this practically small difference. B.2.4 Non-traditional methods Bootstrapping for hypothesis test In order to look to see if the observed sample mean of 23.44 is statistically greater than \\(\\mu_0 = 23\\), we need to account for the sample size. We also need to determine a process that replicates how the original sample of size 5534 was selected. We can use the idea of bootstrapping to simulate the population from which the sample came and then generate samples from that simulated population to account for sampling variability. Recall how bootstrapping would apply in this context: Sample with replacement from our original sample of 5534 women and repeat this process 10,000 times, calculate the mean for each of the 10,000 bootstrap samples created in Step 1., combine all of these bootstrap statistics calculated in Step 2 into a boot_distn object, and shift the center of this distribution over to the null value of 23. (This is needed since it will be centered at 23.44 via the process of bootstrapping.) set.seed(2018) null_distn_one_mean &lt;- age_at_marriage %&gt;% specify(response = age) %&gt;% hypothesize(null = &quot;point&quot;, mu = 23) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) null_distn_one_mean %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a right-tailed test so we will be looking for values that are greater than or equal to 23.44 for our \\(p\\)-value. null_distn_one_mean %&gt;% visualize(obs_stat = x_bar, direction = &quot;greater&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_one_mean %&gt;% get_pvalue(obs_stat = x_bar, direction = &quot;greater&quot;) pvalue # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0 So our \\(p\\)-value is 0 and we reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are far into the tail of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\mu\\) using our sample data using bootstrapping. Note that we don’t need to shift this distribution since we want the center of our confidence interval to be our point estimate \\(\\bar{x}_{obs} = 23.44\\). boot_distn_one_mean &lt;- age_at_marriage %&gt;% specify(response = age) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) ci &lt;- boot_distn_one_mean %&gt;% get_ci() ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 23.3148 23.5669 boot_distn_one_mean %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 23 is not contained in this confidence interval as a plausible value of \\(\\mu\\) (the unknown population mean) and the entire interval is larger than 23. This matches with our hypothesis test results of rejecting the null hypothesis in favor of the alternative (\\(\\mu &gt; 23\\)). Interpretation: We are 95% confident the true mean age of first marriage for all US women from 2006 to 2010 is between 23.315 and 23.567. B.2.5 Traditional methods Check conditions Remember that in order to use the shortcut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations are collected independently. The cases are selected independently through random sampling so this condition is met. Approximately normal: The distribution of the response variable should be normal or the sample size should be at least 30. The histogram for the sample above does show some skew. The Q-Q plot below also shows some skew. ggplot(data = age_at_marriage, mapping = aes(sample = age)) + stat_qq() The sample size here is quite large though (\\(n = 5534\\)) so both conditions are met. Test statistic The test statistic is a random variable based on the sample data. Here, we want to look at a way to estimate the population mean \\(\\mu\\). A good guess is the sample mean \\(\\bar{X}\\). Recall that this sample mean is actually a random variable that will vary as different samples are (theoretically, would be) collected. We are looking to see how likely is it for us to have observed a sample mean of \\(\\bar{x}_{obs} = 23.44\\) or larger assuming that the population mean is 23 (assuming the null hypothesis is true). If the conditions are met and assuming \\(H_0\\) is true, we can “standardize” this original test statistic of \\(\\bar{X}\\) into a \\(T\\) statistic that follows a \\(t\\) distribution with degrees of freedom equal to \\(df = n - 1\\): \\[ T =\\dfrac{ \\bar{X} - \\mu_0}{ S / \\sqrt{n} } \\sim t (df = n - 1) \\] where \\(S\\) represents the standard deviation of the sample and \\(n\\) is the sample size. Observed test statistic While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. We can use the t_test() function to perform this analysis for us. t_test_results &lt;- age_at_marriage %&gt;% t_test( formula = age ~ NULL, alternative = &quot;greater&quot;, mu = 23 ) t_test_results One Sample t-test data: . t = 7, df = 5533, p-value = 0.000000000002 alternative hypothesis: true mean is greater than 23 95 percent confidence interval: 23.3 Inf sample estimates: mean of x 23.4 We see here that the \\(t_{obs}\\) value is 6.936. Compute \\(p\\)-value The \\(p\\)-value—the probability of observing an \\(t_{obs}\\) value of 6.936 or more in our null distribution of a \\(t\\) with 5533 degrees of freedom—is essentially . State conclusion We, therefore, have sufficient evidence to reject the null hypothesis. Our initial guess that our observed sample mean was statistically greater than the hypothesized mean has supporting evidence here. Based on this sample, we have evidence that the mean age of first marriage for all US women from 2006 to 2010 is greater than 23 years. Confidence interval t.test( x = age_at_marriage$age, alternative = &quot;two.sided&quot;, mu = 23 )$conf [1] 23.3 23.6 attr(,&quot;conf.level&quot;) [1] 0.95 B.2.6 Comparing results Observing the bootstrap distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions also being met (the large sample size was the driver here) leads us to better guess that using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) will lead to similar results. B.3 One proportion B.3.1 Problem statement The CEO of a large electric utility claims that 80 percent of his 1,000,000 customers are satisfied with the service they receive. To test this claim, the local newspaper surveyed 100 customers, using simple random sampling. 73 were satisfied and the remaining were unsatisfied. Based on these findings from the sample, can we reject the CEO’s hypothesis that 80% of the customers are satisfied? [Tweaked a bit from http://stattrek.com/hypothesis-test/proportion.aspx?Tutorial=AP] B.3.2 Competing hypotheses In words Null hypothesis: The proportion of all customers of the large electric utility satisfied with service they receive is equal 0.80. Alternative hypothesis: The proportion of all customers of the large electric utility satisfied with service they receive is different from 0.80. In symbols (with annotations) \\(H_0: \\pi = p_{0}\\), where \\(\\pi\\) represents the proportion of all customers of the large electric utility satisfied with service they receive and \\(p_0\\) is 0.8. \\(H_A: \\pi \\ne 0.8\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.3.3 Exploring the sample data elec &lt;- c(rep(&quot;satisfied&quot;, 73), rep(&quot;unsatisfied&quot;, 27)) %&gt;% enframe() %&gt;% rename(satisfy = value) The bar graph below also shows the distribution of satisfy. ggplot(data = elec, aes(x = satisfy)) + geom_bar() The observed statistic is computed as p_hat &lt;- elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% calculate(stat = &quot;prop&quot;) p_hat Response: satisfy (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 0.73 Guess about statistical significance We are looking to see if the sample proportion of 0.73 is statistically different from \\(p_0 = 0.8\\) based on this sample. They seem to be quite close, and our sample size is not huge here (\\(n = 100\\)). Let’s guess that we do not have evidence to reject the null hypothesis. B.3.4 Non-traditional methods Simulation for hypothesis test In order to look to see if 0.73 is statistically different from 0.8, we need to account for the sample size. We also need to determine a process that replicates how the original sample of size 100 was selected. We can use the idea of an unfair coin to simulate this process. We will simulate flipping an unfair coin (with probability of success 0.8 matching the null hypothesis) 100 times. Then we will keep track of how many heads come up in those 100 flips. Our simulated statistic matches with how we calculated the original statistic \\(\\hat{p}\\): the number of heads (satisfied) out of our total sample of 100. We then repeat this process many times (say 10,000) to create the null distribution looking at the simulated proportions of successes: set.seed(2018) null_distn_one_prop &lt;- elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = 0.8) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;prop&quot;) null_distn_one_prop %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a two-tailed test so we will be looking for values that are 0.8 - 0.73 = 0.07 away from 0.8 in BOTH directions for our \\(p\\)-value: null_distn_one_prop %&gt;% visualize(obs_stat = p_hat, direction = &quot;both&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_one_prop %&gt;% get_pvalue(obs_stat = p_hat, direction = &quot;both&quot;) pvalue # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0.1136 So our \\(p\\)-value is 0.114 and we fail to reject the null hypothesis at the 5% level. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\pi\\) using our sample data. To do so, we use bootstrapping, which involves sampling with replacement from our original sample of 100 survey respondents and repeating this process 10,000 times, calculating the proportion of successes for each of the 10,000 bootstrap samples created in Step 1., combining all of these bootstrap statistics calculated in Step 2 into a boot_distn object, identifying the 2.5th and 97.5th percentiles of this distribution (corresponding to the 5% significance level chosen) to find a 95% confidence interval for \\(\\pi\\), and interpret this confidence interval in the context of the problem. boot_distn_one_prop &lt;- elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;prop&quot;) Just as we use the mean function for calculating the mean over a numerical variable, we can also use it to compute the proportion of successes for a categorical variable where we specify what we are calling a “success” after the ==. (Think about the formula for calculating a mean and how R handles logical statements such as satisfy == \"satisfied\" for why this must be true.) ci &lt;- boot_distn_one_prop %&gt;% get_ci() ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 0.64 0.81 boot_distn_one_prop %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0.80 is contained in this confidence interval as a plausible value of \\(\\pi\\) (the unknown population proportion). This matches with our hypothesis test results of failing to reject the null hypothesis. Interpretation: We are 95% confident the true proportion of customers who are satisfied with the service they receive is between 0.64 and 0.81. B.3.5 Traditional methods Check conditions Remember that in order to use the shortcut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations are collected independently. The cases are selected independently through random sampling so this condition is met. Approximately normal: The number of expected successes and expected failures is at least 10. This condition is met since 73 and 27 are both greater than 10. Test statistic The test statistic is a random variable based on the sample data. Here, we want to look at a way to estimate the population proportion \\(\\pi\\). A good guess is the sample proportion \\(\\hat{P}\\). Recall that this sample proportion is actually a random variable that will vary as different samples are (theoretically, would be) collected. We are looking to see how likely is it for us to have observed a sample proportion of \\(\\hat{p}_{obs} = 0.73\\) or more extreme assuming that the population proportion is 0.80 (assuming the null hypothesis is true). If the conditions are met and assuming \\(H_0\\) is true, we can standardize this original test statistic of \\(\\hat{P}\\) into a \\(Z\\) statistic that follows a \\(N(0, 1)\\) distribution. \\[ Z =\\dfrac{ \\hat{P} - p_0}{\\sqrt{\\dfrac{p_0(1 - p_0)}{n} }} \\sim N(0, 1) \\] Observed test statistic While one could compute this observed test statistic by “hand” by plugging the observed values into the formula, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. The calculation has been done in R below for completeness though: p_hat &lt;- 0.73 p0 &lt;- 0.8 n &lt;- 100 (z_obs &lt;- (p_hat - p0) / sqrt((p0 * (1 - p0)) / n)) [1] -1.75 We see here that the \\(z_{obs}\\) value is around -1.75. Our observed sample proportion of 0.73 is 1.75 standard errors below the hypothesized parameter value of 0.8. Visualize and compute \\(p\\)-value elec %&gt;% specify(response = satisfy, success = &quot;satisfied&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = 0.8) %&gt;% calculate(stat = &quot;z&quot;) %&gt;% visualize(method = &quot;theoretical&quot;, obs_stat = z_obs, direction = &quot;both&quot;) Rather than setting `method = &quot;theoretical&quot;` with a simulation-based null distribution, the preferred method for visualizing theory-based distributions with infer is now to pass the output of `assume()` as the first argument to `visualize()`. 2 * pnorm(z_obs) [1] 0.0801 The \\(p\\)-value—the probability of observing an \\(z_{obs}\\) value of -1.75 or more extreme (in both directions) in our null distribution—is around 8%. Note that we could also do this test directly using the prop.test function. prop.test( x = table(elec$satisfy), n = length(elec$satisfy), alternative = &quot;two.sided&quot;, p = 0.8, correct = FALSE ) prop.test does a \\(\\chi^2\\) test here but this matches up exactly with what we would expect: \\(x^2_{obs} = 3.06 = (-1.75)^2 = (z_{obs})^2\\) and the \\(p\\)-values are the same because we are focusing on a two-tailed test. Note that the 95 percent confidence interval given above matches well with the one calculated using bootstrapping. State conclusion We, therefore, do not have sufficient evidence to reject the null hypothesis. Our initial guess that our observed sample proportion was not statistically different from the hypothesized proportion has not been invalidated. Based on this sample, we have do not evidence that the proportion of all customers of the large electric utility satisfied with service they receive is different from 0.80, at the 5% level. B.3.6 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions also being met leads us to better guess that using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) will lead to similar results. B.4 Two proportions B.4.1 Problem statement A 2010 survey asked 827 randomly sampled registered voters in California “Do you support? Or do you oppose? Drilling for oil and natural gas off the Coast of California? Or do you not know enough to say?” Conduct a hypothesis test to determine if the data provide strong evidence that the proportion of college graduates who do not have an opinion on this issue is different than that of non-college graduates. (Tweaked a bit from Diez, Barr, and Çetinkaya-Rundel 2014 [Chapter 6]) B.4.2 Competing hypotheses In words Null hypothesis: There is no association between having an opinion on drilling and having a college degree for all registered California voters in 2010. Alternative hypothesis: There is an association between having an opinion on drilling and having a college degree for all registered California voters in 2010. Another way in words Null hypothesis: The probability that a Californian voter in 2010 having no opinion on drilling and is a college graduate is the same as that of a non-college graduate. Alternative hypothesis: These parameter probabilities are different. In symbols (with annotations) \\(H_0: \\pi_{college} = \\pi_{no\\_college}\\) or \\(H_0: \\pi_{college} - \\pi_{no\\_college} = 0\\), where \\(\\pi\\) represents the probability of not having an opinion on drilling. \\(H_A: \\pi_{college} - \\pi_{no\\_college} \\ne 0\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.4.3 Exploring the sample data offshore &lt;- read_csv(&quot;https://moderndive.com/data/offshore.csv&quot;) counts &lt;- offshore %&gt;% tabyl(college_grad, response) counts college_grad no opinion opinion no 131 258 yes 104 334 Observe that of the college graduates, a proportion of 104/(104 + 334) = 0.237 have no opinion on drilling. On the other hand, of the non-college graduates, a proportion of 131/(131 + 258) = 0.337 have no opinion on drilling, whereas . The difference in these proportions is 0.237 - 0.337 = -0.099. Let’s visualize these in a barchart. However, we first reverse the order of the levels in the categorical variable response using the fct_rev() function from the forcats package. We do this because the default ordering of levels in a factor is alphanumeric. However, we are interested in proportions that have no opinion and not opinion. Thus we need to reverse the default alphanumeric order. offshore &lt;- offshore %&gt;% mutate(response = fct_rev(response)) ggplot(offshore, aes(x = college_grad, fill = response)) + geom_bar(position = &quot;fill&quot;) + labs(x = &quot;College grad?&quot;, y = &quot;Proportion with no opinion on drilling&quot;) + coord_flip() Guess about statistical significance We are looking to see if a difference exists in the size of the bars corresponding to no opinion for the plot. Based solely on the plot, we have little reason to believe that a difference exists since the bars seem to be about the same size, BUT…it’s important to use statistics to see if that difference is actually statistically significant! B.4.4 Non-traditional methods Collecting summary info The observed statistic is d_hat &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) d_hat Response: response (factor) Explanatory: college_grad (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -0.0993180 Randomization for hypothesis test In order to ascertain if the observed sample proportion with no opinion for college graduates of 0.237 is statistically different than the observed sample proportion with no opinion for non-college graduates of 0.337, we need to account for the sample sizes. Note that this is the same as ascertaining if the observed difference in sample proportions -0.099 is statistically different than 0. We also need to determine a process that replicates how the original group sizes of 389 and 438 were selected. We can use the idea of randomization testing (also known as permutation testing) to simulate the population from which the sample came (with two groups of different sizes) and then generate samples using shuffling from that simulated population to account for sampling variability. set.seed(2018) null_distn_two_props &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) null_distn_two_props %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a two-tailed test so we will be looking for values that are greater than or equal to 0.099 or less than or equal to -0.099 for our \\(p\\)-value. null_distn_two_props %&gt;% visualize(obs_stat = d_hat, direction = &quot;two_sided&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_two_props %&gt;% get_pvalue(obs_stat = d_hat, direction = &quot;two_sided&quot;) pvalue # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0.0024 So our \\(p\\)-value is 0.002 and we reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are far into the tails of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\pi_{college} - \\pi_{no\\_college}\\) using our sample data with bootstrapping. boot_distn_two_props &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) ci &lt;- boot_distn_two_props %&gt;% get_ci() ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 -0.160030 -0.0379112 boot_distn_two_props %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0 is not contained in this confidence interval as a plausible value of \\(\\pi_{college} - \\pi_{no\\_college}\\) (the unknown population parameter). This matches with our hypothesis test results of rejecting the null hypothesis. Since zero is not a plausible value of the population parameter, we have evidence that the proportion of college graduates in California with no opinion on drilling is different than that of non-college graduates. Interpretation: We are 95% confident the true proportion of non-college graduates with no opinion on offshore drilling in California is between 0.16 dollars smaller to 0.04 dollars smaller than for college graduates. B.4.5 Traditional methods B.4.5.1 Check conditions Remember that in order to use the short-cut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: Each case that was selected must be independent of all the other cases selected. This condition is met since cases were selected at random to observe. Sample size: The number of pooled successes and pooled failures must be at least 10 for each group. We need to first figure out the pooled success rate: \\[\\hat{p}_{obs} = \\dfrac{131 + 104}{827} = 0.28.\\] We now determine expected (pooled) success and failure counts: \\(0.28 \\cdot (131 + 258) = 108.92\\), \\(0.72 \\cdot (131 + 258) = 280.08\\) \\(0.28 \\cdot (104 + 334) = 122.64\\), \\(0.72 \\cdot (104 + 334) = 315.36\\) Independent selection of samples: The cases are not paired in any meaningful way. We have no reason to suspect that a college graduate selected would have any relationship to a non-college graduate selected. B.4.6 Test statistic The test statistic is a random variable based on the sample data. Here, we are interested in seeing if our observed difference in sample proportions corresponding to no opinion on drilling (\\(\\hat{p}_{college, obs} - \\hat{p}_{no\\_college, obs}\\) = -0.092) is statistically different than 0. Assuming that conditions are met and the null hypothesis is true, we can use the standard normal distribution to standardize the difference in sample proportions (\\(\\hat{P}_{college} - \\hat{P}_{no\\_college}\\)) using the standard error of \\(\\hat{P}_{college} - \\hat{P}_{no\\_college}\\) and the pooled estimate: \\[ Z =\\dfrac{ (\\hat{P}_{college} - \\hat{P}_{no_college}) - 0}{\\sqrt{\\dfrac{\\hat{P}(1 - \\hat{P})}{n_1} + \\dfrac{\\hat{P}(1 - \\hat{P})}{n_2} }} \\sim N(0, 1) \\] where \\(\\hat{P} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}}.\\) Observed test statistic While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. We can use the prop.test function to perform this analysis for us. z_hat &lt;- offshore %&gt;% specify(response ~ college_grad, success = &quot;no opinion&quot;) %&gt;% calculate(stat = &quot;z&quot;, order = c(&quot;yes&quot;, &quot;no&quot;)) z_hat Response: response (factor) Explanatory: college_grad (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -3.16081 The observed difference in sample proportions is 3.16 standard deviations smaller than 0. The \\(p\\)-value—the probability of observing a \\(Z\\) value of -3.16 or more extreme in our null distribution—is 0.0016. This can also be calculated in R directly: 2 * pnorm(-3.16, lower.tail = TRUE) [1] 0.00158 B.4.7 State conclusion We, therefore, have sufficient evidence to reject the null hypothesis. Our initial guess that a statistically significant difference did not exist in the proportions of no opinion on offshore drilling between college educated and non-college educated Californians was not validated. We do have evidence to suggest that there is a dependency between college graduation and position on offshore drilling for Californians. B.4.8 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions were not met since the number of pairs was small, but the sample data was not highly skewed. Using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) lead to similar results. B.5 Two means (independent samples) B.5.1 Problem statement Average income varies from one region of the country to another, and it often reflects both lifestyles and regional living expenses. Suppose a new graduate is considering a job in two locations, Cleveland, OH and Sacramento, CA, and he wants to see whether the average income in one of these cities is higher than the other. He would like to conduct a hypothesis test based on two randomly selected samples from the 2000 Census. (Tweaked a bit from Diez, Barr, and Çetinkaya-Rundel 2014 [Chapter 5]) B.5.2 Competing hypotheses In words Null hypothesis: There is no association between income and location (Cleveland, OH and Sacramento, CA). Alternative hypothesis: There is an association between income and location (Cleveland, OH and Sacramento, CA). Another way in words Null hypothesis: The mean income is the same for both cities. Alternative hypothesis: The mean income is different for the two cities. In symbols (with annotations) \\(H_0: \\mu_{sac} = \\mu_{cle}\\) or \\(H_0: \\mu_{sac} - \\mu_{cle} = 0\\), where \\(\\mu\\) represents the average income. \\(H_A: \\mu_{sac} - \\mu_{cle} \\ne 0\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.5.3 Exploring the sample data cle_sac &lt;- read.delim(&quot;https://moderndive.com/data/cleSac.txt&quot;) %&gt;% rename( metro_area = Metropolitan_area_Detailed, income = Total_personal_income ) %&gt;% na.omit() inc_summ &lt;- cle_sac %&gt;% group_by(metro_area) %&gt;% summarize( sample_size = n(), mean = mean(income), sd = sd(income), minimum = min(income), lower_quartile = quantile(income, 0.25), median = median(income), upper_quartile = quantile(income, 0.75), max = max(income) ) kable(inc_summ) %&gt;% kable_styling( font_size = ifelse(is_latex_output(), 10, 16), latex_options = c(&quot;hold_position&quot;) ) metro_area sample_size mean sd minimum lower_quartile median upper_quartile max Cleveland_ OH 212 27467 27681 0 8475 21000 35275 152400 Sacramento_ CA 175 32428 35774 0 8050 20000 49350 206900 The boxplot below also shows the mean for each group highlighted by the red dots. ggplot(cle_sac, aes(x = metro_area, y = income)) + geom_boxplot() + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, color = &quot;red&quot;) Guess about statistical significance We are looking to see if a difference exists in the mean income of the two levels of the explanatory variable. Based solely on the boxplot, we have reason to believe that no difference exists. The distributions of income seem similar and the means fall in roughly the same place. B.5.4 Non-traditional methods Collecting summary info We now compute the observed statistic: d_hat &lt;- cle_sac %&gt;% specify(income ~ metro_area) %&gt;% calculate( stat = &quot;diff in means&quot;, order = c(&quot;Sacramento_ CA&quot;, &quot;Cleveland_ OH&quot;) ) d_hat Response: income (numeric) Explanatory: metro_area (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 4960.48 Randomization for hypothesis test In order to look to see if the observed sample mean for Sacramento of 27467.066 is statistically different than that for Cleveland of 32427.543, we need to account for the sample sizes. Note that this is the same as looking to see if \\(\\bar{x}_{sac} - \\bar{x}_{cle}\\) is statistically different than 0. We also need to determine a process that replicates how the original group sizes of 212 and 175 were selected. We can use the idea of randomization testing (also known as permutation testing) to simulate the population from which the sample came (with two groups of different sizes) and then generate samples using shuffling from that simulated population to account for sampling variability. set.seed(2018) null_distn_two_means &lt;- cle_sac %&gt;% specify(income ~ metro_area) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate( stat = &quot;diff in means&quot;, order = c(&quot;Sacramento_ CA&quot;, &quot;Cleveland_ OH&quot;) ) null_distn_two_means %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a two-tailed test so we will be looking for values that are greater than or equal to 4960.477 or less than or equal to -4960.477 for our \\(p\\)-value. null_distn_two_means %&gt;% visualize(obs_stat = d_hat, direction = &quot;both&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_two_means %&gt;% get_pvalue(obs_stat = d_hat, direction = &quot;both&quot;) pvalue # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0.1262 So our \\(p\\)-value is 0.126 and we fail to reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are not very far into the tail of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\mu_{sac} - \\mu_{cle}\\) using our sample data with bootstrapping. Here we will bootstrap each of the samples with replacement instead of shuffling. This is done by setting the type argument in the generate function to “bootstrap”. boot_distn_two_means &lt;- cle_sac %&gt;% specify(income ~ metro_area) %&gt;% generate(reps = 10000) %&gt;% calculate( stat = &quot;diff in means&quot;, order = c(&quot;Sacramento_ CA&quot;, &quot;Cleveland_ OH&quot;) ) ci &lt;- boot_distn_two_means %&gt;% get_ci() ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 -1359.50 11499.7 boot_distn_two_means %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0 is contained in this confidence interval as a plausible value of \\(\\mu_{sac} - \\mu_{cle}\\) (the unknown population parameter). This matches with our hypothesis test results of failing to reject the null hypothesis. Since zero is a plausible value of the population parameter, we do not have evidence that Sacramento incomes are different than Cleveland incomes. Interpretation: We are 95% confident the true mean yearly income for those living in Sacramento is between 1359.5 dollars smaller to 11499.69 dollars higher than for Cleveland. Note: You could also use the null distribution based on randomization with a shift to have its center at \\(\\bar{x}_{sac} - \\bar{x}_{cle} = \\$4960.48\\) instead of at 0 and calculate its percentiles. The confidence interval produced via this method should be comparable to the one done using bootstrapping above. B.5.5 Traditional methods Check conditions Remember that in order to use the short-cut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations are independent in both groups. This metro_area variable is met since the cases are randomly selected from each city. Approximately normal: The distribution of the response for each group should be normal or the sample sizes should be at least 30. ggplot(cle_sac, aes(x = income)) + geom_histogram(color = &quot;white&quot;, binwidth = 20000) + facet_wrap(~metro_area) We have some reason to doubt the normality assumption here since both the histograms show deviation from a normal model fitting the data well for each group. The sample sizes for each group are greater than 100 though so the assumptions should still apply. Independent samples: The samples should be collected without any natural pairing. There is no mention of there being a relationship between those selected in Cleveland and in Sacramento. B.5.6 Test statistic The test statistic is a random variable based on the sample data. Here, we are interested in seeing if our observed difference in sample means (\\(\\bar{x}_{sac, obs} - \\bar{x}_{cle, obs}\\) = 4960.477) is statistically different than 0. Assuming that conditions are met and the null hypothesis is true, we can use the \\(t\\) distribution to standardize the difference in sample means (\\(\\bar{X}_{sac} - \\bar{X}_{cle}\\)) using the approximate standard error of \\(\\bar{X}_{sac} - \\bar{X}_{cle}\\) (invoking \\(S_{sac}\\) and \\(S_{cle}\\) as estimates of unknown \\(\\sigma_{sac}\\) and \\(\\sigma_{cle}\\)). \\[ T =\\dfrac{ (\\bar{X}_1 - \\bar{X}_2) - 0}{ \\sqrt{\\dfrac{S_1^2}{n_1} + \\dfrac{S_2^2}{n_2}} } \\sim t (df = min(n_1 - 1, n_2 - 1)) \\] where 1 = Sacramento and 2 = Cleveland with \\(S_1^2\\) and \\(S_2^2\\) the sample variance of the incomes of both cities, respectively, and \\(n_1 = 175\\) for Sacramento and \\(n_2 = 212\\) for Cleveland. Observed test statistic Note that we could also do (ALMOST) this test directly using the t.test function. The x and y arguments are expected to both be numeric vectors here so we’ll need to appropriately filter our datasets. cle_sac %&gt;% specify(income ~ metro_area) %&gt;% calculate( stat = &quot;t&quot;, order = c(&quot;Cleveland_ OH&quot;, &quot;Sacramento_ CA&quot;) ) Response: income (numeric) Explanatory: metro_area (factor) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -1.50062 We see here that the observed test statistic value is around -1.5. While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. B.5.7 Compute \\(p\\)-value The \\(p\\)-value—the probability of observing an \\(t_{174}\\) value of -1.501 or more extreme (in both directions) in our null distribution—is 0.13. This can also be calculated in R directly: 2 * pt(-1.501, df = min(212 - 1, 175 - 1), lower.tail = TRUE) [1] 0.135 We can also approximate by using the standard normal curve: 2 * pnorm(-1.501) [1] 0.133 B.5.8 State conclusion We, therefore, do not have sufficient evidence to reject the null hypothesis. Our initial guess that a statistically significant difference not existing in the means was backed by this statistical analysis. We do not have evidence to suggest that the true mean income differs between Cleveland, OH and Sacramento, CA based on this data. B.5.9 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions also being met leads us to better guess that using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) will lead to similar results. B.6 Two means (paired samples) Problem statement Trace metals in drinking water affect the flavor and an unusually high concentration can pose a health hazard. Ten pairs of data were taken measuring zinc concentration in bottom water and surface water at 10 randomly selected locations on a stretch of river. Do the data suggest that the true average concentration in the surface water is smaller than that of bottom water? (Note that units are not given.) [Tweaked a bit from https://onlinecourses.science.psu.edu/stat500/node/51] B.6.1 Competing hypotheses In words Null hypothesis: The mean concentration in the bottom water is the same as that of the surface water at different paired locations. Alternative hypothesis: The mean concentration in the surface water is smaller than that of the bottom water at different paired locations. In symbols (with annotations) \\(H_0: \\mu_{diff} = 0\\), where \\(\\mu_{diff}\\) represents the mean difference in concentration for surface water minus bottom water. \\(H_A: \\mu_{diff} &lt; 0\\) Set \\(\\alpha\\) It’s important to set the significance level before starting the testing using the data. Let’s set the significance level at 5% here. B.6.2 Exploring the sample data zinc_tidy &lt;- read_csv(&quot;https://moderndive.com/data/zinc_tidy.csv&quot;) We want to look at the differences in surface - bottom for each location: zinc_diff &lt;- zinc_tidy %&gt;% group_by(loc_id) %&gt;% summarize(pair_diff = diff(concentration)) %&gt;% ungroup() Next we calculate the mean difference as our observed statistic: d_hat &lt;- zinc_diff %&gt;% specify(response = pair_diff) %&gt;% calculate(stat = &quot;mean&quot;) d_hat Response: pair_diff (numeric) # A tibble: 1 × 1 stat &lt;dbl&gt; 1 -0.0804 The histogram below also shows the distribution of pair_diff. ggplot(zinc_diff, aes(x = pair_diff)) + geom_histogram(binwidth = 0.04, color = &quot;white&quot;) Guess about statistical significance We are looking to see if the sample paired mean difference of -0.08 is statistically less than 0. They seem to be quite close, but we have a small number of pairs here. Let’s guess that we will fail to reject the null hypothesis. B.6.3 Non-traditional methods Bootstrapping for hypothesis test In order to look to see if the observed sample mean difference \\(\\bar{x}_{diff} = -0.08\\) is statistically less than 0, we need to account for the number of pairs. We also need to determine a process that replicates how the paired data was selected in a way similar to how we calculated our original difference in sample means. Treating the differences as our data of interest, we next use the process of bootstrapping to build other simulated samples and then calculate the mean of the bootstrap samples. We hypothesize that the mean difference is zero. This process is similar to comparing the One Mean example seen above, but using the differences between the two groups as a single sample with a hypothesized mean difference of 0. set.seed(2018) null_distn_paired_means &lt;- zinc_diff %&gt;% specify(response = pair_diff) %&gt;% hypothesize(null = &quot;point&quot;, mu = 0) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) null_distn_paired_means %&gt;% visualize() We can next use this distribution to observe our \\(p\\)-value. Recall this is a left-tailed test so we will be looking for values that are less than or equal to -0.08 for our \\(p\\)-value. null_distn_paired_means %&gt;% visualize(obs_stat = d_hat, direction = &quot;less&quot;) Calculate \\(p\\)-value pvalue &lt;- null_distn_paired_means %&gt;% get_pvalue(obs_stat = d_hat, direction = &quot;less&quot;) pvalue # A tibble: 1 × 1 p_value &lt;dbl&gt; 1 0 So our \\(p\\)-value is essentially 0 and we reject the null hypothesis at the 5% level. You can also see this from the histogram above that we are far into the left tail of the null distribution. Bootstrapping for confidence interval We can also create a confidence interval for the unknown population parameter \\(\\mu_{diff}\\) using our sample data (the calculated differences) with bootstrapping. This is similar to the bootstrapping done in a one sample mean case, except now our data is differences instead of raw numerical data. Note that this code is identical to the pipeline shown in the hypothesis test above except the hypothesize() function is not called. boot_distn_paired_means &lt;- zinc_diff %&gt;% specify(response = pair_diff) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;mean&quot;) ci &lt;- boot_distn_paired_means %&gt;% get_ci() ci # A tibble: 1 × 2 lower_ci upper_ci &lt;dbl&gt; &lt;dbl&gt; 1 -0.1116 -0.0501975 boot_distn_paired_means %&gt;% visualize(endpoints = ci, direction = &quot;between&quot;) We see that 0 is not contained in this confidence interval as a plausible value of \\(\\mu_{diff}\\) (the unknown population parameter). This matches with our hypothesis test results of rejecting the null hypothesis. Since zero is not a plausible value of the population parameter and since the entire confidence interval falls below zero, we have evidence that surface zinc concentration levels are lower, on average, than bottom level zinc concentrations. Interpretation: We are 95% confident the true mean zinc concentration on the surface is between 0.11 units smaller to 0.05 units smaller than on the bottom. B.6.4 Traditional methods Check conditions Remember that in order to use the shortcut (formula-based, theoretical) approach, we need to check that some conditions are met. Independent observations: The observations among pairs are independent. The locations are selected independently through random sampling so this condition is met. Approximately normal: The distribution of population of differences is normal or the number of pairs is at least 30. The histogram above does show some skew so we have reason to doubt the population being normal based on this sample. We also only have 10 pairs which is fewer than the 30 needed. A theory-based test may not be valid here. Test statistic The test statistic is a random variable based on the sample data. Here, we want to look at a way to estimate the population mean difference \\(\\mu_{diff}\\). A good guess is the sample mean difference \\(\\bar{X}_{diff}\\). Recall that this sample mean is actually a random variable that will vary as different samples are (theoretically, would be) collected. We are looking to see how likely is it for us to have observed a sample mean of \\(\\bar{x}_{diff} = -0.08\\) or smaller assuming that the population mean difference is 0 (assuming the null hypothesis is true). If the conditions are met and assuming \\(H_0\\) is true, we can “standardize” this original test statistic of \\(\\bar{X}_{diff}\\) into a \\(T\\) statistic that follows a \\(t\\) distribution with degrees of freedom equal to \\(df = n - 1\\): \\[ T =\\dfrac{ \\bar{X}_{diff} - 0}{ S_{diff} / \\sqrt{n} } \\sim t (df = n - 1) \\] where \\(S\\) represents the standard deviation of the sample differences and \\(n\\) is the number of pairs. Observed test statistic While one could compute this observed test statistic by “hand”, the focus here is on the set-up of the problem and in understanding which formula for the test statistic applies. We can use the t_test function on the differences to perform this analysis for us. t_test_results &lt;- zinc_diff %&gt;% t_test( formula = pair_diff ~ NULL, alternative = &quot;less&quot;, mu = 0 ) t_test_results One Sample t-test data: . t = 3, df = 19, p-value = 1 alternative hypothesis: true mean is less than 0 95 percent confidence interval: -Inf 4.08 sample estimates: mean of x 2.71 We see here that the \\(t_{obs}\\) value is 3.422. Compute \\(p\\)-value The \\(p\\)-value—the probability of observing a \\(t_{obs}\\) value of 3.422 or less in our null distribution of a \\(t\\) with 9 degrees of freedom—is . This can also be calculated in R directly: pt(-4.8638, df = nrow(zinc_diff) - 1, lower.tail = TRUE) [1] 0.000446 State conclusion We, therefore, have sufficient evidence to reject the null hypothesis. Our initial guess that our observed sample mean difference was not statistically less than the hypothesized mean of 0 has been invalidated here. Based on this sample, we have evidence that the mean concentration in the bottom water is greater than that of the surface water at different paired locations. B.6.5 Comparing results Observing the bootstrap distribution and the null distribution that were created, it makes quite a bit of sense that the results are so similar for traditional and non-traditional methods in terms of the \\(p\\)-value and the confidence interval since these distributions look very similar to normal distributions. The conditions were not met since the number of pairs was small, but the sample data was not highly skewed. Using any of the methods whether they are traditional (formula-based) or non-traditional (computational-based) lead to similar results here. References "],["C-appendixC.html", "C Tips and Tricks C.1 Data wrangling C.2 Interactive graphics", " C Tips and Tricks Needed packages Let’s load all the packages needed for this chapter (this assumes you’ve already installed them). Recall from our discussion in Section 4.3 that loading the tidyverse package by running library(tidyverse) loads the following commonly used data science packages all at once: ggplot2 for data visualization dplyr for data wrangling tidyr for converting data to “tidy” format readr for importing spreadsheet data into R As well as the more advanced purrr, tibble, stringr, and forcats packages If needed, read Section 1.3 for information on how to install and load R packages. library(tidyverse) library(scales) library(janitor) library(dygraphs) library(nycflights13) C.1 Data wrangling In this Section, we address some of the most common data wrangling questions we’ve encountered in student projects (shout out to Dr. Jenny Smetzer for her work setting this up!): C.1.1: Dealing with missing values C.1.2: Reordering bars in a barplot C.1.3: Showing money on an axis C.1.4: Changing values inside cells C.1.5: Converting a numerical variable to a categorical one C.1.6: Computing proportions C.1.7: Dealing with %, commas, and $ Let’s load an example movies dataset, pare down the rows and columns a bit, and then show the first 10 rows using slice(). movies_ex &lt;- read_csv(&quot;https://moderndive.com/data/movies.csv&quot;) %&gt;% filter(type %in% c(&quot;action&quot;, &quot;comedy&quot;, &quot;drama&quot;, &quot;animated&quot;, &quot;fantasy&quot;, &quot;rom comedy&quot;)) %&gt;% select(-over200) movies_ex %&gt;% slice(1:10) # A tibble: 10 × 5 name score rating type millions &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 2 Fast 2 Furious 48.9000 PG-13 action NA 2 A Guy Thing 39.5 PG-13 rom comedy 15.545 3 A Man Apart 42.9000 R action 26.2480 4 A Mighty Wind 79.9000 PG-13 comedy 17.781 5 Agent Cody Banks 57.9000 PG action 47.8110 6 Alex &amp; Emma 35.1000 PG-13 rom comedy 14.219 7 American Wedding 50.7000 R comedy 104.441 8 Anger Management 62.6000 PG-13 comedy 134.404 9 Anything Else 63.3000 R rom comedy 3.21200 10 Bad Boys II 38.1000 R action 138.397 C.1.1 Dealing with missing values You see the revenue in millions value for the movie “2 Fast 2 Furious” is NA (missing). So the following occurs when computing the median revenue: movies_ex %&gt;% summarize(mean_profit = median(millions)) # A tibble: 1 × 1 mean_profit &lt;dbl&gt; 1 NA You should always think about why a data value might be missing and what that missingness may mean. For example, imagine you are conducting a study on the effects of smoking on lung cancer and a lot of your patients’ data is missing because they died of lung cancer. If you just “sweep these patients under the rug” and ignore them, you are clearly biasing the results. While there are statistical methods to deal with missing data they are beyond the reach of this class. The easiest thing to do is to remove all missing cases, but you should always at the very least report to the reader if you do so, as by removing the missing values you may be biasing your results. You can do this with a na.rm = TRUE argument like so: movies_ex %&gt;% summarize(mean_profit = median(millions, na.rm = TRUE)) # A tibble: 1 × 1 mean_profit &lt;dbl&gt; 1 43.4270 If you decide you want to remove the row with the missing data, you can use the filter function like so: movies_no_missing &lt;- movies_ex %&gt;% filter(!is.na(millions)) movies_no_missing %&gt;% slice(1:10) # A tibble: 10 × 5 name score rating type millions &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 A Guy Thing 39.5 PG-13 rom comedy 15.545 2 A Man Apart 42.9000 R action 26.2480 3 A Mighty Wind 79.9000 PG-13 comedy 17.781 4 Agent Cody Banks 57.9000 PG action 47.8110 5 Alex &amp; Emma 35.1000 PG-13 rom comedy 14.219 6 American Wedding 50.7000 R comedy 104.441 7 Anger Management 62.6000 PG-13 comedy 134.404 8 Anything Else 63.3000 R rom comedy 3.21200 9 Bad Boys II 38.1000 R action 138.397 10 Bad Santa 75.8000 R comedy 59.5230 We see “2 Fast 2 Furious” is now gone. C.1.2 Reordering bars in a barplot Let’s compute the total revenue for each movie type and plot a barplot. revenue_by_type &lt;- movies_ex %&gt;% group_by(type) %&gt;% summarize(total_revenue = sum(millions)) revenue_by_type # A tibble: 6 × 2 type total_revenue &lt;chr&gt; &lt;dbl&gt; 1 action NA 2 animated 561.306 3 comedy 2286.81 4 drama 840.038 5 fantasy 508.580 6 rom comedy 492.282 ggplot(revenue_by_type, aes(x = type, y = total_revenue)) + geom_col() + labs(x = &quot;Movie genre&quot;, y = &quot;Total box office revenue (in millions of $)&quot;) Say we want to reorder the categorical variable type so that the bars show in a different order. We can reorder the bars by manually defining the order of the levels in the factor() command: type_levels &lt;- c(&quot;rom comedy&quot;, &quot;action&quot;, &quot;drama&quot;, &quot;animated&quot;, &quot;comedy&quot;, &quot;fantasy&quot;) revenue_by_type &lt;- revenue_by_type %&gt;% mutate(type = factor(type, levels = type_levels)) ggplot(revenue_by_type, aes(x = type, y = total_revenue)) + geom_col() + labs(x = &quot;Movie genre&quot;, y = &quot;Total boxoffice revenue (in millions of $)&quot;) Or if you want to reorder type in ascending order of total_revenue, we use reorder() revenue_by_type &lt;- revenue_by_type %&gt;% mutate(type = reorder(type, total_revenue)) ggplot(revenue_by_type, aes(x = type, y = total_revenue)) + geom_col() + labs( x = &quot;Movie genre&quot;, y = &quot;Total boxoffice revenue (in millions of $)&quot; ) Or if you want to reorder type in descending order of total_revenue, just put a - sign in front of -total_revenue in reorder(): revenue_by_type &lt;- revenue_by_type %&gt;% mutate(type = reorder(type, -total_revenue)) ggplot(revenue_by_type, aes(x = type, y = total_revenue)) + geom_col() + labs( x = &quot;Movie genre&quot;, y = &quot;Total boxoffice revenue (in millions of $)&quot; ) For more advanced categorical variable (i.e. factor) manipulations, check out the forcats package. Note: forcats is an anagram of factors C.1.3 Showing money on an axis movies_ex &lt;- movies_ex %&gt;% mutate(revenue = millions * 10^6) ggplot(data = movies_ex, aes(x = rating, y = revenue)) + geom_boxplot() + labs(x = &quot;rating&quot;, y = &quot;Revenue in $&quot;, title = &quot;Profits for different movie ratings&quot;) Google “ggplot2 axis scale dollars” and click on the first link and search for the word “dollars”. You’ll find: # Don&#39;t forget to load the scales package first! library(scales) ggplot(data = movies_ex, aes(x = rating, y = revenue)) + geom_boxplot() + labs(x = &quot;rating&quot;, y = &quot;Revenue in $&quot;, title = &quot;Profits for different movie ratings&quot;) + scale_y_continuous(labels = dollar) C.1.4 Changing values inside cells The rename() function in the dplyr package renames column/variable names. To “rename” values inside cells of a particular column, you need to mutate() the column using one of the three functions below. There might be other ones too, but these are the three we’ve seen the most. In these examples, we’ll change values in the variable type. if_else() recode() case_when() if_else() Switch all instances of rom comedy with romantic comedy using if_else() from the dplyr package. If a particular row has type == \"rom comedy\", then return \"romantic comedy\", else return whatever was originally in type. Save everything in a new variable type_new: movies_ex %&gt;% mutate(type_new = if_else(type == &quot;rom comedy&quot;, &quot;romantic comedy&quot;, type)) %&gt;% slice(1:10) # A tibble: 10 × 7 name score rating type millions revenue type_new &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2 Fast 2 Furious 48.9000 PG-13 action NA NA action 2 A Guy Thing 39.5 PG-13 rom comedy 15.545 15545000 romantic com… 3 A Man Apart 42.9000 R action 26.2480 26247999 action 4 A Mighty Wind 79.9000 PG-13 comedy 17.781 17781000 comedy 5 Agent Cody Banks 57.9000 PG action 47.8110 47811001 action 6 Alex &amp; Emma 35.1000 PG-13 rom comedy 14.219 14219000 romantic com… 7 American Wedding 50.7000 R comedy 104.441 104441000 comedy 8 Anger Management 62.6000 PG-13 comedy 134.404 134404010 comedy 9 Anything Else 63.3000 R rom comedy 3.21200 3212000. romantic com… 10 Bad Boys II 38.1000 R action 138.397 138397000 action Do the same here, but return \"not romantic comedy\" if type is not \"rom comedy\" and this time overwrite the original type variable movies_ex %&gt;% mutate(type = if_else(type == &quot;rom comedy&quot;, &quot;romantic comedy&quot;, &quot;not romantic comedy&quot;)) %&gt;% slice(1:10) # A tibble: 10 × 6 name score rating type millions revenue &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2 Fast 2 Furious 48.9000 PG-13 not romantic comedy NA NA 2 A Guy Thing 39.5 PG-13 romantic comedy 15.545 15545000 3 A Man Apart 42.9000 R not romantic comedy 26.2480 26247999 4 A Mighty Wind 79.9000 PG-13 not romantic comedy 17.781 17781000 5 Agent Cody Banks 57.9000 PG not romantic comedy 47.8110 47811001 6 Alex &amp; Emma 35.1000 PG-13 romantic comedy 14.219 14219000 7 American Wedding 50.7000 R not romantic comedy 104.441 104441000 8 Anger Management 62.6000 PG-13 not romantic comedy 134.404 134404010 9 Anything Else 63.3000 R romantic comedy 3.21200 3212000. 10 Bad Boys II 38.1000 R not romantic comedy 138.397 138397000 recode() if_else() is rather limited however. What if we want to “rename” all type so that they start with uppercase? Use recode(): movies_ex %&gt;% mutate(type_new = recode(type, &quot;action&quot; = &quot;Action&quot;, &quot;animated&quot; = &quot;Animated&quot;, &quot;comedy&quot; = &quot;Comedy&quot;, &quot;drama&quot; = &quot;Drama&quot;, &quot;fantasy&quot; = &quot;Fantasy&quot;, &quot;rom comedy&quot; = &quot;Romantic Comedy&quot; )) %&gt;% slice(1:10) # A tibble: 10 × 7 name score rating type millions revenue type_new &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2 Fast 2 Furious 48.9000 PG-13 action NA NA Action 2 A Guy Thing 39.5 PG-13 rom comedy 15.545 15545000 Romantic Com… 3 A Man Apart 42.9000 R action 26.2480 26247999 Action 4 A Mighty Wind 79.9000 PG-13 comedy 17.781 17781000 Comedy 5 Agent Cody Banks 57.9000 PG action 47.8110 47811001 Action 6 Alex &amp; Emma 35.1000 PG-13 rom comedy 14.219 14219000 Romantic Com… 7 American Wedding 50.7000 R comedy 104.441 104441000 Comedy 8 Anger Management 62.6000 PG-13 comedy 134.404 134404010 Comedy 9 Anything Else 63.3000 R rom comedy 3.21200 3212000. Romantic Com… 10 Bad Boys II 38.1000 R action 138.397 138397000 Action case_when() case_when() is a little trickier, but allows you to evaluate boolean operations using ==, &gt;, &gt;=, &amp;, |, etc: movies_ex %&gt;% mutate( type_new = case_when( type == &quot;action&quot; &amp; millions &gt; 40 ~ &quot;Big budget action&quot;, type == &quot;rom comedy&quot; &amp; millions &lt; 40 ~ &quot;Small budget romcom&quot;, # Need this for everything else that aren&#39;t the two cases above: TRUE ~ &quot;Rest&quot; ) ) # A tibble: 108 × 7 name score rating type millions revenue type_new &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 2 Fast 2 Furious 48.9000 PG-13 action NA NA Rest 2 A Guy Thing 39.5 PG-13 rom comedy 15.545 15545000 Small budget… 3 A Man Apart 42.9000 R action 26.2480 26247999 Rest 4 A Mighty Wind 79.9000 PG-13 comedy 17.781 17781000 Rest 5 Agent Cody Banks 57.9000 PG action 47.8110 47811001 Big budget a… 6 Alex &amp; Emma 35.1000 PG-13 rom comedy 14.219 14219000 Small budget… 7 American Wedding 50.7000 R comedy 104.441 104441000 Rest 8 Anger Management 62.6000 PG-13 comedy 134.404 134404010 Rest 9 Anything Else 63.3000 R rom comedy 3.21200 3212000. Small budget… 10 Bad Boys II 38.1000 R action 138.397 138397000 Big budget a… # … with 98 more rows C.1.5 Converting a numerical variable to a categorical one Sometimes we want to turn a numerical, continuous variable into a categorical variable. For instance, what if we wanted to have a variable that tells us if a movie made one hundred million dollars or more. That is to say, we can create a binary variable, which is the same thing as a categorical variable with 2 levels. We can again use the mutate() function: movies_ex %&gt;% mutate(big_budget = millions &gt; 100) %&gt;% slice(1:10) # A tibble: 10 × 7 name score rating type millions revenue big_budget &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; 1 2 Fast 2 Furious 48.9000 PG-13 action NA NA NA 2 A Guy Thing 39.5 PG-13 rom comedy 15.545 15545000 FALSE 3 A Man Apart 42.9000 R action 26.2480 26247999 FALSE 4 A Mighty Wind 79.9000 PG-13 comedy 17.781 17781000 FALSE 5 Agent Cody Banks 57.9000 PG action 47.8110 47811001 FALSE 6 Alex &amp; Emma 35.1000 PG-13 rom comedy 14.219 14219000 FALSE 7 American Wedding 50.7000 R comedy 104.441 104441000 TRUE 8 Anger Management 62.6000 PG-13 comedy 134.404 134404010 TRUE 9 Anything Else 63.3000 R rom comedy 3.21200 3212000. FALSE 10 Bad Boys II 38.1000 R action 138.397 138397000 TRUE What if you want to convert a numerical variable into a categorical variable with more than 2 levels? One way is to use the cut() command. For instance, below, we cut() the score variable, to recode it into 4 categories: 0 - 40 = bad 40.1 - 60 = so-so 60.1 - 80 = good 80.1+ = great We set the breaking points for cutting the numerical variable with the c(0, 40, 60, 80, 100) part, and set the labels for each of these bins with the labels = c(\"bad\", \"so-so\", \"good\", \"great\") part. All this action happens inside the mutate() command, so the new categorical variable score_categ is added to the data frame. movies_ex %&gt;% mutate(score_categ = cut(score, breaks = c(0, 40, 60, 80, 100), labels = c(&quot;bad&quot;, &quot;so-so&quot;, &quot;good&quot;, &quot;great&quot;) )) %&gt;% slice(1:10) # A tibble: 10 × 7 name score rating type millions revenue score_categ &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 2 Fast 2 Furious 48.9000 PG-13 action NA NA so-so 2 A Guy Thing 39.5 PG-13 rom comedy 15.545 15545000 bad 3 A Man Apart 42.9000 R action 26.2480 26247999 so-so 4 A Mighty Wind 79.9000 PG-13 comedy 17.781 17781000 good 5 Agent Cody Banks 57.9000 PG action 47.8110 47811001 so-so 6 Alex &amp; Emma 35.1000 PG-13 rom comedy 14.219 14219000 bad 7 American Wedding 50.7000 R comedy 104.441 104441000 so-so 8 Anger Management 62.6000 PG-13 comedy 134.404 134404010 good 9 Anything Else 63.3000 R rom comedy 3.21200 3212000. good 10 Bad Boys II 38.1000 R action 138.397 138397000 bad Other options with the cut function: By default, if the value is exactly the upper bound of an interval, it’s included in the lessor category (e.g. 60.0 is ‘so-so’ not ‘good’), to flip this, include the argument right = FALSE. You could also have R equally divide the variable into a balanced number of groups. For example, specifying breaks = 3 would create 3 groups with approximately the same number of values in each group. C.1.6 Computing proportions By using a group_by() followed not by a summarize() as is often the case, but rather a mutate(). So say we compute the total revenue millions for each movie rating and type: rating_by_type_millions &lt;- movies_ex %&gt;% group_by(rating, type) %&gt;% summarize(millions = sum(millions)) %&gt;% arrange(rating, type) rating_by_type_millions # A tibble: 15 × 3 # Groups: rating [4] rating type millions &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 G animated 495.594 2 PG action 47.8110 3 PG animated 65.712 4 PG comedy 829.616 5 PG drama 160.873 6 PG fantasy 147.461 7 PG-13 action NA 8 PG-13 comedy 1208.31 9 PG-13 drama 306.26 10 PG-13 fantasy 361.119 11 PG-13 rom comedy 406.251 12 R action 1044.82 13 R comedy 248.876 14 R drama 372.905 15 R rom comedy 86.0310 Say within each movie rating (G, PG, PG-13, R), we want to know the proportion of total_millions that made by each movie type (animated, action, comedy, etc). We can: rating_by_type_millions %&gt;% group_by(rating) %&gt;% mutate( # Compute a new column of the sum of millions split by rating: total_millions = sum(millions), # Compute the proportion within each rating: prop = millions / total_millions ) # A tibble: 15 × 5 # Groups: rating [4] rating type millions total_millions prop &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 G animated 495.594 495.594 1 2 PG action 47.8110 1251.47 0.0382038 3 PG animated 65.712 1251.47 0.0525077 4 PG comedy 829.616 1251.47 0.662912 5 PG drama 160.873 1251.47 0.128547 6 PG fantasy 147.461 1251.47 0.117830 7 PG-13 action NA NA NA 8 PG-13 comedy 1208.31 NA NA 9 PG-13 drama 306.26 NA NA 10 PG-13 fantasy 361.119 NA NA 11 PG-13 rom comedy 406.251 NA NA 12 R action 1044.82 1752.63 0.596143 13 R comedy 248.876 1752.63 0.142001 14 R drama 372.905 1752.63 0.212769 15 R rom comedy 86.0310 1752.63 0.0490868 So for example, the 4 proportions corresponding to R rated movies are 0.596 + 0.142 + 0.213 + 0.0491 = 1. C.1.7 Dealing with %, commas, and $ Say you have numerical data that are recorded as percentages, have commas, or are in dollar form and hence are character strings. How do you convert these to numerical values? Using the parse_number() function from the readr package inside a mutate()! Shout out to Stack Overflow library(readr) parse_number(&quot;10.5%&quot;) [1] 10.5 parse_number(&quot;145,897&quot;) [1] 145897 parse_number(&quot;$1,234.5&quot;) [1] 1234 What about the other way around? Use the scales package! library(scales) percent(0.105) [1] &quot;10%&quot; comma(145897) [1] &quot;145,897&quot; dollar(1234.5) [1] &quot;$1,234.50&quot; Congratulations. You are now an R Ninja! C.2 Interactive graphics C.2.1 Interactive linegraphs Another useful tool for viewing linegraphs such as this is the dygraph function in the dygraphs package in combination with the dyRangeSelector function. This allows us to zoom in on a selected range and get an interactive plot for us to work with: library(dygraphs) library(nycflights13) flights_day &lt;- mutate(flights, date = as.Date(time_hour)) flights_summarized &lt;- flights_day %&gt;% group_by(date) %&gt;% summarize(median_arr_delay = median(arr_delay, na.rm = TRUE)) rownames(flights_summarized) &lt;- flights_summarized$date flights_summarized &lt;- select(flights_summarized, -date) dyRangeSelector(dygraph(flights_summarized)) The syntax here is a little different than what we have covered so far. The dygraph function is expecting for the dates to be given as the rownames of the object. We then remove the date variable from the flights_summarized data frame since it is accounted for in the rownames. Lastly, we run the dygraph function on the new data frame that only contains the median arrival delay as a column and then provide the ability to have a selector to zoom in on the interactive plot via dyRangeSelector. (Note that this plot will only be interactive in the HTML version of this book.) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
